{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b3dba6-25a5-4217-82a9-be1f5f8cf03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This version allows you to create predictors for in-state models where an AIS exist as well as a second set of predictors for \n",
    "predicting to your state if your AIS is not present. You only need to make the training predictors if you are training and predicting \n",
    "to the same state.  To predict to a different state you need to make predictors for that state too.\n",
    "'''\n",
    "#Import required Python packages. \n",
    "'''\n",
    "If you get an error here, search for the package name online + conda installation; open a new Anaconda PowerShell window;\n",
    "activate your environment and paste in the code you found online into the prompt.  It will most likely be: pip install \"package-name\" \n",
    "for Python packages or conda install conda-forge:: \"package-name\" for cross-platform packages (i.e., Scikit Learn).  \n",
    "'''\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import zipfile\n",
    "import random\n",
    "import io\n",
    "from io import StringIO, BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from typing import Set, Tuple\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.mask import mask\n",
    "import rasterio.plot\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from affine import Affine\n",
    "import networkx as nx\n",
    "from shapely.geometry import Point, LineString, MultiLineString, MultiPoint\n",
    "from shapely.ops import snap, nearest_points\n",
    "from rasterio.transform import from_bounds\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import unary_union\n",
    "from rasterio.merge import merge\n",
    "from rasterio.errors import RasterioIOError\n",
    "from joblib import Parallel, delayed\n",
    "# Machine Learning & Statistical Modeling\n",
    "import sklearn as skl\n",
    "from sklearn.neighbors import KDTree, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import gaussian_process\n",
    "\n",
    "# Geostatistics & Interpolation\n",
    "import skgstat as skg\n",
    "import gstools as gs\n",
    "from skgstat import models\n",
    "from skgstat.util.likelihood import get_likelihood\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.strtree import STRtree\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "import scipy.ndimage\n",
    "import pykrige.kriging_tools as kt\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# APIs & Requests\n",
    "import requests\n",
    "from pygbif import occurrences as occ\n",
    "from pygbif import species\n",
    "from shapely import union_all\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from contextlib import contextmanager\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from multiprocessing import Manager, Lock\n",
    "import tempfile\n",
    "from shapely.geometry import CAP_STYLE\n",
    "from shapely.ops import unary_union, polygonize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef442c4-0f8e-4b9f-8783-e00e153a5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water quality helper functions\n",
    "def get_usgs_stations_in_state(state_fip, state_abbr, my_path):\n",
    "    \"\"\"\n",
    "    Downloads USGS water monitoring station data and filters it to the specified state boundary.\n",
    "\n",
    "    Parameters:\n",
    "    - state_fip (str): State FIPS code in the form 'US:55'\n",
    "    - state_abbr (str): Two-letter state abbreviation (e.g., 'WI')\n",
    "    - my_path (str): Directory to save shapefile (must end with '/')\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: Filtered monitoring stations within the state\n",
    "    \"\"\"\n",
    "    # === Step 1: Download USGS station data ===\n",
    "    print(\"üì• Downloading USGS station data...\")\n",
    "    url_base = 'https://www.waterqualitydata.us/data/Station/search?'\n",
    "    request_url = f\"{url_base}countrycode=US&statecode=US:{state_fip}\"\n",
    "    station_df = pd.read_csv(request_url)\n",
    "\n",
    "    stations = station_df[['MonitoringLocationIdentifier', 'LatitudeMeasure', 'LongitudeMeasure']].rename(columns={\n",
    "        'MonitoringLocationIdentifier': 'station_id',\n",
    "        'LatitudeMeasure': 'latitude',\n",
    "        'LongitudeMeasure': 'longitude'\n",
    "    })\n",
    "\n",
    "    stations_gdf = gpd.GeoDataFrame(\n",
    "        stations,\n",
    "        geometry=gpd.points_from_xy(stations.longitude, stations.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:5070\")\n",
    "\n",
    "    # === Step 2: Download state boundaries shapefile ===\n",
    "    print(\"üó∫Ô∏è  Downloading state boundary shapefile...\")\n",
    "    state_boundary_url = 'http://www2.census.gov/geo/tiger/TIGER2012/STATE/tl_2012_us_state.zip'\n",
    "    r = requests.get(state_boundary_url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path=my_path)\n",
    "    \n",
    "    # Load shapefile\n",
    "    shp_path = os.path.join(my_path, [f for f in z.namelist() if f.endswith('.shp')][0])\n",
    "    state_boundary = gpd.read_file(shp_path).to_crs(\"EPSG:5070\")\n",
    "\n",
    "    # === Step 3: Filter to selected state ===\n",
    "    state = state_boundary[state_boundary['STUSPS'] == state_abbr]\n",
    "    if state.empty:\n",
    "        raise ValueError(f\"No state found for abbreviation '{state_abbr}'\")\n",
    "\n",
    "    # Match CRS and filter points within state\n",
    "    stations_gdf = stations_gdf.to_crs(state.crs)\n",
    "    filtered_stations = gpd.sjoin(\n",
    "        stations_gdf, state, how=\"inner\", predicate=\"within\"\n",
    "    ).drop(columns=[\"index_right\"])\n",
    "\n",
    "    # === Step 4: Save to file and return ===\n",
    "    out_file = os.path.join(my_path, f\"usgs_stations_{state_abbr}.shp\")\n",
    "    filtered_stations.to_file(out_file)\n",
    "    print(f\"‚úÖ Filtered station shapefile saved to: {out_file}\")\n",
    "\n",
    "    return filtered_stations\n",
    "\n",
    "\n",
    "\n",
    "def find_nearest(row, other_gdf):\n",
    "    # Find the index of the nearest geometry\n",
    "    nearest_idx = other_gdf.distance(row.geometry).idxmin()\n",
    "    return other_gdf.loc[nearest_idx]\n",
    "def add_nearest(gdf1, gdf2):\n",
    "    nearest_neighbors = gdf1.apply(lambda row: find_nearest(row, gdf2), axis=1)\n",
    "    # Add nearest neighbor information to the first GeoDataFrame\n",
    "    gdf1['nearest_id'] = nearest_neighbors['station_id']\n",
    "    gdf1['median'] = nearest_neighbors['median']\n",
    "    parameter_added = gdf1\n",
    "    return parameter_added\n",
    "def thin_geodataframe(gdf, min_dist=100):\n",
    "    \"\"\"\n",
    "    Spatially thin a GeoDataFrame using a minimum distance (in meters).\n",
    "    Ensures points are at least `min_dist` apart.\n",
    "    \"\"\"\n",
    "    if gdf.crs.to_epsg() != 5070:\n",
    "        gdf = gdf.to_crs(\"EPSG:5070\")\n",
    "\n",
    "    # Create mapping from geometry to index\n",
    "    geom_to_index = {id(geom): idx for idx, geom in zip(gdf.index, gdf.geometry)}\n",
    "    strtree = STRtree(gdf.geometry.values)\n",
    "\n",
    "    kept_geoms = []\n",
    "    taken_ids = set()\n",
    "\n",
    "    for geom in gdf.geometry:\n",
    "        if id(geom) in taken_ids:\n",
    "            continue\n",
    "\n",
    "        kept_geoms.append(geom)\n",
    "        buffered = geom.buffer(min_dist)\n",
    "        for neighbor in strtree.query(buffered):\n",
    "            taken_ids.add(id(neighbor))\n",
    "\n",
    "    return gpd.GeoDataFrame(geometry=kept_geoms, crs=gdf.crs)\n",
    "\n",
    "# Data acquisition & formating\n",
    "def get_water_quality(characteristic, state_fip, stations_gdf, my_path):\n",
    "    \"\"\"\n",
    "    Download, clean, summarize, and spatially join water quality data from WQP.\n",
    "\n",
    "    Parameters:\n",
    "    - characteristic (str): Water quality variable (e.g., 'pH', 'Calcium', 'Nitrogen', 'Phosphorus', 'Oxygen')\n",
    "    - state_fip (str): State FIPS code (e.g., '55' for Wisconsin)\n",
    "    - training_stations_gdf (GeoDataFrame): Monitoring stations to match to data\n",
    "    - my_path (str): Path to save output shapefile (include trailing slash)\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: Stations with median value for the specified characteristic\n",
    "    \"\"\"\n",
    "    # === Step 1: Download data ===\n",
    "    print(f\"üì• Downloading {characteristic} data from WQP...\")\n",
    "    url_base = 'https://www.waterqualitydata.us/data/Result/search?'\n",
    "    request_url = f\"{url_base}countrycode=US&statecode=US:{state_fip}&characteristicName={characteristic}&mimeType=csv\"\n",
    "    df = pd.read_csv(request_url)\n",
    "    print(f\"‚úÖ Retrieved {len(df)} records\")\n",
    "\n",
    "    # === Step 2: Clean and rename ===\n",
    "    df = df.dropna(subset=['ResultMeasureValue'])\n",
    "    df['ResultMeasureValue'] = pd.to_numeric(df['ResultMeasureValue'], errors='coerce')\n",
    "    \n",
    "    df_clean = df[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier',\n",
    "                   'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\n",
    "        \"CharacteristicName\": \"Characteristic\",\n",
    "        \"ActivityStartDate\": \"date\",\n",
    "        \"MonitoringLocationIdentifier\": \"station_id\",\n",
    "        \"ResultMeasureValue\": \"value\",\n",
    "        \"ResultMeasure/MeasureUnitCode\": \"unit\"\n",
    "    })\n",
    "\n",
    "    # === Step 3: Standardize units ===\n",
    "    df_clean['unit'] = df_clean['unit'].str.strip().str.lower()\n",
    "\n",
    "    if characteristic.lower() in ['calcium', 'nitrogen', 'phosphorus']:\n",
    "        df_clean.loc[df_clean['unit'].isin(['ug/l', 'ppb', 'mg/g']), 'value'] /= 1000\n",
    "        df_clean['unit'] = df_clean['unit'].replace({\n",
    "            'mg/l': 'mg/l', 'mg/l as p': 'mg/l', 'mg/l po4': 'mg/l',\n",
    "            'mg/kg': 'mg/l', 'mg/kg as p': 'mg/l',\n",
    "            'ug/l': 'mg/l', 'ppb': 'mg/l', 'mg/g': 'mg/l'\n",
    "        })\n",
    "\n",
    "    elif characteristic.lower() == 'ph':\n",
    "        # Keep only valid units or no units\n",
    "        df_clean = df_clean[(df_clean['unit'].isna()) | (df_clean['unit'].isin(['std units', '']))]\n",
    "        df_clean['unit'] = 'std units'\n",
    "\n",
    "    elif characteristic.lower() == 'oxygen':\n",
    "        # Convert mg/L to % saturation using multiplier\n",
    "        df_clean.loc[df_clean['unit'] == 'mg/l', 'value'] *= 12.67\n",
    "        df_clean['unit'] = df_clean['unit'].replace({'mg/l': '% saturatn', '% by vol': '% saturatn'})\n",
    "\n",
    "    # === Step 4: Filter implausible values ===\n",
    "    if characteristic.lower() == 'calcium':\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'mg/l') & (df_clean['value'].between(0, 300))]\n",
    "    elif characteristic.lower() == 'ph':\n",
    "        df_clean = df_clean[(df_clean['value'].between(4, 14))]\n",
    "    elif characteristic.lower() == 'nitrogen':\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'mg/l') & (df_clean['value'].between(0, 500))]\n",
    "    elif characteristic.lower() == 'phosphorus':\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'mg/l') & (df_clean['value'].between(0, 1))]\n",
    "    elif characteristic.lower() == 'oxygen':\n",
    "        df_clean = df_clean[(df_clean['unit'] == '% saturatn') & (df_clean['value'].between(0, 200))]\n",
    "\n",
    "    # === Step 5: Aggregate by station ===\n",
    "    summary = df_clean.groupby('station_id')['value'].median().reset_index()\n",
    "    summary.rename(columns={'value': 'median'}, inplace=True)\n",
    "\n",
    "    # === Step 6: Spatial join with training stations ===\n",
    "    stations_with = pd.merge(stations_gdf, summary, on='station_id', how='inner')\n",
    "    stations_missing = stations_gdf[~stations_gdf['station_id'].isin(stations_with['station_id'])]\n",
    "\n",
    "    # === Step 7: Fill missing via nearest station ===\n",
    "    print(f\"üîÑ Imputing missing {characteristic} values...\")\n",
    "    stations_filled = add_nearest(stations_missing, stations_with)\n",
    "    final_gdf = pd.concat([stations_with, stations_filled], axis=0).drop(columns=[\"nearest_id\", \"latitude\", \"longitude\"], errors='ignore')\n",
    "\n",
    "    # === Step 8: Save shapefile ===\n",
    "    out_name = f\"usgs_{characteristic.lower().replace(' ', '_')}.shp\"\n",
    "    out_path = os.path.join(my_path, out_name)\n",
    "    final_gdf.to_file(out_path)\n",
    "    print(f\"‚úÖ {characteristic} data saved to: {out_path}\")\n",
    "\n",
    "    return final_gdf\n",
    "\n",
    "# Functions for making predictors\n",
    "def interpolate(stations_w_data, filename, bandname=\"Interpolated_Band\", pixel_size=100, use_kriging='auto'):\n",
    "    # Ensure CRS and extract coordinates\n",
    "    stations_w_data = stations_w_data.copy()\n",
    "    stations_w_data = stations_w_data.set_crs(\"EPSG:5070\")\n",
    "    stations_w_data['x'] = stations_w_data.geometry.x\n",
    "    stations_w_data['y'] = stations_w_data.geometry.y\n",
    "    samples_df = stations_w_data[['x', 'y', 'median']]\n",
    "    coords = np.column_stack((samples_df['x'], samples_df['y']))\n",
    "    vals = samples_df['median']\n",
    "\n",
    "    # Scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    art = ax.scatter(samples_df['x'], samples_df['y'], s=10, c=vals, cmap='plasma', vmin=0, vmax=vals.max())\n",
    "    plt.colorbar(art)\n",
    "    plt.title(\"Original Data Points\")\n",
    "    plt.show()\n",
    "\n",
    "    # Define bounds and grid size\n",
    "    xmin, xmax = samples_df['x'].min(), samples_df['x'].max()\n",
    "    ymin, ymax = samples_df['y'].min(), samples_df['y'].max()\n",
    "\n",
    "    # Define grid size based on pixel resolution\n",
    "    ncols = int(np.ceil((xmax - xmin) / pixel_size))\n",
    "    nrows = int(np.ceil((ymax - ymin) / pixel_size))\n",
    "    xres = pixel_size\n",
    "    yres = pixel_size\n",
    "    \n",
    "    print(f\"üìè Pixel size: {xres} m x {yres} m\")\n",
    "    print(f\"üì¶ Grid dimensions: {ncols} cols √ó {nrows} rows\")\n",
    "    \n",
    "    # Smart fallback if 'auto' is used\n",
    "    if use_kriging == 'auto':\n",
    "        use_kriging = (ncols * nrows <= 1e6)\n",
    "    \n",
    "    # Interpolation block\n",
    "    if use_kriging:\n",
    "        try:\n",
    "            print(\"üîπ Attempting Ordinary Kriging interpolation...\")\n",
    "            maxlag = np.median(pdist(coords))\n",
    "            n_lags = min(30, max(10, int(len(coords) / 10)))\n",
    "            V = skg.Variogram(coords, vals, model='exponential', maxlag=maxlag, n_lags=n_lags, normalize=False)\n",
    "            ok = skg.OrdinaryKriging(V, min_points=1, max_points=8, mode='exact')\n",
    "    \n",
    "            grid_x = np.linspace(xmin, xmax, ncols)\n",
    "            grid_y = np.linspace(ymin, ymax, nrows)\n",
    "            xx, yy = np.meshgrid(grid_x, grid_y)\n",
    "    \n",
    "            field = ok.transform(xx.flatten(), yy.flatten()).reshape(xx.shape)\n",
    "            print(\"‚úÖ Kriging completed successfully.\")\n",
    "    \n",
    "        except (MemoryError, ValueError, np.linalg.LinAlgError):\n",
    "            print(\"‚ö†Ô∏è Kriging failed, switching to Nearest Neighbor interpolation...\")\n",
    "            use_kriging = False  # Fallback to KNN\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipping Kriging; using Nearest Neighbor interpolation...\")\n",
    "    \n",
    "    # KNN Interpolation (used either by default or after Kriging fails)\n",
    "    if not use_kriging:\n",
    "        grid_x = np.linspace(xmin, xmax, ncols)\n",
    "        grid_y = np.linspace(ymin, ymax, nrows)\n",
    "        xx, yy = np.meshgrid(grid_x, grid_y)\n",
    "    \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(8, len(coords)), weights='distance', algorithm='kd_tree', n_jobs=-1)\n",
    "        knn.fit(coords, vals)\n",
    "        query_points = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "        field = knn.predict(query_points).reshape(xx.shape)\n",
    "    \n",
    "        print(\"‚úÖ Nearest Neighbor interpolation completed.\")\n",
    "\n",
    "\n",
    "    # Fill NaNs\n",
    "    def fill_missing_values(field):\n",
    "        mask = np.isnan(field)\n",
    "        if np.all(mask):\n",
    "            raise ValueError(\"All values are NaN; interpolation cannot be performed.\")\n",
    "        nearest_indices = scipy.ndimage.distance_transform_edt(mask, return_distances=False, return_indices=True)\n",
    "        field[mask] = field[tuple(nearest_indices[i][mask] for i in range(field.ndim))]\n",
    "        return field\n",
    "\n",
    "    field = fill_missing_values(field)\n",
    "\n",
    "    # Prepare raster\n",
    "    arr = np.flip(field, axis=0).astype(np.float32)\n",
    "    transform = from_origin(xmin, ymax, xres, yres)\n",
    "\n",
    "    with rasterio.open(filename, 'w', driver='GTiff',\n",
    "                       height=arr.shape[0], width=arr.shape[1],\n",
    "                       count=1, dtype=arr.dtype,\n",
    "                       crs=\"EPSG:5070\", transform=transform) as dst:\n",
    "        dst.write(arr, 1)\n",
    "        dst.set_band_description(1, bandname)\n",
    "\n",
    "    print(f\"‚úÖ Raster saved: {filename}\")\n",
    "\n",
    "    # Plot final raster\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(arr, extent=[xmin, xmax, ymin, ymax], cmap='plasma', origin=\"upper\")\n",
    "    plt.colorbar(label=bandname)\n",
    "    plt.title(\"Interpolated Raster\")\n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "    plt.show()\n",
    "\n",
    "# Biological predictor and training/background data helper functions\n",
    "def get_nhd_waterbodies(state_name, local_path, output_prefix=None, save_files=True,\n",
    "                         make_background_water=False, training_path=None, training_state_abbr=None):\n",
    "    \"\"\"\n",
    "    Downloads, extracts, and loads NHD shapefiles for a U.S. state, returning lakes, rivers, and flowlines.\n",
    "    \"\"\"\n",
    "    if output_prefix is None:\n",
    "        output_prefix = state_name.lower()\n",
    "\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "    # Use GPKG filepaths for caching\n",
    "    lakes_gpkg = os.path.join(local_path, f\"{output_prefix}_lakes.gpkg\")\n",
    "    rivers_gpkg = os.path.join(local_path, f\"{output_prefix}_rivers.gpkg\")\n",
    "    streams_gpkg = os.path.join(local_path, f\"{output_prefix}_streams.gpkg\")\n",
    "\n",
    "    if save_files and all(os.path.exists(f) for f in [lakes_gpkg, rivers_gpkg, streams_gpkg]):\n",
    "        print(\"üìÇ Found existing GPKG files. Loading from disk...\")\n",
    "        lakes = gpd.read_file(lakes_gpkg)\n",
    "        rivers = gpd.read_file(rivers_gpkg)\n",
    "        streams = gpd.read_file(streams_gpkg)\n",
    "    else:\n",
    "        # === Step 1‚Äì5: Download, extract, load ===\n",
    "        url = f\"https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHD/State/Shape/NHD_H_{state_name}_State_Shape.zip\"\n",
    "        print(f\"üì• Downloading NHD data for {state_name}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to download NHD data for {state_name}\")\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as tmp_file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                tmp_file.write(chunk)\n",
    "\n",
    "        with zipfile.ZipFile(tmp_file.name, 'r') as z:\n",
    "            z.extractall(path=local_path)\n",
    "        print(\"‚úÖ Extraction complete.\")\n",
    "\n",
    "        shp_files = glob.glob(os.path.join(local_path, \"**\", \"*.shp\"), recursive=True)\n",
    "        if not shp_files:\n",
    "            raise FileNotFoundError(\"‚ùå No shapefiles found in extracted directory.\")\n",
    "\n",
    "        lakes_path = next((f for f in shp_files if \"nhdwaterbody\" in f.lower()), None)\n",
    "        if not lakes_path:\n",
    "            raise FileNotFoundError(\"‚ùå No NHDWaterbody shapefile found.\")\n",
    "        lakes = gpd.read_file(lakes_path)\n",
    "\n",
    "        rivers_path = next((f for f in shp_files if \"nhdarea\" in f.lower()), None)\n",
    "        if not rivers_path:\n",
    "            raise FileNotFoundError(\"‚ùå No NHDArea shapefile found.\")\n",
    "        rivers = gpd.read_file(rivers_path)\n",
    "\n",
    "        print(\"üìÑ Loading and combining stream flowlines...\")\n",
    "        flowline_paths = [f for f in shp_files if \"nhdflowline\" in f.lower()]\n",
    "        if not flowline_paths:\n",
    "            raise FileNotFoundError(\"‚ùå No NHDFlowline shapefiles found.\")\n",
    "        stream_dfs = [gpd.read_file(fp) for fp in flowline_paths]\n",
    "        streams = gpd.GeoDataFrame(pd.concat(stream_dfs, ignore_index=True), crs=stream_dfs[0].crs)\n",
    "\n",
    "        if save_files:\n",
    "            print(\"üíæ Saving as GeoPackage files...\")\n",
    "            lakes.to_file(lakes_gpkg, driver=\"GPKG\")\n",
    "            rivers.to_file(rivers_gpkg, driver=\"GPKG\")\n",
    "            streams.to_file(streams_gpkg, driver=\"GPKG\")\n",
    "\n",
    "    # ‚úÖ Now return regardless of source (cached or new)\n",
    "    result = {\n",
    "        \"lakes\": lakes,\n",
    "        \"rivers\": rivers,\n",
    "        \"streams\": streams\n",
    "    }\n",
    "\n",
    "    if make_background_water and training_path and training_state_abbr:\n",
    "        bg_water = pd.concat([lakes, rivers], ignore_index=True)\n",
    "        output_file = os.path.join(training_path, f\"{training_state_abbr}_bg_water.shp\")\n",
    "        bg_water.to_file(output_file)\n",
    "        result[\"bg_water\"] = bg_water\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def buffer_water_layers(\n",
    "    streams: gpd.GeoDataFrame = None,\n",
    "    lakes: gpd.GeoDataFrame = None,\n",
    "    rivers: gpd.GeoDataFrame = None,\n",
    "    simplify_tolerance: float = None,\n",
    "    filter_streams: bool = True,\n",
    "    save_path: str = None,\n",
    "    use_cached: bool = True,\n",
    "    export_merged_filename: str = None\n",
    ") -> tuple[gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Buffer and merge water layers, optionally using cached files.\n",
    "    \"\"\"\n",
    "\n",
    "    crs = \"EPSG:5070\"\n",
    "\n",
    "    # Define cache paths\n",
    "    if save_path:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        stream_file = os.path.join(save_path, \"buffered_streams.gpkg\")\n",
    "        lake_file = os.path.join(save_path, \"buffered_lakes.gpkg\")\n",
    "        river_file = os.path.join(save_path, \"buffered_rivers.gpkg\")\n",
    "        full_stream_file = os.path.join(save_path, \"full_streams.gpkg\")\n",
    "\n",
    "        cache_exists = all(os.path.exists(f) for f in [stream_file, lake_file, river_file, full_stream_file])\n",
    "        if use_cached and cache_exists:\n",
    "            print(\"üìÇ Loading buffered layers from cache...\")\n",
    "            buffered_streams = gpd.read_file(stream_file)\n",
    "            buffered_lakes = gpd.read_file(lake_file)\n",
    "            buffered_rivers = gpd.read_file(river_file)\n",
    "            full_streams = gpd.read_file(full_stream_file)\n",
    "\n",
    "            buffered_water = pd.concat([buffered_streams, buffered_lakes, buffered_rivers], ignore_index=True)\n",
    "            buffered_water = buffered_water[buffered_water.geometry.notnull()]\n",
    "            buffered_water[\"waterID\"] = range(1, len(buffered_water) + 1)\n",
    "\n",
    "            if export_merged_filename and not os.path.exists(export_merged_filename):\n",
    "                print(f\"üíæ Merging and exporting combined buffered layer to {export_merged_filename}...\")\n",
    "                buffered_water.to_file(export_merged_filename)\n",
    "\n",
    "            return buffered_streams, buffered_lakes, buffered_rivers, full_streams, buffered_water\n",
    "\n",
    "    # If no cache or not using cache, we require streams/lakes/rivers\n",
    "    if streams is None or lakes is None or rivers is None:\n",
    "        raise ValueError(\"streams, lakes, and rivers must be provided if not using cached files.\")\n",
    "\n",
    "    # === Process from scratch ===\n",
    "    print(\"[1/6] Projecting all layers...\")\n",
    "    streams = streams.to_crs(crs)\n",
    "    lakes = lakes.to_crs(crs)\n",
    "    rivers = rivers.to_crs(crs)\n",
    "    full_streams = streams.copy()\n",
    "\n",
    "    print(\"[2/6] Filtering lakes and rivers with areasqkm ‚â• 0.25...\")\n",
    "    lakes = lakes[lakes.is_valid & lakes.geometry.notnull() & (lakes[\"areasqkm\"] >= 0.25)]\n",
    "    rivers = rivers[rivers.is_valid & rivers.geometry.notnull() & (rivers[\"areasqkm\"] >= 0.25)]\n",
    "\n",
    "    print(\"[3/6] Preparing stream geometry...\")\n",
    "    # Drop invalid or missing geometries before simplification\n",
    "    filtered_streams = streams.copy()\n",
    "    filtered_streams = filtered_streams[filtered_streams.geometry.notnull()].copy()\n",
    "    filtered_streams = filtered_streams[filtered_streams.is_valid].copy()\n",
    "    \n",
    "    # Now apply simplification\n",
    "    if simplify_tolerance is not None:\n",
    "        print(f\"[4/6] Simplifying stream geometries with tolerance {simplify_tolerance}...\")\n",
    "        filtered_streams[\"geometry\"] = filtered_streams.geometry.simplify(\n",
    "            tolerance=simplify_tolerance, preserve_topology=True)\n",
    "    else:\n",
    "        print(\"[4/6] Skipping simplification.\")\n",
    "\n",
    "    print(\"[5/6] Buffering lakes and rivers...\")\n",
    "    lakes[\"geometry\"] = lakes.geometry.buffer(50)\n",
    "    rivers[\"geometry\"] = rivers.geometry.buffer(50)\n",
    "\n",
    "    print(\"[6/6] Buffering stream geometries...\")\n",
    "    filtered_streams[\"geometry\"] = filtered_streams.geometry.buffer(50, cap_style=CAP_STYLE.flat)\n",
    "\n",
    "    # Optional save\n",
    "    if save_path:\n",
    "        print(\"üíæ Saving buffered layers...\")\n",
    "        filtered_streams.to_file(stream_file, driver=\"GPKG\")\n",
    "        lakes.to_file(lake_file, driver=\"GPKG\")\n",
    "        rivers.to_file(river_file, driver=\"GPKG\")\n",
    "        full_streams.to_file(full_stream_file, driver=\"GPKG\")\n",
    "\n",
    "    buffered_water = pd.concat([filtered_streams, lakes, rivers], ignore_index=True)\n",
    "    buffered_water = buffered_water[buffered_water.geometry.notnull()]\n",
    "    buffered_water['waterID'] = range(1, len(buffered_water) + 1)\n",
    "\n",
    "    if export_merged_filename:\n",
    "        print(f\"üß± Merging and exporting combined buffered layer to {export_merged_filename}...\")\n",
    "        buffered_water.to_file(export_merged_filename)\n",
    "\n",
    "    return filtered_streams, lakes, rivers, full_streams, buffered_water\n",
    "\n",
    "\n",
    "def make_bg_data(\n",
    "    waterbody_gdf,\n",
    "    training_path,\n",
    "    training_state_abbr,\n",
    "    nas_name,\n",
    "    seed=42,\n",
    "    max_attempts=100,\n",
    "    thin=True,\n",
    "    min_dist=1000\n",
    "):\n",
    "    target_crs = \"EPSG:5070\"\n",
    "    \n",
    "    # Auto-load presence data\n",
    "    pos_data_path = f\"{training_path}{training_state_abbr}_{nas_name}_pos_data.shp\"\n",
    "    species_gdf_thin = gpd.read_file(pos_data_path)\n",
    "\n",
    "    # Reproject everything if needed\n",
    "    if waterbody_gdf.crs != target_crs:\n",
    "        waterbody_gdf = waterbody_gdf.to_crs(target_crs)\n",
    "    if species_gdf_thin.crs != target_crs:\n",
    "        species_gdf_thin = species_gdf_thin.to_crs(target_crs)\n",
    "\n",
    "    # Filter waterbodies by area\n",
    "    if 'areasqkm' not in waterbody_gdf.columns:\n",
    "        raise ValueError(\"Expected 'areasqkm' column in waterbody_gdf\")\n",
    "    waterbody_gdf = waterbody_gdf[waterbody_gdf['areasqkm'] > 0.25].copy()\n",
    "\n",
    "    presence_points = species_gdf_thin[species_gdf_thin['Present'] == 1]\n",
    "    sindex = presence_points.sindex\n",
    "\n",
    "    def has_presence(geom):\n",
    "        if geom.is_empty or not geom.is_valid:\n",
    "            return True\n",
    "        bounds = geom.bounds\n",
    "        candidates = list(sindex.intersection(bounds))\n",
    "        return any(presence_points.iloc[i].geometry.intersects(geom) for i in candidates)\n",
    "\n",
    "    non_ais_waterbodies = waterbody_gdf[~waterbody_gdf['geometry'].apply(has_presence)].copy()\n",
    "\n",
    "    random.seed(seed)\n",
    "    bg_points = []\n",
    "    for geom in non_ais_waterbodies.geometry.values:\n",
    "        minx, miny, maxx, maxy = geom.bounds\n",
    "        for _ in range(max_attempts):\n",
    "            x = random.uniform(minx, maxx)\n",
    "            y = random.uniform(miny, maxy)\n",
    "            pt = Point(x, y)\n",
    "            if geom.contains(pt):\n",
    "                bg_points.append(pt)\n",
    "                break\n",
    "\n",
    "    bg_gdf = gpd.GeoDataFrame(geometry=bg_points, crs=target_crs)\n",
    "\n",
    "    if thin and not bg_gdf.empty:\n",
    "        bg_gdf = thin_geodataframe(bg_gdf, min_dist=min_dist)\n",
    "\n",
    "    bg_gdf['Present'] = 0\n",
    "\n",
    "    # Save to shapefile\n",
    "    output_path = f\"{training_path}{training_state_abbr}_{nas_name}_bg_data.shp\"\n",
    "    bg_gdf.to_file(output_path)\n",
    "\n",
    "    return bg_gdf\n",
    "\n",
    "def process_nas_occurrences(\n",
    "    state,\n",
    "    crs,\n",
    "    path,\n",
    "    target_species_id=None,\n",
    "    target_species_name=None,\n",
    "    buffer_shapefile_suffix=\"_buffered_water.shp\",\n",
    "    species_columns=None,\n",
    "    snap_tolerance=100,\n",
    "    make_background=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads, filters, spatially joins, and pivots NAS invasive species data.\n",
    "\n",
    "    Parameters:\n",
    "        state (str): US state abbreviation (e.g., 'MN')\n",
    "        crs (str or int): Target CRS (e.g., 'EPSG:5070')\n",
    "        path (str): Path to save/load shapefiles (must end with '/')\n",
    "        target_species_id (str, optional): SpeciesID to filter for a single species\n",
    "        target_species_name (str, optional): Common name to include in filename if target_species_id is used\n",
    "        buffer_shapefile_suffix (str): Suffix for the buffered waterbodies shapefile\n",
    "        species_columns (list): List of invasive group column names\n",
    "        snap_tolerance (int): Max distance for nearest spatial join in meters\n",
    "        make_background (bool): Whether to generate and save background points\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Final dataframe with invasive species richness per lake\n",
    "    \"\"\"\n",
    "    if species_columns is None:\n",
    "        species_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "\n",
    "    # Step 1: Call NAS API\n",
    "    url = f\"http://nas.er.usgs.gov/api/v2/occurrence/search?state={state}\"\n",
    "    response = requests.get(url, timeout=None).json()\n",
    "    results = pd.json_normalize(response, 'results')\n",
    "\n",
    "    # Step 2: Filter and clean\n",
    "    all_nas_data = results[results['status'] == 'established'].dropna()\n",
    "    all_nas_data = all_nas_data[[\"speciesID\", \"commonName\", \"group\", \"decimalLatitude\", \"decimalLongitude\"]]\n",
    "\n",
    "    # Step 3: Create GeoDataFrame and save\n",
    "    nas_gdf = gpd.GeoDataFrame(\n",
    "        all_nas_data,\n",
    "        geometry=gpd.points_from_xy(all_nas_data.decimalLongitude, all_nas_data.decimalLatitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(crs).drop(columns=[\"decimalLatitude\", \"decimalLongitude\"])\n",
    "\n",
    "    species_gdf = nas_gdf[nas_gdf[\"speciesID\"] == target_species_id]\n",
    "    species_gdf_thin = thin_geodataframe(species_gdf, min_dist=100)\n",
    "    species_gdf_thin['Present'] = 1\n",
    "    species_filename = f\"{path}{state}_{target_species_name}_pos_data.shp\"\n",
    "    species_gdf_thin.to_file(species_filename)\n",
    "\n",
    "    nas_gdf = nas_gdf[nas_gdf[\"speciesID\"] != target_species_id]\n",
    "    nas_gdf.to_file(f\"{path}{state}_nas.shp\")\n",
    "\n",
    "    nas_gdf = gpd.read_file(f\"{path}{state}_nas.shp\").to_crs(crs)\n",
    "    buffered_water = gpd.read_file(f\"{path}{state}{buffer_shapefile_suffix}\").to_crs(crs)\n",
    "\n",
    "    NAS_ais_obs_df = gpd.sjoin_nearest(nas_gdf[['speciesID', 'commonName', 'group', 'geometry']],\n",
    "                                       buffered_water, how=\"inner\", max_distance=snap_tolerance)\n",
    "\n",
    "    NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace({\n",
    "        'Algae': 'Inv_Algae',\n",
    "        'Plants': 'Inv_Plants',\n",
    "        'Fishes': 'Inv_Fish',\n",
    "        'Crustaceans-Cladocerans': 'Inv_Crustaceans',\n",
    "        'Crustaceans-Amphipods': 'Inv_Crustaceans',\n",
    "        'Mollusks-Bivalves': 'Inv_Mollusks',\n",
    "        'Mollusks-Gastropods': 'Inv_Mollusks'\n",
    "    })\n",
    "\n",
    "    grouped = NAS_ais_obs_df[['waterID', 'commonName', 'group']].groupby(['waterID', 'group'])['commonName'].nunique().reset_index()\n",
    "    pivot_df = grouped.pivot(index='waterID', columns='group', values='commonName').reset_index().fillna(0)\n",
    "    lakes_w_invasives = pd.merge(buffered_water, pivot_df, on='waterID', how='left')\n",
    "\n",
    "    for col in species_columns:\n",
    "        if col not in lakes_w_invasives.columns:\n",
    "            lakes_w_invasives[col] = 0.0\n",
    "        else:\n",
    "            lakes_w_invasives[col] = lakes_w_invasives[col].astype('float64')\n",
    "\n",
    "    inv_rich = lakes_w_invasives[species_columns + ['geometry']].copy()\n",
    "    inv_gdf = gpd.GeoDataFrame(inv_rich, geometry='geometry', crs=lakes_w_invasives.crs)\n",
    "\n",
    "    # Step 11: Optional background generation\n",
    "    if make_background:\n",
    "        make_bg_data(\n",
    "            waterbody_gdf=buffered_water,\n",
    "            training_path=path,\n",
    "            training_state_abbr=state,\n",
    "            nas_name=target_species_name,\n",
    "            seed=42,\n",
    "            max_attempts=100,\n",
    "            thin=True,\n",
    "            min_dist=1000\n",
    "        )\n",
    "\n",
    "    return inv_gdf\n",
    "\n",
    "\n",
    "def export_inv_richness(inv_rich_gdf, output_path):\n",
    "    # Convert input data to EPSG:5070 BEFORE getting bounds\n",
    "    inv_rich_gdf = inv_rich_gdf\n",
    "\n",
    "    data_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "\n",
    "    # Ensure valid geometry\n",
    "    inv_rich_gdf = inv_rich_gdf[inv_rich_gdf.geometry.notnull() & inv_rich_gdf.geometry.is_valid]\n",
    "\n",
    "    # Ensure all required columns exist\n",
    "    for col in data_columns:\n",
    "        if col not in inv_rich_gdf.columns:\n",
    "            inv_rich_gdf[col] = 0\n",
    "    \n",
    "    # Warn if file exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Warning: {output_path} already exists and will be overwritten.\")\n",
    "\n",
    "    if inv_rich_gdf.empty:\n",
    "        raise ValueError(\"Error: The input GeoDataFrame is empty!\")\n",
    "\n",
    "    # Get bounding box in CRS 5070\n",
    "    xmin, ymin, xmax, ymax = inv_rich_gdf.total_bounds  \n",
    "    pixel_size = 100 \n",
    "\n",
    "    # Ensure bounds are valid\n",
    "    if xmin == xmax:\n",
    "        xmin -= pixel_size\n",
    "        xmax += pixel_size\n",
    "    if ymin == ymax:\n",
    "        ymin -= pixel_size\n",
    "        ymax += pixel_size\n",
    "\n",
    "    # Recalculate width and height\n",
    "    width = max(1, int((xmax - xmin) / pixel_size))\n",
    "    height = max(1, int((ymax - ymin) / pixel_size))\n",
    "\n",
    "    # Correct transform in CRS 5070\n",
    "    transform = Affine(pixel_size, 0, xmin, 0, -pixel_size, ymax)\n",
    "    num_bands = len(data_columns) + 1  # Extra band for sum\n",
    "    raster = np.zeros((num_bands, height, width), dtype=np.float32)\n",
    "\n",
    "    # Debug: Check valid geometries\n",
    "    valid_geom_count = inv_rich_gdf.geometry.notnull().sum()\n",
    "    print(f\"Valid geometries count: {valid_geom_count}\")\n",
    "\n",
    "    # Rasterize each data column\n",
    "    for i, column in enumerate(data_columns):\n",
    "        shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column]) if geom is not None and not np.isnan(value)]\n",
    "        if not shapes:\n",
    "            print(f\"Skipping {column}: No valid geometries!\")\n",
    "            continue  # Skip empty bands\n",
    "        raster_band = rasterize(\n",
    "            shapes,\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype='float32',\n",
    "            default_value=1,  # Ensure full coverage of waterbody\n",
    "            all_touched=True  # Ensures every pixel within the geometry gets a value\n",
    "        )\n",
    "        shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column])]\n",
    "        raster[i] = raster_band  # Keep original values\n",
    "        print(f\"Rasterized {column}: Min={raster_band.min()}, Max={raster_band.max()}, Non-zero pixels={np.count_nonzero(raster_band)}\")\n",
    "\n",
    "    # Compute the sum band\n",
    "    raster[-1] = np.sum(raster[:-1], axis=0)\n",
    "    print(f\"Final Sum Band: Min={raster[-1].min()}, Max={raster[-1].max()}, Non-zero pixels={np.count_nonzero(raster[-1])}\")\n",
    "\n",
    "    # Save as a multi-band GeoTIFF in EPSG:5070\n",
    "    with rasterio.open(\n",
    "        output_path, 'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=num_bands,\n",
    "        dtype=raster.dtype,\n",
    "        crs=\"EPSG:5070\",\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        for band in range(num_bands):\n",
    "            dst.write(raster[band], band + 1)\n",
    "        band_names = data_columns + [\"Inv_Richness\"]\n",
    "        for band, name in enumerate(band_names, start=1):\n",
    "            dst.set_band_description(band, name)\n",
    "\n",
    "    # Debug: Plot overlay of geometries on raster\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.imshow(raster[-1], cmap='viridis', extent=[xmin, xmax, ymin, ymax], origin=\"upper\")\n",
    "    gpd.GeoSeries(inv_rich_gdf.geometry).plot(ax=ax, facecolor=\"none\", edgecolor=\"red\")\n",
    "    plt.title(\"Overlaying Geometries on Raster\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot bands\n",
    "    fig, axes = plt.subplots(1, num_bands, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(raster[i], cmap='viridis')\n",
    "        ax.set_title(band_names[i])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Raster file saved as '{output_path}' in CRS {\"EPSG:5070\"} with band names: {band_names}\")\n",
    "\n",
    "def process_and_export_native_fish_raster(\n",
    "    my_path: str,\n",
    "    my_crs: str,\n",
    "    state_abbr: str,\n",
    "    resolution: int = 100,\n",
    "    fish_shapefile: str = 'usgs_native_fish_rich.shp',\n",
    "    state_shapefile: str = 'tl_2012_us_state.shp',\n",
    "    buffer_shapefile_suffix: str = '_buffered_water.shp'\n",
    "):\n",
    "    def clip_points_by_polygon(points_gdf, polygon_gdf):\n",
    "        if points_gdf.crs != polygon_gdf.crs:\n",
    "            points_gdf = points_gdf.to_crs(polygon_gdf.crs)\n",
    "        clipped_points = gpd.sjoin(points_gdf, polygon_gdf, how='inner')\n",
    "        return clipped_points.drop(columns=polygon_gdf.columns.difference(['geometry']))\n",
    "\n",
    "    def sum_numeric_columns(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "        sum_columns = [col for col in gdf.columns if col.isdigit() and len(col) == 6]\n",
    "        gdf[\"Native_Fish_Richness\"] = gdf[sum_columns].sum(axis=1)\n",
    "        return gdf[[\"Native_Fish_Richness\", \"geometry\"]]\n",
    "\n",
    "    def spatial_join_with_nearest(poly_gdf: gpd.GeoDataFrame, point_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "        if poly_gdf.crs != point_gdf.crs:\n",
    "            poly_gdf = poly_gdf.to_crs(point_gdf.crs)\n",
    "        return gpd.sjoin_nearest(poly_gdf, point_gdf, how=\"left\", distance_col=\"distance\")\n",
    "\n",
    "    def export_native_raster(joined_gdf: gpd.GeoDataFrame, my_path, state_abbr, column_name: str = \"Native_Fish_Richness\"):\n",
    "        bounds = joined_gdf.total_bounds\n",
    "        transform = rasterio.transform.from_origin(bounds[0], bounds[3], resolution, resolution)\n",
    "        out_shape = (\n",
    "            int(np.ceil((bounds[3] - bounds[1]) / resolution)),\n",
    "            int(np.ceil((bounds[2] - bounds[0]) / resolution))\n",
    "        )\n",
    "\n",
    "        raster = rasterize(\n",
    "            [(geom, value) for geom, value in zip(joined_gdf.geometry, joined_gdf[column_name])],\n",
    "            out_shape=out_shape,\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype=rasterio.float32\n",
    "        )\n",
    "\n",
    "        parent_dir = os.path.abspath(os.path.join(my_path, os.pardir))\n",
    "        output_filename = os.path.join(my_path, f\"{state_abbr}_{column_name}.tif\")\n",
    "\n",
    "        with rasterio.open(\n",
    "            output_filename, \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=out_shape[0],\n",
    "            width=out_shape[1],\n",
    "            count=1,\n",
    "            dtype=rasterio.float32,\n",
    "            crs=\"EPSG:5070\",\n",
    "            transform=transform\n",
    "        ) as dst:\n",
    "            dst.write(raster, 1)\n",
    "            dst.set_band_description(1, column_name)\n",
    "\n",
    "        # Optional plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(raster, cmap='viridis', extent=[bounds[0], bounds[2], bounds[1], bounds[3]])\n",
    "        plt.colorbar(label=f'{column_name} Richness')\n",
    "        plt.title('Rasterized Native Fish Richness')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.show()\n",
    "\n",
    "    # Load or download native fish data\n",
    "    fish_path = os.path.join(my_path, fish_shapefile)\n",
    "    if os.path.exists(fish_path):\n",
    "        fish_gdf = gpd.read_file(fish_path).to_crs(my_crs)\n",
    "    else:\n",
    "        url = \"https://www.sciencebase.gov/catalog/file/get/6086df60d34eadd49d31b04a?f=__disk__ad%2F21%2Ffc%2Fad21fc677379f4e45caa4bd506ca1c587d5f01f7\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            csv_data = StringIO(response.text)\n",
    "            fish_df = pd.read_csv(csv_data)\n",
    "            fish_gdf = gpd.GeoDataFrame(\n",
    "                fish_df,\n",
    "                geometry=gpd.points_from_xy(fish_df.longitude, fish_df.latitude),\n",
    "                crs=\"EPSG:4326\"\n",
    "            ).to_crs(my_crs)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Failed to download native fish data: {response.status_code}\")\n",
    "\n",
    "    # Load state boundary and buffered water\n",
    "    state_boundary = gpd.read_file(os.path.join(my_path, state_shapefile)).dropna().to_crs(my_crs)\n",
    "    state_boundary = state_boundary[state_boundary['STUSPS'] == state_abbr]\n",
    "    buffered_water_path = os.path.join(my_path, state_abbr + buffer_shapefile_suffix)\n",
    "    buffered_water = gpd.read_file(buffered_water_path).to_crs(my_crs)\n",
    "\n",
    "    # Process native fish richness\n",
    "    clipped_fish = clip_points_by_polygon(fish_gdf, state_boundary)\n",
    "    native_fish_gdf = sum_numeric_columns(clipped_fish)\n",
    "    water_with_fish = spatial_join_with_nearest(buffered_water, native_fish_gdf)\n",
    "\n",
    "    # Convert int columns to float\n",
    "    for col in water_with_fish.select_dtypes(include=['int64']).columns:\n",
    "        water_with_fish[col] = water_with_fish[col].astype('float64')\n",
    "\n",
    "    export_native_raster(water_with_fish, my_path, state_abbr)\n",
    "\n",
    "\n",
    "def combine_geotiffs(input_files, output_file):\n",
    "    \"\"\"\n",
    "    Combine single- or multi-band GeoTIFFs into one multi-band GeoTIFF,\n",
    "    preserving band names where available.\n",
    "\n",
    "    Parameters:\n",
    "        input_files (list): List of input raster file paths.\n",
    "        output_file (str): Path to output multi-band GeoTIFF.\n",
    "        state (str): Optional metadata tag (not used in core logic).\n",
    "    \"\"\"\n",
    "    all_bands = []\n",
    "    band_descriptions = []\n",
    "    meta = None\n",
    "\n",
    "    for file in input_files:\n",
    "        try:\n",
    "            with rasterio.open(file) as src:\n",
    "                if meta is None:\n",
    "                    meta = src.meta.copy()\n",
    "                    meta.update({\n",
    "                        'count': 0,  # Will be updated after reading all bands\n",
    "                        'dtype': src.dtypes[0]  # Assumes consistent dtype\n",
    "                    })\n",
    "\n",
    "                for bidx in range(1, src.count + 1):\n",
    "                    band_data = src.read(bidx)\n",
    "                    all_bands.append(band_data)\n",
    "\n",
    "                    # Try to get band description (e.g., band name)\n",
    "                    desc = src.descriptions[bidx - 1] or f'band_{bidx}'\n",
    "                    band_descriptions.append(desc)\n",
    "\n",
    "        except RasterioIOError:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not read {file}, skipping...\")\n",
    "\n",
    "    if not all_bands:\n",
    "        raise RuntimeError(\"‚ùå No valid input rasters found.\")\n",
    "\n",
    "    meta['count'] = len(all_bands)\n",
    "\n",
    "    with rasterio.open(output_file, 'w', **meta) as dst:\n",
    "        for idx, band in enumerate(all_bands, start=1):\n",
    "            dst.write(band, idx)\n",
    "            if band_descriptions[idx - 1]:\n",
    "                dst.set_band_description(idx, band_descriptions[idx - 1])\n",
    "\n",
    "def extract_roads_and_endpoints(\n",
    "    local_path,\n",
    "    state_abbr,\n",
    "    state_fips,\n",
    "    state_name,\n",
    "    buffered_water,\n",
    "    nas_id,\n",
    "    my_crs,\n",
    "    snap_dist=500\n",
    "):\n",
    "    def download_tiger_roads(state_fips, local_path):\n",
    "        url = f'https://www2.census.gov/geo/tiger/TIGER2022/PRISECROADS/tl_2022_{state_fips}_prisecroads.zip'\n",
    "        r = requests.get(url)\n",
    "        if r.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to download from {url}\")\n",
    "        z = zipfile.ZipFile(BytesIO(r.content))\n",
    "        z.extractall(path=local_path)\n",
    "\n",
    "    def download_ramps(local_path):\n",
    "        url = \"https://www.sciencebase.gov/catalog/file/get/63b81b50d34e92aad3cc004d?facet=Boatramps_United_States_final_20230104\"\n",
    "        extensions = {'.dbf', '.prj', '.shp', '.shx'}\n",
    "        response = requests.get(url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "            for f in zip_ref.namelist():\n",
    "                if os.path.splitext(f)[1].lower() in extensions:\n",
    "                    zip_ref.extract(f, local_path)\n",
    "\n",
    "    def get_endpoints(geometry):\n",
    "        if geometry.geom_type == 'LineString':\n",
    "            return [geometry.coords[0], geometry.coords[-1]]\n",
    "        return []\n",
    "\n",
    "    def nas_api_call(nas_id, state_abbr):\n",
    "        url = f\"http://nas.er.usgs.gov/api/v2/occurrence/search?species_ID={nas_id}&state={state_abbr}\"\n",
    "        response = requests.get(url).json()\n",
    "        return pd.json_normalize(response, 'results')\n",
    "\n",
    "    def sjoin_nearest_to_centroid_replace_geom(left_gdf, right_gdf, **kwargs):\n",
    "        left_centroids = left_gdf.copy()\n",
    "        left_centroids[\"geometry_centroid\"] = left_centroids.geometry.centroid\n",
    "        left_centroids = left_centroids.set_geometry(\"geometry_centroid\")\n",
    "        right_temp = right_gdf.copy()\n",
    "        right_temp[\"geometry_right\"] = right_temp.geometry\n",
    "        right_temp = right_temp.set_geometry(\"geometry_right\")\n",
    "        joined = gpd.sjoin_nearest(left_centroids, right_temp, how=\"left\", **kwargs)\n",
    "        joined[\"distance\"] = joined.geometry_centroid.distance(joined[\"geometry_right\"])\n",
    "        result = left_gdf.copy()\n",
    "        result[\"geometry\"] = joined[\"geometry_right\"]\n",
    "        result[\"epointID\"] = joined[\"epointID\"]\n",
    "        result[\"distance_to_nearest\"] = joined[\"distance\"]\n",
    "        return result\n",
    "\n",
    "    # Step 1: Download ramps and roads\n",
    "    download_ramps(os.path.abspath(os.path.join(local_path)))\n",
    "    download_tiger_roads(state_fips, local_path)\n",
    "\n",
    "    # Step 2: Load data\n",
    "    ramps = gpd.read_file(local_path + 'Boatramps_United_States_final_20230104.shp').set_crs(my_crs, allow_override=True)\n",
    "    ramp_geo = ramps.loc[ramps['State'] == state_name, ['geometry']].copy()\n",
    "    ramp_geo['ramp_ID'] = range(1, len(ramp_geo) + 1)\n",
    "\n",
    "    buffered_water = buffered_water.to_crs(my_crs)\n",
    "    pos_data = nas_api_call(nas_id, state_abbr)\n",
    "    pos_gdf = gpd.GeoDataFrame(\n",
    "        pos_data[[\"decimalLatitude\", \"decimalLongitude\"]],\n",
    "        geometry=gpd.points_from_xy(pos_data.decimalLongitude, pos_data.decimalLatitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(my_crs)\n",
    "\n",
    "    # Step 3: Identify water presence/absence\n",
    "    water_check = buffered_water.sjoin(pos_gdf, how=\"left\", predicate=\"contains\")\n",
    "    water_check = water_check.drop_duplicates(subset=\"waterID\")\n",
    "    neg_water = water_check[water_check['index_right'].isna()].drop(columns=[\"index_right\", \"decimalLatitude\", \"decimalLongitude\"], errors=\"ignore\")\n",
    "    pos_water = water_check.dropna(subset=[\"index_right\"]).drop(columns=[\"index_right\", \"decimalLatitude\", \"decimalLongitude\"], errors=\"ignore\")\n",
    "    pos_water[\"Present\"], neg_water[\"Present\"] = 1.0, 0.0\n",
    "    water_with_presence = pd.concat([pos_water, neg_water])\n",
    "\n",
    "    # Step 4: Match ramps to water\n",
    "    ramps_in_water = ramp_geo.sjoin(water_with_presence, how=\"left\", predicate=\"within\")\n",
    "    ramps_not_in_water = ramps_in_water[ramps_in_water['index_right'].isna()].drop(columns=[\"index_right\"], errors=\"ignore\").copy()\n",
    "    ramps_in_water = ramps_in_water.dropna(subset=[\"index_right\"]).drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    ramps_in_water[\"waterID\"] = ramps_in_water[\"waterID\"].astype(\"int64\")\n",
    "\n",
    "    water_ids_with_ramps = ramps_in_water['waterID'].tolist()\n",
    "    water_no_ramps = water_with_presence[~water_with_presence['waterID'].isin(water_ids_with_ramps)]\n",
    "\n",
    "    # Step 5: Extract and deduplicate road endpoints\n",
    "    my_roads = gpd.read_file(os.path.join(local_path, f\"tl_2022_{state_fips}_prisecroads.shp\")).to_crs(my_crs)\n",
    "    endpoints = my_roads['geometry'].apply(get_endpoints).explode()\n",
    "    endpoints_df = pd.DataFrame(endpoints.tolist(), columns=['x', 'y']).drop_duplicates()\n",
    "    endpoints_gdf = gpd.GeoDataFrame(endpoints_df, geometry=gpd.points_from_xy(endpoints_df['x'], endpoints_df['y']), crs=my_crs).drop(columns=['x', 'y'])\n",
    "    endpoints_gdf['epointID'] = range(1, len(endpoints_gdf) + 1)\n",
    "\n",
    "    # Step 6: Snap ramps and lakes to endpoints\n",
    "    ramps_in_water_sj = sjoin_nearest_to_centroid_replace_geom(ramps_in_water, endpoints_gdf)\n",
    "    lakes_no_ramp_sj = sjoin_nearest_to_centroid_replace_geom(water_no_ramps, endpoints_gdf)\n",
    "    lakes_no_ramp_epoints = lakes_no_ramp_sj[['waterID', 'geometry', 'Present', 'epointID', 'distance_to_nearest']].dropna()\n",
    "    ramps_in_water_epoints = ramps_in_water_sj[['waterID', 'geometry', 'Present', 'epointID', 'distance_to_nearest']].dropna()\n",
    "    all_endpoints = pd.concat([ramps_in_water_epoints, lakes_no_ramp_epoints])\n",
    "    pos_endpoints = all_endpoints.loc[all_endpoints['Present'] == 1.0]\n",
    "    neg_endpoints = all_endpoints.loc[all_endpoints['Present'] == 0.0]\n",
    "\n",
    "    return my_roads, pos_endpoints, neg_endpoints, ramps_in_water_epoints, lakes_no_ramp_epoints\n",
    "\n",
    "def build_network_optimized(road_network_gdf, precision=6):\n",
    "    \"\"\"\n",
    "    Build a NetworkX graph from a GeoDataFrame of road LineStrings.\n",
    "\n",
    "    Nodes are rounded to a given precision to reduce floating-point redundancy.\n",
    "\n",
    "    Args:\n",
    "        road_network_gdf (GeoDataFrame): Must contain LineString or MultiLineString geometries.\n",
    "        precision (int): Number of decimal places to round coordinates to for node deduplication.\n",
    "\n",
    "    Returns:\n",
    "        networkx.Graph: Graph with nodes as (x, y) tuples and edges weighted by length.\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "    from shapely.geometry import LineString\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for geom in road_network_gdf.geometry:\n",
    "        if geom is None:\n",
    "            continue\n",
    "\n",
    "        # Handle both LineString and MultiLineString\n",
    "        if isinstance(geom, LineString):\n",
    "            lines = [geom]\n",
    "        elif hasattr(geom, 'geoms'):  # MultiLineString\n",
    "            lines = list(geom.geoms)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for line in lines:\n",
    "            coords = list(line.coords)\n",
    "            for u, v in zip(coords[:-1], coords[1:]):\n",
    "                if len(u) < 2 or len(v) < 2:\n",
    "                    continue  # skip invalid coordinate pairs\n",
    "\n",
    "                u_rounded = tuple(round(c, precision) for c in u[:2])\n",
    "                v_rounded = tuple(round(c, precision) for c in v[:2])\n",
    "                dist = LineString([u, v]).length\n",
    "\n",
    "                # Add undirected edge with distance as weight\n",
    "                G.add_edge(u_rounded, v_rounded, weight=dist)\n",
    "\n",
    "    return G\n",
    "\n",
    "def build_nearest_node_index(graph):\n",
    "    \"\"\"\n",
    "    Build a KDTree for efficient nearest-node lookup in a NetworkX graph.\n",
    "\n",
    "    Args:\n",
    "        graph (networkx.Graph): A graph with 2D tuple coordinates as nodes.\n",
    "\n",
    "    Returns:\n",
    "        cKDTree: KD-tree built from graph node coordinates.\n",
    "        list: List of node tuples in the same order as the KD-tree.\n",
    "    \"\"\"\n",
    "    # Ensure all nodes are 2D points (x, y)\n",
    "    nodes = [node for node in graph.nodes if len(node) == 2]\n",
    "    coords = [list(node) for node in nodes]  # convert tuples to lists for KDTree\n",
    "\n",
    "    tree = cKDTree(coords)\n",
    "    return tree, nodes\n",
    "    \n",
    "def find_nearest_node_kd_tree(kd_tree, point, precision=6):\n",
    "    \"\"\"\n",
    "    Find the nearest graph node to a given point using a KD-tree.\n",
    "\n",
    "    Args:\n",
    "        kd_tree (cKDTree): KDTree built from graph node coordinates.\n",
    "        point (shapely.geometry.Point): Point to find the nearest node to.\n",
    "        precision (int): Decimal places to round the coordinates.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The (x, y) coordinates of the nearest node, rounded.\n",
    "    \"\"\"\n",
    "    _, index = kd_tree.query([point.x, point.y])\n",
    "    nearest_coords = kd_tree.data[index]\n",
    "    return tuple(round(coord, precision) for coord in nearest_coords)\n",
    "    \n",
    "def assign_endpoints_multi_source_dijkstra(presences_gdf, endpoints_gdf, G, kd_tree, precision=6):\n",
    "    \"\"\"\n",
    "    Assign each endpoint to its nearest presence based on road network distance.\n",
    "\n",
    "    Args:\n",
    "        presences_gdf (GeoDataFrame): GeoDataFrame of presence points with 'epointID'.\n",
    "        endpoints_gdf (GeoDataFrame): GeoDataFrame of negative endpoints with 'epointID'.\n",
    "        G (networkx.Graph): Graph of road network.\n",
    "        kd_tree (cKDTree): KDTree built from graph nodes.\n",
    "        precision (int): Decimal precision for rounding coordinates.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: Each dict contains 'epointID', 'target_point_id', and 'distance_roads'.\n",
    "    \"\"\"\n",
    "    # Step 1: Snap presence points to graph nodes\n",
    "    presence_nodes = {}\n",
    "    node_to_presence_id = {}\n",
    "\n",
    "    for _, row in tqdm(presences_gdf.iterrows(), total=len(presences_gdf), desc=\"Snapping presences\"):\n",
    "        node = find_nearest_node_kd_tree(kd_tree, row.geometry.centroid, precision)\n",
    "        if not G.has_node(node):\n",
    "            continue  # Skip if the node isn't actually in the graph\n",
    "        presence_nodes[row[\"epointID\"]] = node\n",
    "        node_to_presence_id[node] = row[\"epointID\"]\n",
    "\n",
    "    if not presence_nodes:\n",
    "        raise ValueError(\"No presence points could be snapped to graph nodes.\")\n",
    "\n",
    "    # Step 2: Multi-source Dijkstra from all presence nodes\n",
    "    source_nodes = list(presence_nodes.values())\n",
    "    distances, predecessors = nx.multi_source_dijkstra(G, sources=source_nodes, weight='weight')\n",
    "\n",
    "    # Step 3: For each negative endpoint, find the closest presence\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(endpoints_gdf.iterrows(), total=len(endpoints_gdf), desc=\"Processing negatives\"):\n",
    "        neg_id = row[\"epointID\"]\n",
    "        neg_point = row.geometry.centroid\n",
    "        neg_node = find_nearest_node_kd_tree(kd_tree, neg_point, precision)\n",
    "\n",
    "        if not G.has_node(neg_node):\n",
    "            results.append({\n",
    "                \"epointID\": neg_id,\n",
    "                \"target_point_id\": None,\n",
    "                \"distance_roads\": np.inf\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        if neg_node in source_nodes:\n",
    "            # Direct match to a presence node\n",
    "            dist = 0.0\n",
    "            target_id = node_to_presence_id[neg_node]\n",
    "        elif neg_node in distances:\n",
    "            # Trace back the path to the nearest source\n",
    "            path = []\n",
    "            current = neg_node\n",
    "            while current not in source_nodes:\n",
    "                prev = predecessors.get(current)\n",
    "                if not prev:\n",
    "                    break\n",
    "                current = prev[0]  # Multi-source returns list of predecessors\n",
    "                path.append(current)\n",
    "\n",
    "            source_node = current if current in source_nodes else None\n",
    "            target_id = node_to_presence_id.get(source_node, None)\n",
    "            dist = distances[neg_node]\n",
    "        else:\n",
    "            # Not reachable\n",
    "            dist = np.inf\n",
    "            target_id = None\n",
    "\n",
    "        results.append({\n",
    "            \"epointID\": neg_id,\n",
    "            \"target_point_id\": target_id,\n",
    "            \"distance_roads\": dist\n",
    "        })\n",
    "\n",
    "    return results\n",
    "    \n",
    "def vector_to_raster(gdf, output_path, nas_name, value_field=None, resolution=100):\n",
    "    # Ensure geometries are valid\n",
    "    gdf = gdf[~gdf.geometry.isna()]\n",
    "    gdf = gdf[gdf.geometry.is_valid]\n",
    "\n",
    "    # Ensure it has a CRS\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"GeoDataFrame must have a CRS.\")\n",
    "\n",
    "    # Reproject to projected CRS if necessary\n",
    "    if gdf.crs.is_geographic:\n",
    "        gdf = gdf.to_crs(\"EPSG:5070\")  # NAD83 / Conus Albers (meters)\n",
    "\n",
    "    # Get bounds\n",
    "    minx, miny, maxx, maxy = gdf.total_bounds\n",
    "    width = int(np.ceil((maxx - minx) / resolution))\n",
    "    height = int(np.ceil((maxy - miny) / resolution))\n",
    "\n",
    "    # Check again\n",
    "    if width <= 0 or height <= 0:\n",
    "        raise ValueError(\"Calculated width or height is <= 0. Check your resolution and geometry bounds.\")\n",
    "\n",
    "    # Create transform\n",
    "    transform = from_origin(minx, maxy, resolution, resolution)\n",
    "\n",
    "    # Prepare shapes\n",
    "    shapes = (\n",
    "        ((geom, value) for geom, value in zip(gdf.geometry, gdf[value_field]))\n",
    "        if value_field else\n",
    "        ((geom, 1) for geom in gdf.geometry)\n",
    "    )\n",
    "\n",
    "    # Rasterize\n",
    "    raster = rasterize(\n",
    "        shapes=shapes,\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill= np.nan,\n",
    "        dtype='float32' if not value_field else 'float32'\n",
    "    )\n",
    "\n",
    "    # Write to GeoTIFF\n",
    "    band_name = f\"distance_road_{nas_name}\"\n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=raster.dtype,\n",
    "        crs=gdf.crs,\n",
    "        transform=transform,\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "        dst.set_band_description(1, band_name)\n",
    "\n",
    "    print(f\"Raster written to {output_path}\")\n",
    "\n",
    "def compute_and_export_road_distances(\n",
    "    my_roads: gpd.GeoDataFrame,\n",
    "    pos_endpoints: gpd.GeoDataFrame,\n",
    "    neg_endpoints: gpd.GeoDataFrame,\n",
    "    ramps_epoints: gpd.GeoDataFrame,\n",
    "    lakes_epoints: gpd.GeoDataFrame,\n",
    "    buffered_water: gpd.GeoDataFrame,\n",
    "    my_crs: str,\n",
    "    my_path: str,\n",
    "    my_state: str,\n",
    "    nas_name: str,\n",
    "    resolution: int = 100\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Computes shortest road network distances from invasive presence points to absence points,\n",
    "    joins those distances to ramp/lake endpoints, and outputs a raster and plot.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Waterbodies with distance_roads_total assigned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure CRS consistency\n",
    "    my_roads = my_roads.to_crs(my_crs)\n",
    "    pos_endpoints = pos_endpoints.to_crs(my_crs)\n",
    "    neg_endpoints = neg_endpoints.to_crs(my_crs)\n",
    "\n",
    "    # Build the road network and nearest node index\n",
    "    G = build_network_optimized(my_roads)\n",
    "    kd_tree, graph_nodes = build_nearest_node_index(G)\n",
    "\n",
    "    # Run Dijkstra from positive to negative endpoints\n",
    "    dijkstra_results = assign_endpoints_multi_source_dijkstra(\n",
    "        presences_gdf=pos_endpoints,\n",
    "        endpoints_gdf=neg_endpoints,\n",
    "        G=G,\n",
    "        kd_tree=kd_tree,\n",
    "        precision=6\n",
    "    )\n",
    "    dist_df = pd.DataFrame(dijkstra_results)\n",
    "    dist_df = dist_df.loc[dist_df.groupby('epointID')['distance_roads'].idxmin()].reset_index(drop=True)\n",
    "\n",
    "    # Merge distances to endpoints\n",
    "    ramps_w_dist = pd.merge(ramps_epoints, dist_df, on='epointID', how='left')\n",
    "    lakes_w_dist = pd.merge(lakes_epoints, dist_df, on='epointID', how='left')\n",
    "\n",
    "    # Clean and select needed columns\n",
    "    keep_cols = ['geometry', 'target_point_id', 'epointID', 'waterID', 'Present', 'distance_roads', 'distance_to_nearest']\n",
    "    ramps_clean = ramps_w_dist[keep_cols].dropna()\n",
    "    lakes_clean = lakes_w_dist[keep_cols].dropna()\n",
    "\n",
    "    # Downcast numeric columns\n",
    "    for df in [ramps_clean, lakes_clean]:\n",
    "        for col in ['distance_roads', 'distance_to_nearest']:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "    # Fill missing distances\n",
    "    ramps_clean['adjusted_distance_to_nearest'] = ramps_clean['distance_to_nearest']\n",
    "    lakes_clean['adjusted_distance_to_nearest'] = lakes_clean['distance_to_nearest']\n",
    "\n",
    "    ramps_clean['distance_roads_filled'] = ramps_clean['distance_roads'].fillna(0)\n",
    "    lakes_clean['distance_roads_filled'] = lakes_clean['distance_roads'].fillna(0)\n",
    "\n",
    "    # Compute total distances\n",
    "    ramps_clean['distance_roads_total'] = 0\n",
    "    ramps_clean.loc[ramps_clean['Present'] == 0, 'distance_roads_total'] = (\n",
    "        ramps_clean['distance_roads'] + ramps_clean['adjusted_distance_to_nearest']\n",
    "    )\n",
    "    ramps_clean.loc[ramps_clean['Present'] != 0, 'distance_roads_total'] = ramps_clean['distance_roads_filled']\n",
    "\n",
    "    lakes_clean['distance_roads_total'] = 0\n",
    "    lakes_clean.loc[lakes_clean['Present'] == 0, 'distance_roads_total'] = (\n",
    "        lakes_clean['distance_roads'] + lakes_clean['adjusted_distance_to_nearest']\n",
    "    )\n",
    "    lakes_clean.loc[lakes_clean['Present'] != 0, 'distance_roads_total'] = lakes_clean['distance_roads_filled']\n",
    "\n",
    "    # Clean up intermediate columns\n",
    "    ramps_clean.drop(columns=['distance_roads_filled'], inplace=True)\n",
    "    lakes_clean.drop(columns=['distance_roads_filled'], inplace=True)\n",
    "\n",
    "    # Final merge and assignment\n",
    "    final_dist_df = pd.concat([ramps_clean, lakes_clean], ignore_index=True)\n",
    "    min_dist_by_water = final_dist_df.loc[final_dist_df.groupby(\"waterID\")[\"distance_roads_total\"].idxmin()]\n",
    "    water_w_dist = pd.merge(buffered_water, min_dist_by_water, on=\"waterID\", how=\"left\")\n",
    "    water_w_dist_final = water_w_dist[[\"waterID\", \"geometry_x\", \"distance_roads_total\"]].rename(columns={\"geometry_x\": \"geometry\"}).dropna()\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    water_w_dist_final.plot(\n",
    "        ax=ax, column=\"distance_roads_total\", cmap=\"viridis\", linewidth=0.5, legend=True\n",
    "    )\n",
    "    ax.set_title(\"Water with Distance to Roads\", fontsize=14)\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    plt.show()\n",
    "\n",
    "    # Export raster\n",
    "    output_filename = f\"{my_path}{my_state}_road_distance_{nas_name}.tif\"\n",
    "    vector_to_raster(\n",
    "        gdf=water_w_dist_final,\n",
    "        output_path=output_filename,\n",
    "        nas_name=nas_name,  # must match function signature\n",
    "        value_field=\"distance_roads_total\",\n",
    "        resolution=resolution\n",
    "    )\n",
    "\n",
    "    return water_w_dist_final\n",
    "\n",
    "# Distance to source streams\n",
    "@contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_callback\n",
    "        tqdm_object.close()\n",
    "def build_network(streams_gdf):\n",
    "    G = nx.DiGraph()\n",
    "    for geom in streams_gdf.geometry:\n",
    "        if geom is None or geom.is_empty:\n",
    "            continue\n",
    "\n",
    "        if isinstance(geom, LineString):\n",
    "            coords = list(geom.coords)\n",
    "            edges = zip(coords[:-1], coords[1:])\n",
    "        elif isinstance(geom, MultiLineString):\n",
    "            edges = []\n",
    "            for line in geom:\n",
    "                coords = list(line.coords)\n",
    "                edges.extend(zip(coords[:-1], coords[1:]))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for u, v in edges:\n",
    "            u_r = (round(u[0], 6), round(u[1], 6))\n",
    "            v_r = (round(v[0], 6), round(v[1], 6))\n",
    "            if u_r != v_r:\n",
    "                G.add_edge(u_r, v_r, weight=Point(u_r).distance(Point(v_r)))\n",
    "    return G\n",
    "\n",
    "def snap_points_to_vertices(points_gdf, vertices_tree, vertices_coords):\n",
    "    coords = np.array([(geom.x, geom.y) for geom in points_gdf.geometry])\n",
    "    _, idxs = vertices_tree.query(coords, k=1)\n",
    "    snapped_points = vertices_coords[idxs]\n",
    "    return snapped_points, idxs\n",
    "def calculate_dijkstra_for_point(source_node, G, vertices_in_buffer):\n",
    "    try:\n",
    "        lengths = nx.single_source_dijkstra_path_length(G, source_node, weight='weight')\n",
    "    except nx.NodeNotFound:\n",
    "        return np.full(len(vertices_in_buffer), discon_val, dtype=np.float32)\n",
    "\n",
    "    vertex_distances = np.full(len(vertices_in_buffer), big_value, dtype=np.float32)\n",
    "    for j, target_geom in enumerate(vertices_in_buffer.geometry):\n",
    "        target_node = (round(target_geom.x, 6), round(target_geom.y, 6))\n",
    "        if target_node in lengths:\n",
    "            vertex_distances[j] = lengths[target_node]\n",
    "    return vertex_distances\n",
    "# --- Main Processing Function ---\n",
    "def process_state(state_abbr, my_path, streams_all, presence_all):\n",
    "    print(f\"\\n--- Processing {state_abbr} ---\")\n",
    "    shp_dir = os.path.join(my_path, f\"{state_abbr}_vertex_distances\")\n",
    "    os.makedirs(shp_dir, exist_ok=True)\n",
    "    shp_path = os.path.join(my_path, f\"{state_abbr}_vertex_distances.shp\")\n",
    "\n",
    "    try:\n",
    "        if streams_all.empty:\n",
    "            print(f\"Skipping {state_abbr}: no streams found.\")\n",
    "            return None\n",
    "\n",
    "        streams = streams_all.copy()\n",
    "        presence_points = presence_all\n",
    "        if presence_points.crs is None:\n",
    "            presence_points.set_crs(out_crs, inplace=True)\n",
    "        if presence_points.empty:\n",
    "            print(f\"{state_abbr}: no presence points found in state. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Extracting vertices for {state_abbr}...\")\n",
    "        all_vertices = []\n",
    "        for geom in tqdm(streams.geometry, desc=f\"{state_abbr} stream vertices\", unit=\"geom\"):\n",
    "            if geom is None or geom.is_empty:\n",
    "                continue\n",
    "            if isinstance(geom, LineString):\n",
    "                all_vertices.extend(geom.coords)\n",
    "            elif isinstance(geom, MultiLineString):\n",
    "                for line in geom.geoms:\n",
    "                    all_vertices.extend(line.coords)\n",
    "\n",
    "        vertices_gdf = gpd.GeoDataFrame(\n",
    "            geometry=gpd.points_from_xy(\n",
    "                [pt[0] for pt in all_vertices],\n",
    "                [pt[1] for pt in all_vertices]\n",
    "            ),\n",
    "            crs=streams.crs\n",
    "        ).drop_duplicates()\n",
    "\n",
    "        vertices_gdf = vertices_gdf[~((vertices_gdf.geometry.x == 0) & (vertices_gdf.geometry.y == 0))]\n",
    "\n",
    "        vertices_coords = np.array([(pt.x, pt.y) for pt in vertices_gdf.geometry])\n",
    "        vertices_tree = cKDTree(vertices_coords)\n",
    "\n",
    "        G = build_network(streams)\n",
    "        print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "        snapped_coords, _ = snap_points_to_vertices(presence_points, vertices_tree, vertices_coords)\n",
    "        snapped_coords = np.round(snapped_coords, 6)\n",
    "\n",
    "        def is_node_in_graph(geom):\n",
    "            return (round(geom.x, 6), round(geom.y, 6)) in G\n",
    "\n",
    "        vertices_in_graph = vertices_gdf[vertices_gdf.geometry.apply(is_node_in_graph)]\n",
    "\n",
    "        if vertices_in_graph.empty:\n",
    "            print(f\"{state_abbr}: no stream vertices found in graph. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Running parallel Dijkstra for {len(snapped_coords)} presence-adjacent points in {state_abbr}...\")\n",
    "\n",
    "        presence_tree = cKDTree(snapped_coords)\n",
    "        vertex_coords = np.array([(geom.x, geom.y) for geom in vertices_in_graph.geometry])\n",
    "        _, nearest_presence_idxs = presence_tree.query(vertex_coords, k=1)\n",
    "\n",
    "        presence_to_vertices = defaultdict(list)\n",
    "        for v_idx, p_idx in enumerate(nearest_presence_idxs):\n",
    "            presence_to_vertices[p_idx].append(v_idx)\n",
    "\n",
    "        print(f\"Running optimized Dijkstra for {len(presence_to_vertices)} unique presence points in {state_abbr}...\")\n",
    "\n",
    "        min_distances = np.full(len(vertices_in_graph), big_value, dtype=np.float32)\n",
    "\n",
    "        with tqdm_joblib(tqdm(desc=f\"{state_abbr} Dijkstra\", total=len(presence_to_vertices), unit=\"pt\")):\n",
    "            results = Parallel(n_jobs=2, backend=\"threading\")(\n",
    "                delayed(calculate_dijkstra_for_point)(\n",
    "                    tuple(snapped_coords[p_idx]), \n",
    "                    G, \n",
    "                    vertices_in_graph.iloc[v_idxs]\n",
    "                )\n",
    "                for p_idx, v_idxs in presence_to_vertices.items()\n",
    "            )\n",
    "\n",
    "        for (p_idx, v_idxs), dist_array in zip(presence_to_vertices.items(), results):\n",
    "            for local_idx, global_idx in enumerate(v_idxs):\n",
    "                min_distances[global_idx] = min(min_distances[global_idx], dist_array[local_idx])\n",
    "\n",
    "        vertices_in_graph[\"distance_r\"] = min_distances.astype(np.float32)\n",
    "\n",
    "        vertices_in_graph.to_file(shp_path)\n",
    "        print(f\"{state_abbr} vertex shapefile exported to: {shp_path}\")\n",
    "        return shp_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {state_abbr}: {e}\")\n",
    "        return None\n",
    "# Cell 2: Utility functions\n",
    "\n",
    "def safe_read_file(fp):\n",
    "    try:\n",
    "        return gpd.read_file(fp)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading {fp}: {e}\")\n",
    "\n",
    "def build_kdtree(vertices):\n",
    "    coords = np.array(list(zip(vertices.geometry.x, vertices.geometry.y)))\n",
    "    tree = cKDTree(coords)\n",
    "    return tree, coords\n",
    "\n",
    "def query_parallel(tree, points, n_jobs=-1):\n",
    "    results = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "        delayed(tree.query)(pt) for pt in tqdm(points, desc=\"Querying KDTree\", unit=\"pt\")\n",
    "    )\n",
    "    return zip(*results)\n",
    "\n",
    "def add_line_ids(stream_lines):\n",
    "    stream_lines['line_id'] = stream_lines.index.astype(str)\n",
    "    return stream_lines\n",
    "\n",
    "def assign_vertices_to_lines(vertices, stream_lines):\n",
    "    assert vertices.crs == stream_lines.crs, \"CRS mismatch between vertices and stream lines\"\n",
    "    print(\"Assigning vertices to stream lines via spatial join...\")\n",
    "    joined = gpd.sjoin(vertices, stream_lines[['line_id', 'geometry']], how='left', predicate='within')\n",
    "    if joined['line_id'].isna().all():\n",
    "        raise ValueError(\"No vertices could be matched to stream lines. Check geometries.\")\n",
    "    return joined\n",
    "\n",
    "def filter_disconnected_lines(vertices):\n",
    "    print(\"Filtering disconnected stream segments...\")\n",
    "    bad_line_ids = (\n",
    "        vertices.groupby('line_id')['distance_r']\n",
    "        .apply(lambda x: (x == discon_val).all())\n",
    "        .loc[lambda x: x].index\n",
    "    )\n",
    "    return bad_line_ids\n",
    "\n",
    "def split_waterbodies_by_connection(waterbodies, stream_lines, bad_line_ids):\n",
    "    print(\"Splitting waterbodies into connected and disconnected sets...\")\n",
    "    disconnected = stream_lines[stream_lines['line_id'].isin(bad_line_ids)]\n",
    "    connected = stream_lines[~stream_lines['line_id'].isin(bad_line_ids)]\n",
    "    waterbodies = waterbodies.to_crs(stream_lines.crs)\n",
    "    wb_disconnected = gpd.sjoin(waterbodies, disconnected, how='inner', predicate='intersects')\n",
    "    wb_connected = gpd.sjoin(waterbodies, connected, how='inner', predicate='intersects')\n",
    "    wb_disconnected_ids = set(wb_disconnected.index)\n",
    "    wb_connected_ids = set(wb_connected.index)\n",
    "    purely_disconnected_ids = wb_disconnected_ids - wb_connected_ids\n",
    "    return (\n",
    "        waterbodies[~waterbodies.index.isin(purely_disconnected_ids)],\n",
    "        waterbodies[waterbodies.index.isin(purely_disconnected_ids)]\n",
    "    )\n",
    "# Cell 3: Rasterization and plotting\n",
    "def rasterize_distances(vertices, waterbodies_connected, waterbodies_disconnected, output_raster_fp, nas_name, pixel_size=30):\n",
    "    print(\"Rasterizing distances to source...\")\n",
    "    vertices = vertices[vertices['distance_r'].notna()]\n",
    "    vertices = vertices[vertices['distance_r'] != discon_val]\n",
    "    vertices['distance_r'] = vertices['distance_r'].astype(float)\n",
    "\n",
    "    tree, coords = build_kdtree(vertices)\n",
    "    values = vertices['distance_r'].values\n",
    "\n",
    "    all_waterbodies = gpd.GeoSeries(pd.concat([waterbodies_connected.geometry, waterbodies_disconnected.geometry]))\n",
    "    minx, miny, maxx, maxy = all_waterbodies.total_bounds\n",
    "    width = int(np.ceil((maxx - minx) / pixel_size))\n",
    "    height = int(np.ceil((maxy - miny) / pixel_size))\n",
    "    transform = from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "    x_coords = np.linspace(minx + pixel_size / 2, maxx - pixel_size / 2, width)\n",
    "    y_coords = np.linspace(maxy - pixel_size / 2, miny + pixel_size / 2, height)\n",
    "    xx, yy = np.meshgrid(x_coords, y_coords)\n",
    "    pixel_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    print(\"Rasterizing waterbody masks...\")\n",
    "    all_waterbody_mask = rasterize(\n",
    "        [(geom, 1) for geom in all_waterbodies],\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype='uint8'\n",
    "    )\n",
    "    connected_mask = rasterize(\n",
    "        [(geom, 1) for geom in waterbodies_connected.geometry],\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype='uint8'\n",
    "    )\n",
    "\n",
    "    output_array = np.full((height, width), discon_val, dtype='float32')\n",
    "    idx_connected = np.where(connected_mask.ravel() == 1)[0]\n",
    "\n",
    "    print(f\"Querying {len(idx_connected)} connected pixels to KDTree...\")\n",
    "    dist, nn_idx = query_parallel(tree, list(pixel_points[idx_connected]))\n",
    "    output_array.ravel()[idx_connected] = values[list(nn_idx)]\n",
    "    output_array[all_waterbody_mask == 0] = np.nan\n",
    "\n",
    "    valid_mask = output_array != discon_val\n",
    "    if np.any(valid_mask):\n",
    "        max_val = np.nanmax(output_array[valid_mask])\n",
    "        output_array[~valid_mask] = max_val\n",
    "    else:\n",
    "        print(\"Warning: No valid distances found. Raster may be empty.\")\n",
    "\n",
    "    print(f\"Writing output raster to {output_raster_fp}...\")\n",
    "    band_name = f\"distance_river_{nas_name}\"\n",
    "    with rasterio.open(\n",
    "        output_raster_fp,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype='float32',\n",
    "        crs=waterbodies_connected.crs,\n",
    "        transform=transform,\n",
    "        nodata=np.nan\n",
    "    ) as dst:\n",
    "        dst.write(output_array, 1)\n",
    "        dst.set_band_description(1, band_name)\n",
    "\n",
    "    return output_array\n",
    "\n",
    "def preview_raster(array):\n",
    "    plt.imshow(array, cmap='viridis')\n",
    "    plt.colorbar(label='Distance to Source')\n",
    "    plt.title('Rasterized Distance to Stream Source')\n",
    "    plt.show()\n",
    "\n",
    "def log_metadata(output_fp, metadata_dict):\n",
    "    print(f\"Logging metadata to {output_fp}...\")\n",
    "    with open(output_fp, 'w') as f:\n",
    "        json.dump(metadata_dict, f, indent=2)\n",
    "\n",
    "def prepare_stream_network(stream_path, simplify_tolerance, crs):\n",
    "    streams = gpd.read_file(stream_path).to_crs(crs)\n",
    "    streams = streams.drop_duplicates(subset='geometry').reset_index(drop=True)\n",
    "    dissolved = unary_union(streams.geometry)\n",
    "    simplified = gpd.GeoDataFrame(geometry=[dissolved], crs=crs).explode(index_parts=False).reset_index(drop=True)\n",
    "\n",
    "    if simplify_tolerance is not None:\n",
    "        simplified[\"geometry\"] = simplified.simplify(tolerance=simplify_tolerance, preserve_topology=True)\n",
    "\n",
    "    return simplified\n",
    "    \n",
    "def prepare_presence_points(presence_path, crs):\n",
    "    presence = gpd.read_file(presence_path)\n",
    "    return presence.to_crs(crs)\n",
    "def run_stream_distance_model(state_abbr, streams_all, presence_all, output_dir):\n",
    "    return process_state(\n",
    "        state_abbr=state_abbr,\n",
    "        streams_all=streams_all,\n",
    "        presence_all=presence_all,\n",
    "        my_path=output_dir\n",
    "    )    \n",
    "\n",
    "def postprocess_stream_network(vertices_fp, streams_fp, water_fp):\n",
    "    streams = add_line_ids(safe_read_file(streams_fp))\n",
    "    vertices = safe_read_file(vertices_fp)\n",
    "    waterbodies = safe_read_file(water_fp)\n",
    "    \n",
    "    vertices = assign_vertices_to_lines(vertices.to_crs(streams.crs), streams)\n",
    "    bad_line_ids = filter_disconnected_lines(vertices)\n",
    "    wb_connected, wb_disconnected = split_waterbodies_by_connection(waterbodies, streams, bad_line_ids)\n",
    "    \n",
    "    return vertices, wb_connected, wb_disconnected, bad_line_ids\n",
    "\n",
    "def make_stream_distance_raster(vertices, wb_connected, wb_disconnected, output_fp, metadata_fp, nas_name, pixel_size=100, state_abbr=None):\n",
    "    array = rasterize_distances(vertices, wb_connected, wb_disconnected, output_fp, nas_name, pixel_size)\n",
    "    log_metadata(metadata_fp, {\n",
    "        'state': state_abbr,\n",
    "        'pixel_size': pixel_size,\n",
    "        'bad_line_ids': list(filter_disconnected_lines(vertices)),\n",
    "        'vertex_count': len(vertices),\n",
    "        'raster_output': output_fp\n",
    "    })\n",
    "    return array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b791c42-4834-470c-8822-f566d061f157",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "‚ö†Ô∏è  Note on Warnings:\n",
    "Some warnings may appear during this script. These warnings do not indicate pipeline failure and can be safely ignored unless followed by an error.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f571cc5-36da-4c2f-ae7f-1935516b9131",
   "metadata": {},
   "source": [
    "# User defined variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc18a25-f28e-419e-adb0-99f9ea9048ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_state_abbr = 'ID' # State USPS abbreviation\n",
    "training_state_name = 'Idaho' \n",
    "nas_id = 237 # European Frogbit = 1110; Eurasian watermilfoil = 237; Hydrilla verticillata = 6 (get these identifiers from https://nas.er.usgs.gov/api/v2/species)\n",
    "nas_name = 'EMF'\n",
    "# IA = 19; ID = 16; IL = 17; MN = 27; MO = 29; MT = 30; OR = 41;  WA = 53; WI = 55\n",
    "training_fip = 16\n",
    "# Replace last 2 digits with your state's FIP code\n",
    "training_path = 'my_data/' + training_state_abbr + '/' # leave this alone \n",
    "my_crs = \"EPSG:5070\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca726f14-f463-46f8-9222-44cf017372f8",
   "metadata": {},
   "source": [
    "# Start of Water Quality predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec64d6-9d31-4ac3-8e7c-932c56fda9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stations_gdf = get_usgs_stations_in_state(\n",
    "    state_fip= training_fip,      \n",
    "    state_abbr= training_state_abbr,\n",
    "    my_path= training_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266ef9a8-ee47-48e7-b706-09fe339c1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcium\n",
    "training_ca = get_water_quality(\"Calcium\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991fd1a-3fdb-46e1-917e-3a6be419f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pH\n",
    "training_pH= get_water_quality(\"pH\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa2053-0ad3-41be-9496-f6d05b036e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolved Oxygen\n",
    "training_DO = get_water_quality('Oxygen', state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549808a-df07-4d91-8210-874023bf2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nitrogen\n",
    "training_N = get_water_quality(\"Nitrogen\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b64d5-1d3b-419b-8017-5f8501b7ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phosphorus\n",
    "training_Phos = get_water_quality(\"Phosphorus\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae73a3d-cc1d-4787-b92a-b1f117d783a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can always load assets if you made them before\n",
    "# training_ca = gpd.read_file(training_path + 'usgs_calcium.shp')\n",
    "# training_pH = gpd.read_file(training_path + 'usgs_pH.shp')\n",
    "# training_N = gpd.read_file(training_path + 'usgs_N.shp')\n",
    "# training_do = gpd.read_file(training_path + 'usgs_do.shp')\n",
    "#training_phos = gpd.read_file(training_path + 'usgs_phos.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10750f-473e-4d16-9335-13702b340562",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_ca, training_path + training_state_abbr + '_ca.tif', 'Ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f62f2b-8f7a-4595-b90a-9b9c39cc5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_pH, training_path + training_state_abbr + '_pH.tif', 'pH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebd38d-ab62-4dfd-b26d-013b984c8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_N, training_path + training_state_abbr + '_N.tif', 'Nitrogen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833a24d-7038-4b02-9d3b-94f40d9b7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_DO, training_path + training_state_abbr + '_do.tif', 'DO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cce07c-fcda-4652-befb-ad4f1cd729a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_Phos, training_path + training_state_abbr + '_phos.tif', 'Phos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5fd19-d150-4034-9825-641a393420f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_wq_training = [\n",
    "    training_path + training_state_abbr + \"_ca.tif\", training_path + training_state_abbr + \"_pH.tif\", training_path + training_state_abbr + \"_N.tif\",\n",
    "    training_path + training_state_abbr + \"_do.tif\", training_path + training_state_abbr + \"_phos.tif\"]\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_wq_training = training_path + training_state_abbr + \"_combined_wq.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_wq_training, output_file_wq_training)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_wq_training}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c36f9-09cb-4bec-9696-715cc0021b94",
   "metadata": {},
   "source": [
    "# Start of Biological Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881a0c6-767b-454c-93e1-6827811bf8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download/import water data.\n",
    "water_data_training = get_nhd_waterbodies(\n",
    "    state_name=training_state_name,\n",
    "    local_path=training_path,\n",
    "    output_prefix=training_state_abbr,\n",
    "    save_files=True,\n",
    "    make_background_water=True, # set to false here, unless you want this for future model training within your state\n",
    "    training_path=training_path,\n",
    "    training_state_abbr=training_state_abbr\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b3414-dd24-40b7-a3cc-3e3de18c610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer individual water layers\n",
    "buffered_streams_training, buffered_lakes_training, buffered_rivers_training, full_streams_training, buffered_water_training = buffer_water_layers(\n",
    "    streams=water_data_training[\"streams\"],\n",
    "    lakes=water_data_training[\"lakes\"],\n",
    "    rivers=water_data_training[\"rivers\"],\n",
    "    filter_streams=False,\n",
    "    save_path= training_path,\n",
    "    use_cached=False,\n",
    "    export_merged_filename=f\"{training_path}{training_state_abbr}_buffered_water.shp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5849a2a-97b7-4776-8942-13b03d0fce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_rich_training = process_nas_occurrences(\n",
    "    state= training_state_abbr,\n",
    "    crs= my_crs,\n",
    "    path= training_path,\n",
    "    target_species_id= nas_id,\n",
    "    target_species_name= nas_name,\n",
    "    make_background=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6496350-c1d0-487f-8e40-4dcb4807a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_inv_richness(inv_rich_training, training_path + training_state_abbr + '_' + nas_name +'_inv_richness.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fb947-7162-4dcb-812e-3c068b480730",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_export_native_fish_raster(\n",
    "    my_path=training_path, \n",
    "    my_crs=\"EPSG:5070\",\n",
    "    state_abbr=training_state_abbr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90ed0d-1e64-4c00-a2a1-e654fe721d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_bio_training = [\n",
    "    training_path + training_state_abbr +'_' + nas_name + \"_inv_richness.tif\", \n",
    "    training_path + training_state_abbr + \"_Native_Fish_Richness.tif\"]\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_bio_training = training_path + training_state_abbr + '_' + nas_name + \"_combined_bio.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_bio_training, output_file_bio_training)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_bio_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c5a859-49f9-48ad-a526-a663b10f8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note these distance predictors are highly correlated. You should make the predictor that is most relevant to your taxa\n",
    "or if your target does spread by both mechanisms make both and let the model sort it out. This is kind of nuanced as Road Distance may also\n",
    "be informative of AIS which were originally stocked and now spread by stream/river network such as Rainbow and Brook Trout.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92117b4c-5d65-4838-9a2f-eb95d3722505",
   "metadata": {},
   "source": [
    "# Road Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953688f1-bd76-451c-b2a2-577a51dec72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and format data for road network/distance analysis\n",
    "my_roads, pos_endpoints, neg_endpoints, ramps_in_water_epoints, lakes_no_ramp_epoints = extract_roads_and_endpoints(\n",
    "    local_path=training_path,\n",
    "    state_abbr=training_state_abbr,\n",
    "    state_fips=training_fip,\n",
    "    state_name=training_state_abbr,\n",
    "    buffered_water = buffered_water_training,\n",
    "    nas_id=nas_id,\n",
    "    my_crs=my_crs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a84bab-686d-4aee-8ed4-afc7f6d7ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_with_dist = compute_and_export_road_distances(\n",
    "    my_roads=my_roads,\n",
    "    pos_endpoints=pos_endpoints,\n",
    "    neg_endpoints=neg_endpoints,\n",
    "    ramps_epoints=ramps_in_water_epoints,\n",
    "    lakes_epoints=lakes_no_ramp_epoints,\n",
    "    buffered_water=buffered_water_training,\n",
    "    my_crs=my_crs,\n",
    "    my_path= training_path,\n",
    "    my_state=training_state_abbr,\n",
    "    nas_name= nas_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ac656-ef2a-4b07-80f2-a47082fff9e0",
   "metadata": {},
   "source": [
    "# Stream/River Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22bdbf4-1a25-42f6-a5ca-89ea3cd8b447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing ID ---\n",
      "Extracting vertices for ID...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ID stream vertices: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 643100/643100 [02:14<00:00, 4767.67geom/s]\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "stream_path = os.path.join(training_path, f\"{training_state_abbr}_streams.shp\")\n",
    "presence_path = os.path.join(training_path, f\"{training_state_abbr}_{nas_name}_pos_data.shp\")\n",
    "output_raster_fp = os.path.join(training_path, f\"{training_state_abbr}_dist_to_src_river_{nas_name}_.tif\")\n",
    "metadata_fp = output_raster_fp.replace('.tif', '_metadata.json')\n",
    "discon_val = 9999999999\n",
    "# 1. Prepare inputs\n",
    "streams_all = prepare_stream_network(stream_path, simplify_tolerance=None, crs=my_crs)\n",
    "presence_all = prepare_presence_points(presence_path, crs=my_crs)\n",
    "run_stream_distance_model(training_state_abbr, streams_all, presence_all, training_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d74554-55c4-4448-b6fa-a46997b7241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Run Dijkstra model\n",
    "\n",
    "run_stream_distance_model(training_state_abbr, streams_all, presence_all, training_path)\n",
    "\n",
    "# 3. Postprocess results\n",
    "vertices_fp = os.path.join(training_path, f\"{training_state_abbr}_vertex_distances.shp\")\n",
    "streams_fp = stream_path\n",
    "water_fp = os.path.join(training_path, f\"{training_state_abbr}_buffered_water.shp\")\n",
    "\n",
    "vertices, wb_conn, wb_disc, _ = postprocess_stream_network(vertices_fp, streams_fp, water_fp)\n",
    "\n",
    "# 4. Rasterize\n",
    "raster_array = make_stream_distance_raster(vertices, wb_conn, wb_disc, output_raster_fp, metadata_fp, nas_name, pixel_size=100, state_abbr=training_state_abbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf31ded-47b0-4e3f-a13b-a18f2bd8b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_dist_training = [\n",
    "    training_path + training_state_abbr +'_road_distance_' + nas_name + \".tif\", \n",
    "    training_path + training_state_abbr + '_dist_to_src_river_' + nas_name + '.tif']\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_dist_training = training_path + training_state_abbr + '_' + nas_name + \"_combined_dist.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_dist_training, output_file_dist_training)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_dist_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45495d-502c-4af3-9af9-000c0d67da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you are training and predicting to the same state,\n",
    "#upload the combined rasters for water quality and biotic parameters to your GEE account manually \n",
    "#and run the github_formatter_clean GEE script with your data.\n",
    "#If you want to train the model with data from multiple states, repeat the steps above with updated user defined variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab23be3-93d9-45b4-83d1-7cca8871cca3",
   "metadata": {},
   "source": [
    "# Predictors for interstate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42690ad3-a314-4dfc-9a9b-0641bd8a8085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you are predicting to a state without the AIS you will need to make predictors.\n",
    "#This bit of code does that without creating the training data and also derives road distance from border points to waterbodies if your prediction state is uninfested\n",
    "#Note: The Water Quality Predictors and Buffered Water can also be used in the future for training (i.e., you don't need to remake them for your state if you already made them once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c9165-5734-4b71-823c-2e099abb3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict to state variables\n",
    "predict_state_abbr = 'ID' # State USPS abbreviation\n",
    "predict_state_name = 'Idaho' \n",
    "# IA = 19; ID = 16; IL = 17; MN = 27; MO = 29; MT = 30; OR = 41;  WA = 53; WI = 55\n",
    "predict_fip = '16'\n",
    "# Replace last 2 digits with your state's FIP code\n",
    "predict_path = 'my_data/' + predict_state_abbr + '/' # leave this alone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa845b-a89e-4e1f-90a0-cb6fec3308fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_stations_gdf = get_usgs_stations_in_state(\n",
    "    state_fip= predict_fip,      \n",
    "    state_abbr= predict_state_abbr,\n",
    "    my_path= predict_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110da676-eaeb-4a9e-95e7-815ef8772c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcium\n",
    "predict_ca = get_water_quality(\"Calcium\", state_fip=predict_fip, stations_gdf=predict_stations_gdf, my_path=predict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb8b69-243b-4584-b14e-610a04aca705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolved Oxygen\n",
    "predict_DO = get_water_quality('Oxygen', state_fip=predict_fip, stations_gdf=predict_stations_gdf, my_path=predict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f51e8-e276-45fa-a3a6-a06724311300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pH\n",
    "predict_pH= get_water_quality(\"pH\", state_fip=predict_fip, stations_gdf=predict_stations_gdf, my_path=predict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1f741-9285-4f4a-8228-8379af5e6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nitrogen\n",
    "predict_N = get_water_quality(\"Nitrogen\", state_fip=predict_fip, stations_gdf=predict_stations_gdf, my_path=predict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b95c25-62ee-4382-a9c3-5b3c07d59838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phosphorus\n",
    "predict_Phos = get_water_quality(\"Phosphorus\", state_fip=predict_fip, stations_gdf=predict_stations_gdf, my_path=predict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e465d-7f44-433e-91ed-6ef0d1e99e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_ca_result = gpd.read_file(predict_path + 'usgs_calcium.shp')\n",
    "# predict_pH_result = gpd.read_file(predict_path + 'usgs_pH.shp')\n",
    "# predict_di_N_result = gpd.read_file(predict_path + 'usgs_N.shp')\n",
    "# predict_do_result = gpd.read_file(predict_path + 'usgs_do.shp')\n",
    "# predict_phos_result = gpd.read_file(predict_path + 'usgs_phos.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3208f8-1ca4-4ea8-9193-27e43d9cec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(predict_ca, predict_path + predict_state_abbr + '_ca.tif', 'Ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ab14f-b6c4-4284-8700-4c58bf9f1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(predict_pH, predict_path + predict_state_abbr + '_pH.tif', 'pH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dedf81-ea20-4739-abb3-4bb7e8544a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(predict_N, predict_path + predict_state_abbr + '_N.tif', 'Nitrogen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c8b44-caad-470d-a84e-72914a419d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(predict_DO, predict_path + predict_state_abbr + '_do.tif', 'DO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d35aa1e-6aa5-425b-95b5-d3b5a7053b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(predict_Phos, predict_path + predict_state_abbr + '_phos.tif', 'Phos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfcc0b6-4646-4da3-b523-9ea0300453a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_wq_predict = [\n",
    "    predict_path + predict_state_abbr + \"_ca.tif\", predict_path + predict_state_abbr + \"_pH.tif\", predict_path + predict_state_abbr + \"_N.tif\",\n",
    "    predict_path + predict_state_abbr + \"_do.tif\", predict_path + predict_state_abbr + \"_phos.tif\"]\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_wq_predict = predict_path + predict_state_abbr + \"_combined_wq.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_wq_predict, output_file_wq_predict)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_wq_predict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735ece1-11cf-4a44-b651-14e2d7ecbcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data_predict = get_nhd_waterbodies(\n",
    "    state_name=predict_state_name,\n",
    "    local_path=predict_path,\n",
    "    output_prefix=predict_state_abbr,\n",
    "    save_files=False,\n",
    "    make_background_water=False, # set to false here, unless you want this for future model training within your state\n",
    "    training_path=predict_path,\n",
    "    training_state_abbr=predict_state_abbr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c904e361-9f38-4e01-8497-8e82e086511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer individual water layers\n",
    "buffered_streams_predict, buffered_lakes_predict, buffered_rivers_predict, full_streams_predict, buffered_water_predict = buffer_water_layers(\n",
    "    streams=water_data_predict[\"streams\"],\n",
    "    lakes=water_data_predict[\"lakes\"],\n",
    "    rivers=water_data_predict[\"rivers\"],\n",
    "    simplify_tolerance=1000,\n",
    "    filter_streams=False,\n",
    "    save_path= predict_path,\n",
    "    use_cached=True,\n",
    "    export_merged_filename=f\"{predict_path}{predict_state_abbr}_buffered_water.shp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc288d-37e2-4ac4-9c8f-861ccab4ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_rich_predict = process_nas_occurrences(\n",
    "    state= predict_state_abbr,\n",
    "    crs= my_crs,\n",
    "    path= predict_path,\n",
    "    target_species_id= nas_id, # we still give this function your nas_id and name even though the target is not present. The function just skips the associated steps.\n",
    "    target_species_name= nas_name,\n",
    "    make_background=False) # set to false here because this is the data we are predicting to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3d030-eadc-424e-9b8b-80770dfc0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_inv_richness(inv_rich_predict, predict_path + predict_state_abbr + '_' + nas_name + '_inv_richness.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d5087-31d7-4f7d-b0f5-86eb2cf871eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_export_native_fish_raster(\n",
    "    my_path=predict_path,\n",
    "    my_crs=\"EPSG:5070\",\n",
    "    state_abbr=predict_state_abbr,\n",
    "    fish_shapefile = 'usgs_native_fish_rich.shp',\n",
    "    state_shapefile = 'tl_2012_us_state.shp',\n",
    "    buffer_shapefile_suffix = '_buffered_water.shp',\n",
    "    resolution = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7088b-7931-4b75-9a98-01c6be6ac01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_bio_predict = [\n",
    "    predict_path + predict_state_abbr + '_' + nas_name + \"_inv_richness.tif\", \n",
    "    predict_path + predict_state_abbr + \"_Native_Fish_Richness.tif\"]\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_bio_predict = predict_path + predict_state_abbr +'_' + nas_name +\"_combined_bio.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_bio_predict, output_file_bio_predict)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_bio_predict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4221d2-40db-49e0-bd78-7887d3c69600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Road Distance helper functions for non-infested states\n",
    "def get_endpoints(geometry):\n",
    "    \"\"\"\n",
    "    Extracts start and end points from a LineString or MultiLineString.\n",
    "    Args:\n",
    "        geometry (shapely.geometry): A LineString or MultiLineString geometry.\n",
    "    Returns:\n",
    "        list of shapely.geometry.Point: [start_point, end_point] or list of such pairs.\n",
    "    \"\"\"\n",
    "    if geometry is None or geometry.is_empty:\n",
    "        return []\n",
    "\n",
    "    if isinstance(geometry, LineString):\n",
    "        coords = list(geometry.coords)\n",
    "        return [Point(coords[0]), Point(coords[-1])]\n",
    "\n",
    "    elif geometry.geom_type == 'MultiLineString':\n",
    "        endpoints = []\n",
    "        for line in geometry.geoms:\n",
    "            coords = list(line.coords)\n",
    "            endpoints.append(Point(coords[0]))\n",
    "            endpoints.append(Point(coords[-1]))\n",
    "        return endpoints\n",
    "\n",
    "    return []\n",
    "\n",
    "def download_roads_and_ramps(\n",
    "    state_fips: str,\n",
    "    output_dir: str,\n",
    "    ramp_url: str = \"https://www.sciencebase.gov/catalog/file/get/63b81b50d34e92aad3cc004d?facet=Boatramps_United_States_final_20230104\",\n",
    "    ramp_extensions: Set[str] = {'.dbf', '.prj', '.shp', '.shx'},\n",
    "    verbose: bool = True\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Downloads TIGER roads and USGS boat ramp shapefiles.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: paths to the roads and ramps shapefiles (.shp files)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Roads ---\n",
    "    road_url = f'https://www2.census.gov/geo/tiger/TIGER2022/PRISECROADS/tl_2022_{state_fips}_prisecroads.zip'\n",
    "    roads_shp_path = None\n",
    "    try:\n",
    "        if verbose: print(f\"üì° Downloading roads from: {road_url}\")\n",
    "        r = requests.get(road_url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "            z.extractall(path=output_dir)\n",
    "            for name in z.namelist():\n",
    "                if name.endswith('.shp') and f\"tl_2022_{state_fips}_prisecroads\" in name:\n",
    "                    roads_shp_path = os.path.join(output_dir, name)\n",
    "                    break\n",
    "            if verbose and roads_shp_path:\n",
    "                print(f\"‚úÖ Roads shapefile: {roads_shp_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to download or extract roads: {e}\")\n",
    "\n",
    "    # --- Boat Ramps ---\n",
    "    ramps_shp_path = None\n",
    "    try:\n",
    "        if verbose: print(f\"\\nüì° Downloading ramps from: {ramp_url}\")\n",
    "        r = requests.get(ramp_url, stream=True, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        with zipfile.ZipFile(io.BytesIO(r.content)) as zip_ref:\n",
    "            selected_files = [\n",
    "                f for f in zip_ref.namelist()\n",
    "                if os.path.splitext(f)[1].lower() in ramp_extensions\n",
    "            ]\n",
    "            for file_name in selected_files:\n",
    "                zip_ref.extract(file_name, output_dir)\n",
    "                if verbose:\n",
    "                    print(f\"‚úÖ Extracted: {file_name}\")\n",
    "                if file_name.endswith(\".shp\") and \"Boatramps_United_States\" in file_name:\n",
    "                    ramps_shp_path = os.path.join(output_dir, file_name)\n",
    "        if verbose and ramps_shp_path:\n",
    "            print(f\"‚úÖ Ramps shapefile: {ramps_shp_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to download or extract ramps: {e}\")\n",
    "\n",
    "    return roads_shp_path, ramps_shp_path\n",
    "\n",
    "def get_nas_occurrences(nas_id):\n",
    "    \"\"\"Fetch NAS occurrences for a given species ID.\"\"\"\n",
    "    url = f\"http://nas.er.usgs.gov/api/v2/occurrence/search?species_ID={nas_id}\"\n",
    "    response = requests.get(url, timeout=None)\n",
    "    data = response.json()\n",
    "    return pd.json_normalize(data, 'results')\n",
    "\n",
    "def get_road_boundary_intersections(roads_gdf, boundary_gdf):\n",
    "    \"\"\"Extract intersection points where roads cross the state boundary.\"\"\"\n",
    "    boundary = boundary_gdf.boundary.unary_union\n",
    "    crossing_lines = roads_gdf[roads_gdf.crosses(boundary)]\n",
    "    intersection = crossing_lines.unary_union.intersection(boundary)\n",
    "    \n",
    "    points = []\n",
    "    def extract_points(geom):\n",
    "        if geom.is_empty: return\n",
    "        if isinstance(geom, Point): points.append(geom)\n",
    "        elif isinstance(geom, MultiPoint): points.extend(geom.geoms)\n",
    "        elif isinstance(geom, GeometryCollection):\n",
    "            for g in geom.geoms: extract_points(g)\n",
    "\n",
    "    extract_points(intersection)\n",
    "    return gpd.GeoDataFrame(geometry=points, crs=roads_gdf.crs)\n",
    "\n",
    "def sjoin_nearest_replace_geom(target_gdf, reference_gdf, max_distance=None):\n",
    "    \"\"\"\n",
    "    Spatially joins target to nearest reference feature and replaces geometry with reference's geometry.\n",
    "    Adds 'snap_dist' column showing distance between original and snapped.\n",
    "    \"\"\"\n",
    "    joined = gpd.sjoin_nearest(\n",
    "        target_gdf,\n",
    "        reference_gdf,\n",
    "        how=\"left\",\n",
    "        max_distance=max_distance,\n",
    "        distance_col=\"snap_dist\"\n",
    "    )\n",
    "\n",
    "    # Manually copy over the reference geometry\n",
    "    if \"geometry_right\" in joined.columns:\n",
    "        joined[\"reference_geometry\"] = joined[\"geometry_right\"]\n",
    "    elif \"geometry\" in reference_gdf.columns:\n",
    "        joined[\"reference_geometry\"] = reference_gdf.loc[joined[\"index_right\"]].geometry.values\n",
    "    else:\n",
    "        raise ValueError(\"Reference geometry not found in join results.\")\n",
    "\n",
    "    if \"reference_geometry\" not in joined.columns:\n",
    "        raise ValueError(\"reference_geometry column missing after spatial join. Likely join failure.\")\n",
    "\n",
    "    # Replace geometry with the joined reference geometry\n",
    "    joined = joined.set_geometry(\"reference_geometry\")\n",
    "    joined = joined.drop(columns=[\"snap_dist\", \"index_right\"])\n",
    "\n",
    "    return joined\n",
    "\n",
    "def extract_unique_endpoints(roads_gdf):\n",
    "    \"\"\"Get unique road endpoints from LineStrings.\"\"\"\n",
    "    endpoints = roads_gdf['geometry'].apply(get_endpoints).explode()\n",
    "    coords_df = pd.DataFrame(endpoints.tolist(), columns=['x', 'y']).drop_duplicates()\n",
    "    return gpd.GeoDataFrame(coords_df, geometry=gpd.points_from_xy(coords_df['x'], coords_df['y']), crs=roads_gdf.crs).drop(columns=['x', 'y'])\n",
    "\n",
    "def snap_points_to_nearest_poly(poly: gpd.GeoDataFrame, point: gpd.GeoDataFrame, snapdist: float) -> gpd.GeoDataFrame:\n",
    "    poly = poly.to_crs(point.crs)\n",
    "    point = point.set_crs(point.crs)\n",
    "\n",
    "    # Assign IDs\n",
    "    poly = poly.copy()\n",
    "    point = point.copy()\n",
    "    poly[\"polyid\"] = range(len(poly))\n",
    "    point[\"pointid\"] = range(len(point))\n",
    "\n",
    "    # Save original polygon geometry and ID (e.g., waterID)\n",
    "    poly[\"polygeom\"] = poly.geometry\n",
    "    if \"waterID\" not in poly.columns:\n",
    "        poly[\"waterID\"] = poly[\"polyid\"]\n",
    "\n",
    "    # Do spatial join\n",
    "    sj = gpd.sjoin_nearest(point, poly, how=\"left\", max_distance=snapdist, distance_col=\"snap_dist\")\n",
    "\n",
    "    # Drop unmatched\n",
    "    sj = sj[sj[\"polygeom\"].notnull()].copy()\n",
    "    if sj.empty:\n",
    "        print(\"Warning: No nearby polygons found within snapdist.\")\n",
    "        return gpd.GeoDataFrame(columns=point.columns, geometry=point.geometry.name, crs=point.crs)\n",
    "\n",
    "    # Compute nearest points\n",
    "    sj[\"nearestpoint\"] = sj.apply(lambda x: nearest_points(x.geometry, x[\"polygeom\"])[1], axis=1)\n",
    "\n",
    "    # Snap to nearest point\n",
    "    def safe_snap(row):\n",
    "        try:\n",
    "            if row[\"nearestpoint\"] and row.geometry.distance(row[\"nearestpoint\"]) <= snapdist:\n",
    "                return snap(row.geometry, row[\"nearestpoint\"], snapdist)\n",
    "        except:\n",
    "            pass\n",
    "        return row.geometry\n",
    "\n",
    "    # Replace geometry\n",
    "    snapped_geoms = sj.apply(safe_snap, axis=1)\n",
    "    sj = sj.drop(columns=\"geometry\")\n",
    "    sj = sj.set_geometry(gpd.GeoSeries(snapped_geoms, crs=point.crs))\n",
    "\n",
    "    # Keep only necessary columns and rename for downstream\n",
    "    return sj[[\"rampID\", \"waterID\", \"nearestpoint\", \"geometry\"]]\n",
    "\n",
    "def assign_ramps_to_water(ramps_gdf, water_gdf, snap_distance, crs):\n",
    "    \"\"\"Assign ramps to waterbodies or snap if outside but nearby.\"\"\"\n",
    "    ramps_gdf = ramps_gdf.set_crs(crs).to_crs(crs)\n",
    "    water_gdf = water_gdf.to_crs(crs)\n",
    "    \n",
    "    ramps_in_water = ramps_gdf.sjoin(water_gdf, how=\"left\", predicate=\"within\")\n",
    "    ramps_not_in_water = ramps_in_water[ramps_in_water['index_right'].isna()].drop(columns=[\"index_right\"])\n",
    "    ramps_in_water = ramps_in_water.dropna(subset=[\"index_right\"]).drop(columns=[\"index_right\"])\n",
    "    \n",
    "    snapped = snap_points_to_nearest_poly(water_gdf, ramps_not_in_water, snap_distance)\n",
    "    snapped_renamed = snapped.rename(columns={\"nearestpoint\": \"geometry\"})\n",
    "    \n",
    "    return pd.concat([ramps_in_water, snapped_renamed])\n",
    "\n",
    "def assign_entry_to_endpoint_road_distances(\n",
    "    entry_points_gdf: gpd.GeoDataFrame,\n",
    "    endpoints_gdf: gpd.GeoDataFrame,\n",
    "    G,\n",
    "    kd_tree,\n",
    "    precision=6,\n",
    "    max_snap_distance=100,\n",
    "    endpoint_id_col='epointID',\n",
    "    entry_point_id_col='entryID'\n",
    "):\n",
    "    \"\"\"\n",
    "    For each endpoint, assign shortest distance along road graph to the nearest entry point.\n",
    "    Returns a list of dicts with: epointID, entryID, distance_roads\n",
    "    \"\"\"\n",
    "    # Assign unique IDs if missing\n",
    "    if endpoint_id_col not in endpoints_gdf.columns:\n",
    "        endpoints_gdf = endpoints_gdf.reset_index(drop=True)\n",
    "        endpoints_gdf[endpoint_id_col] = endpoints_gdf.index\n",
    "    if entry_point_id_col not in entry_points_gdf.columns:\n",
    "        entry_points_gdf = entry_points_gdf.reset_index(drop=True)\n",
    "        entry_points_gdf[entry_point_id_col] = entry_points_gdf.index\n",
    "\n",
    "    # Graph node coordinates\n",
    "    node_coords = np.array(list(G.nodes))\n",
    "    coord_to_node = {tuple(coord): coord for coord in node_coords}\n",
    "\n",
    "    # Step 1: Snap entry points to graph nodes\n",
    "    entry_nodes = {}  # entryID -> snapped graph node\n",
    "    node_to_entry_id = {}  # graph node -> entryID\n",
    "\n",
    "    for idx, row in entry_points_gdf.iterrows():\n",
    "        x, y = row.geometry.x, row.geometry.y\n",
    "        snap_dist, node_idx = kd_tree.query([x, y])\n",
    "        snap_point = tuple(node_coords[node_idx])\n",
    "        eu_dist = np.linalg.norm(np.array([x, y]) - np.array(snap_point))\n",
    "\n",
    "        if eu_dist <= max_snap_distance:\n",
    "            entry_id = row[entry_point_id_col]\n",
    "            entry_nodes[entry_id] = snap_point\n",
    "            node_to_entry_id[snap_point] = entry_id\n",
    "\n",
    "    if not entry_nodes:\n",
    "        raise ValueError(\"No entry points could be snapped to graph nodes.\")\n",
    "\n",
    "    # Step 2: Multi-source Dijkstra from all entry nodes\n",
    "    length_dict = {}  # target_node -> (entry_node, distance)\n",
    "\n",
    "    for source_node in entry_nodes.values():\n",
    "        lengths = nx.single_source_dijkstra_path_length(G, source_node)\n",
    "        for target_node, dist in lengths.items():\n",
    "            if target_node not in length_dict or dist < length_dict[target_node][1]:\n",
    "                length_dict[target_node] = (source_node, dist)\n",
    "\n",
    "    # Step 3: Snap endpoints to graph, lookup shortest distance\n",
    "    results = []\n",
    "    for idx, row in endpoints_gdf.iterrows():\n",
    "        geom = row.geometry\n",
    "        if geom.geom_type != 'Point':\n",
    "            geom = geom.centroid\n",
    "\n",
    "        snap_dist, node_idx = kd_tree.query([geom.x, geom.y])\n",
    "        snap_point = tuple(node_coords[node_idx])\n",
    "        eu_dist = np.linalg.norm(np.array([geom.x, geom.y]) - np.array(snap_point))\n",
    "\n",
    "        if eu_dist <= max_snap_distance and snap_point in length_dict:\n",
    "            entry_node, dist_roads = length_dict[snap_point]\n",
    "            entry_id = node_to_entry_id.get(entry_node)\n",
    "            results.append({\n",
    "                endpoint_id_col: row[endpoint_id_col],\n",
    "                entry_point_id_col: entry_id,\n",
    "                'distance_roads': dist_roads\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                endpoint_id_col: row[endpoint_id_col],\n",
    "                entry_point_id_col: None,\n",
    "                'distance_roads': np.nan\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "def nearest_distance(row, target_gdf):\n",
    "    \"\"\"\n",
    "    Compute the minimum distance from a point to a set of target geometries.\n",
    "    Args:\n",
    "        row (GeoSeries): A row with a geometry column.\n",
    "        target_gdf (GeoDataFrame): A GeoDataFrame of geometries to measure distance to.\n",
    "    Returns:\n",
    "        float: Minimum distance in CRS units (e.g., meters).\n",
    "    \"\"\"\n",
    "    if target_gdf.empty or row.geometry is None or row.geometry.is_empty:\n",
    "        return float('inf')\n",
    "    return row.geometry.distance(target_gdf.unary_union)\n",
    "\n",
    "# Calculate shortest distances for an array of precomputed nearest neighbor pairs\n",
    "def calculate_shortest_distances_with_pairs(point_pairs, source_gdf, target_gdf, network_graph):\n",
    "    results = []\n",
    "    \n",
    "    for pair in point_pairs:\n",
    "        source_id, target_id = pair\n",
    "        # Get the source and target geometries\n",
    "        source_row = source_gdf[source_gdf[\"epointID\"] == source_id].iloc[0]\n",
    "        target_row = target_gdf[target_gdf[\"intID\"] == target_id].iloc[0]\n",
    "        \n",
    "        source_geom = source_row.geometry\n",
    "        target_geom = target_row.geometry\n",
    "        \n",
    "        # Find nearest network nodes for source and target\n",
    "        source_node = find_nearest_node(network_graph, source_geom)\n",
    "        target_node = find_nearest_node(network_graph, target_geom)\n",
    "        \n",
    "        # Compute the shortest path distance along the network\n",
    "        try:\n",
    "            path_length = nx.shortest_path_length(\n",
    "                network_graph, source_node, target_node, weight=\"weight\"\n",
    "            )\n",
    "        except nx.NetworkXNoPath:\n",
    "            path_length = float(\"inf\")  # No path found\n",
    "        \n",
    "        # Record the result\n",
    "        results.append({\n",
    "            \"epointID\": source_id,\n",
    "            \"intID\": target_id,\n",
    "            \"network_distance\": path_length\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9012e76-8b6b-4149-b2a6-0b1f7d00ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_distance_to_source(\n",
    "    nas_id: int,\n",
    "    state_fips: str,\n",
    "    state_abbr: str,\n",
    "    boundary_fp: str,\n",
    "    buffered_water_gdf: gpd.GeoDataFrame,\n",
    "    crs: str = \"EPSG:5070\",\n",
    "    snap_distance: float = 1000,\n",
    "    output_dir: str = \"./data\"\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Computes distance-to-source from NAS entry points to lakes using roads and ramps.\n",
    "    Downloads roads/ramps if not present.\n",
    "    \"\"\"\n",
    "\n",
    "    ### STEP 0: Prep output paths and check for cached files ###\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    roads_fp = os.path.join(output_dir, f\"{state_fips}_roads.shp\")\n",
    "    ramps_fp = os.path.join(output_dir, f\"{state_fips}_ramps.shp\")\n",
    "\n",
    "    if not (os.path.exists(roads_fp) and os.path.exists(ramps_fp)):\n",
    "        roads_fp, ramps_fp = download_roads_and_ramps(\n",
    "            state_fips=state_fips,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "\n",
    "    ### STEP 1: Load and prep base layers ###\n",
    "    roads = gpd.read_file(roads_fp).to_crs(crs)\n",
    "    ramps = gpd.read_file(ramps_fp).to_crs(crs)\n",
    "    boundaries = gpd.read_file(boundary_fp).to_crs(crs)\n",
    "    state_boundary = boundaries[boundaries['STUSPS'] == state_abbr]\n",
    "\n",
    "    buffered_water = buffered_water_gdf.to_crs(crs).copy()\n",
    "    buffered_water[\"Present\"] = 0.0\n",
    "    buffered_water[\"waterID\"] = range(1, len(buffered_water) + 1)\n",
    "\n",
    "    ### STEP 2‚Äì10: [Same as your original function...] ###\n",
    "    # Download NAS occurrences\n",
    "    pos_df = get_nas_occurrences(nas_id)\n",
    "    pos_gdf = gpd.GeoDataFrame(\n",
    "        pos_df[[\"decimalLatitude\", \"decimalLongitude\"]],\n",
    "        geometry=gpd.points_from_xy(pos_df.decimalLongitude, pos_df.decimalLatitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(crs)\n",
    "\n",
    "    G = build_network_optimized(roads, precision=6)\n",
    "    kd_tree, node_list = build_nearest_node_index(G)\n",
    "\n",
    "    entry_points = get_road_boundary_intersections(roads, state_boundary)\n",
    "    entry_points[\"intID\"] = range(1, len(entry_points) + 1)\n",
    "    nearest_idx = entry_points.geometry.apply(lambda pt: pos_gdf.distance(pt).idxmin())\n",
    "    entry_points[\"nearest_pres_geom\"] = pos_gdf.loc[nearest_idx].geometry.values\n",
    "    entry_points[\"euclidean_to_pres\"] = entry_points.geometry.distance(entry_points[\"nearest_pres_geom\"])\n",
    "    entry_points[\"road_node\"] = entry_points[\"geometry\"].apply(\n",
    "        lambda geom: find_nearest_node_kd_tree(kd_tree, geom, precision=6)\n",
    "    )\n",
    "    entry_points = entry_points[entry_points[\"road_node\"].notna()]\n",
    "\n",
    "    endpoints = roads['geometry'].apply(get_endpoints).explode()\n",
    "    endpoints_df = pd.DataFrame(\n",
    "        [(pt.x, pt.y) for pt in endpoints if pt is not None],\n",
    "        columns=['x', 'y']\n",
    "    ).drop_duplicates()\n",
    "    endpoints_gdf = gpd.GeoDataFrame(endpoints_df,\n",
    "        geometry=gpd.points_from_xy(endpoints_df['x'], endpoints_df['y']),\n",
    "        crs=roads.crs\n",
    "    ).to_crs(crs).drop(columns=['x', 'y'])\n",
    "    endpoints_gdf['epointID'] = range(1, len(endpoints_gdf) + 1)\n",
    "\n",
    "    ramps = ramps[ramps['State'] == state_abbr][['geometry']].copy()\n",
    "    ramps['rampID'] = range(1, len(ramps) + 1)\n",
    "    ramps_final = assign_ramps_to_water(ramps, buffered_water, snap_distance, crs)\n",
    "\n",
    "    ramp_waters = ramps_final['waterID'].unique()\n",
    "    water_no_ramps = buffered_water[~buffered_water['waterID'].isin(ramp_waters)]\n",
    "\n",
    "    ramps_sj = sjoin_nearest_replace_geom(ramps_final, endpoints_gdf)\n",
    "    lakes_sj = sjoin_nearest_replace_geom(water_no_ramps, endpoints_gdf)\n",
    "    all_endpoints = pd.concat([ramps_sj, lakes_sj])\n",
    "\n",
    "    dijkstra_results = assign_entry_to_endpoint_road_distances(\n",
    "        entry_points_gdf=entry_points,\n",
    "        endpoints_gdf=all_endpoints,\n",
    "        G=G,\n",
    "        kd_tree=kd_tree,\n",
    "        precision=6\n",
    "    )\n",
    "    dist_df = pd.DataFrame(dijkstra_results)\n",
    "    dist_df = dist_df[dist_df['epointID'].notna()]\n",
    "    idx = dist_df.groupby('epointID')['distance_roads'].idxmin()\n",
    "    idx = idx[idx.notna()]\n",
    "    dist_df = dist_df.loc[idx].reset_index(drop=True)\n",
    "\n",
    "    ramps_with_dist = pd.merge(ramps_sj, dist_df, on='epointID', how='left')\n",
    "    lakes_with_dist = pd.merge(lakes_sj, dist_df, on='epointID', how='left')\n",
    "    all_joined = pd.concat([ramps_with_dist, lakes_with_dist])\n",
    "\n",
    "    entry_points_df = entry_points[['intID', 'euclidean_to_pres']].rename(columns={'intID': 'entryID'})\n",
    "    dist_joined = pd.merge(all_joined, entry_points_df, on='entryID', how='left')\n",
    "\n",
    "    dist_joined['distance_to_source'] = dist_joined['distance_roads']\n",
    "    valid_idx = dist_joined.groupby(\"waterID\")[\"distance_to_source\"].idxmin()\n",
    "    valid_idx = valid_idx[valid_idx.notna()]\n",
    "    min_dist_by_water = dist_joined.loc[valid_idx]\n",
    "\n",
    "    water_with_dist = pd.merge(\n",
    "        buffered_water, min_dist_by_water[['waterID', 'distance_to_source']], on='waterID', how='left'\n",
    "    )\n",
    "    max_distance = water_with_dist['distance_to_source'].max()\n",
    "    water_with_dist['distance_to_source'] = water_with_dist['distance_to_source'].fillna(max_distance * 1.1)\n",
    "\n",
    "    return water_with_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701782c3-1c62-42f4-8923-0f6e5e1edf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_distance_results = compute_distance_to_source(\n",
    "    nas_id=nas_id,\n",
    "    state_fips=predict_fip,\n",
    "    state_abbr=predict_state_abbr,\n",
    "    boundary_fp= predict_path + 'tl_2012_us_state.shp',\n",
    "    buffered_water_gdf=buffered_water_predict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7efc76-a01e-4c8c-bce8-14029c1205b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_to_raster(\n",
    "        gdf=predict_distance_results,\n",
    "        output_path=predict_path + predict_state_abbr + \"_distance_road_\" + nas_name + \".tif\",\n",
    "        nas_name=nas_name,\n",
    "        value_field=\"distance_to_source\",\n",
    "        resolution=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249553ef-b341-45e8-ae99-47d94447f86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b627c1e-d186-46d8-a59e-d9f6d03b4d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce7c10-ced5-4b4d-b1e6-7af5b82f5346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58655e35-260b-4357-88a0-0a58fafa7c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d47b84-4c33-4baa-bcc0-dcc7e2ce9083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

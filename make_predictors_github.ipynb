{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc18a25-f28e-419e-adb0-99f9ea9048ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # User defined variables\n",
    "# my_state = 'MN' # State USPS abbreviation\n",
    "# state_name = 'Minnesota'\n",
    "# # IA = 19; ID = 16; IL = 17; MN = 27; MO = 29; MT = 30; OR = 41;  WA = 53; WI = 55\n",
    "# state_fip = 'US%3A27' # Replace last 2 digits with your state's FIP code\n",
    "# my_path = 'data/' + my_state + '/' # leave this alone   \n",
    "# my_crs = 5070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52cac097-3ddf-4747-9f20-2f7415faa655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import requests\n",
    "from shapely.geometry import Point\n",
    "from functools import reduce\n",
    "import folium\n",
    "import io\n",
    "import os\n",
    "import zipfile\n",
    "from matplotlib import pyplot as plt\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import skgstat as skg\n",
    "import gstools as gs\n",
    "from skgstat import models\n",
    "from skgstat.util.likelihood import get_likelihood\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "import pykrige.kriging_tools as kt\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import random\n",
    "from rasterio.features import rasterize\n",
    "import scipy.ndimage\n",
    "from rasterio.transform import from_origin\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from rasterio.mask import mask\n",
    "import rasterio.plot\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import glob\n",
    "from affine import Affine\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "#Helper functions\n",
    "def get_station_data(state):\n",
    "    URL_BASE_2 = 'https://www.waterqualitydata.us/data/Station/search?'\n",
    "    url_request_2 = f\"{URL_BASE_2}countrycode=US&statecode={state}\"\n",
    "    response_2 = pd.read_csv(url_request_2)\n",
    "    return response_2\n",
    "def find_nearest(row, other_gdf):\n",
    "    # Find the index of the nearest geometry\n",
    "    nearest_idx = other_gdf.distance(row.geometry).idxmin()\n",
    "    return other_gdf.loc[nearest_idx]\n",
    "\n",
    "def add_nearest(gdf1, gdf2):\n",
    "    nearest_neighbors = gdf1.apply(lambda row: find_nearest(row, gdf2), axis=1)\n",
    "    # Add nearest neighbor information to the first GeoDataFrame\n",
    "    gdf1['nearest_id'] = nearest_neighbors['station_id']\n",
    "    gdf1['median'] = nearest_neighbors['median']\n",
    "    parameter_added = gdf1\n",
    "    return parameter_added\n",
    "\n",
    "def get_data(state, characteristic, my_format):\n",
    "    URL_BASE = 'https://www.waterqualitydata.us/data/Result/search?'\n",
    "    url_request = f\"{URL_BASE}countrycode=US&statecode={state}&characteristicName={characteristic}&mimeType={my_format}\"\n",
    "    response = pd.read_csv(url_request)\n",
    "    return response\n",
    "\n",
    "def interpolate(stations_w_data, filename, bandname=\"Interpolated_Band\", max_grid_size=500):\n",
    "    # Extract coordinates and values\n",
    "    stations_w_data = stations_w_data.to_crs(my_crs)\n",
    "    stations_w_data['x'] = stations_w_data.geometry.x\n",
    "    stations_w_data['y'] = stations_w_data.geometry.y\n",
    "    samples_df = stations_w_data[['x', 'y', 'median']]\n",
    "    x_point = samples_df['x']\n",
    "    y_point = samples_df['y']\n",
    "    coords = np.column_stack((x_point, y_point))\n",
    "    vals = samples_df['median']\n",
    "    \n",
    "    # Scatter plot of original data\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    art = ax.scatter(coords[:, 0], coords[:, 1], s=10, c=vals, cmap='plasma', vmin=0, vmax=vals.max())\n",
    "    plt.colorbar(art)\n",
    "    plt.title(\"Original Data Points\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Adaptive grid sizing\n",
    "    xmin, xmax = x_point.min(), x_point.max()\n",
    "    ymin, ymax = y_point.min(), y_point.max()\n",
    "    \n",
    "    grid_x = min(max_grid_size, int((xmax - xmin) / 500))\n",
    "    grid_y = min(max_grid_size, int((ymax - ymin) / 500))\n",
    "\n",
    "    use_kriging = grid_x * grid_y <= 1e6\n",
    "    \n",
    "    try:\n",
    "        if use_kriging:\n",
    "            print(\"🔹 Attempting Ordinary Kriging interpolation...\")\n",
    "            \n",
    "            pairwise_distances = pdist(coords)  # Compute pairwise distances between points\n",
    "            maxlag = np.median(pairwise_distances)  # Use median distance\n",
    "            n_lags = min(30, max(10, int(len(coords) / 10)))\n",
    "            V = skg.Variogram(coords, vals, model='exponential', maxlag=maxlag, n_lags=n_lags, normalize=False)\n",
    "            ok = skg.OrdinaryKriging(V, min_points=1, max_points=8, mode='exact')\n",
    "            xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "            field = ok.transform(xx.flatten(), yy.flatten()).reshape(xx.shape)\n",
    "            s2 = ok.sigma.reshape(xx.shape)\n",
    "            print(\"✅ Kriging completed successfully.\")\n",
    "        else:\n",
    "            raise MemoryError  \n",
    "    except (MemoryError, ValueError, np.linalg.LinAlgError):\n",
    "        print(\"⚠️ Kriging failed, using Nearest Neighbor interpolation...\")\n",
    "        xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "        knn = KNeighborsRegressor(n_neighbors=min(8, len(coords)), weights='distance', algorithm='kd_tree', n_jobs=-1)\n",
    "        knn.fit(coords, vals)\n",
    "        query_points = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "        field = knn.predict(query_points).reshape(xx.shape)\n",
    "        s2 = np.zeros_like(field)\n",
    "        print(\"✅ Nearest Neighbor interpolation completed.\")\n",
    "\n",
    "    # Fill missing values\n",
    "    def fill_missing_values(field):\n",
    "        mask = np.isnan(field)\n",
    "        if np.all(mask):\n",
    "            raise ValueError(\"All values are NaN; interpolation cannot be performed.\")\n",
    "        nearest_indices = scipy.ndimage.distance_transform_edt(mask, return_distances=False, return_indices=True)\n",
    "        field[mask] = field[tuple(nearest_indices[i][mask] for i in range(field.ndim))]\n",
    "        return field\n",
    "\n",
    "    field = fill_missing_values(field)\n",
    "\n",
    "    # Export interpolated raster\n",
    "    nrows, ncols = field.shape\n",
    "    xres = (xmax - xmin) / float(ncols)\n",
    "    yres = (ymax - ymin) / float(nrows)\n",
    "    arr = np.abs(field).astype(np.float32)\n",
    "\n",
    "    # Ensure array is properly oriented\n",
    "    arr = np.abs(field.T).astype(np.float32)  # Transpose to fix 90-degree rotation\n",
    "    \n",
    "    # Define transform with corrected Y-axis orientation\n",
    "    transform = from_origin(xmin, ymax, xres, -yres)  # ymax ensures correct vertical alignment\n",
    "    \n",
    "    # Export raster\n",
    "    with rasterio.open(filename, 'w', driver='GTiff',\n",
    "                       height=arr.shape[0], width=arr.shape[1],  # Ensure correct height/width order\n",
    "                       count=1, dtype=str(arr.dtype),\n",
    "                       crs= my_crs,\n",
    "                       transform=transform) as new_dataset:\n",
    "        new_dataset.write(arr, 1)\n",
    "        new_dataset.set_band_description(1, bandname)\n",
    "    \n",
    "    print(f\"✅ Raster saved correctly: {filename}\")\n",
    "    \n",
    "    # Plot corrected raster\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(arr, extent=[xmin, xmax, ymin, ymax], cmap='plasma', origin=\"lower\")  # Corrected orientation\n",
    "    plt.colorbar(label=bandname)\n",
    "    plt.title(\"Interpolated Raster (Corrected Orientation)\")\n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "    plt.show()\n",
    "\n",
    "# This function requests and retrieves the AIS records from the USGS NAS database API\n",
    "def nas_api_call(state):\n",
    "    URL_BASE = 'http://nas.er.usgs.gov/api/v2/'\n",
    "    url_request = f\"{URL_BASE}/occurrence/search?state={state}\"\n",
    "    response = requests.get(url_request, timeout=None).json()\n",
    "    results = pd.json_normalize(response, 'results')\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def make_bg_data(geo_df):    \n",
    "    species_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "    existing_columns = [col for col in species_columns if col in geo_df.columns]  \n",
    "    random_points = []\n",
    "\n",
    "    # Ensure input is in EPSG:5070\n",
    "    if geo_df.crs and geo_df.crs != my_crs:\n",
    "        geo_df = geo_df.to_crs(my_crs)\n",
    "\n",
    "    for _, row in geo_df.iterrows():\n",
    "        if not existing_columns or row[existing_columns].sum() == 0:  \n",
    "            polygon = row.geometry\n",
    "            if polygon.is_empty or not polygon.is_valid:\n",
    "                random_points.append(None)\n",
    "                continue\n",
    "\n",
    "            minx, miny, maxx, maxy = polygon.bounds\n",
    "\n",
    "            while True:\n",
    "                random_point = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "                if polygon.contains(random_point):\n",
    "                    random_points.append(random_point)\n",
    "                    break\n",
    "        else:\n",
    "            random_points.append(None)\n",
    "\n",
    "    # Filter out None values before creating GeoDataFrame\n",
    "    new_gdf = gpd.GeoDataFrame(geometry=[p for p in random_points if p is not None], crs=\"EPSG:5070\")\n",
    "    \n",
    "    return new_gdf\n",
    "\n",
    "# Function to clip point geometries by polygon geometries\n",
    "def clip_points_by_polygon(points_gdf, polygon_gdf):\n",
    "    # Ensure that both GeoDataFrames are in the same CRS\n",
    "    if points_gdf.crs != polygon_gdf.crs:\n",
    "        points_gdf = points_gdf.to_crs(polygon_gdf.crs)\n",
    "\n",
    "    # Clip the points with the polygon(s)\n",
    "    clipped_points = gpd.sjoin(points_gdf, polygon_gdf, how='inner')\n",
    "\n",
    "    # Drop the geometry from polygon_gdf that was added during the join (if needed)\n",
    "    clipped_points = clipped_points.drop(columns=polygon_gdf.columns.difference(['geometry']))\n",
    "\n",
    "    return clipped_points\n",
    "\n",
    "def sum_numeric_columns(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    # Identify columns with six-digit string headers\n",
    "    sum_columns = [col for col in gdf.columns if col.isdigit() and len(col) == 6]\n",
    "    \n",
    "    # Sum across these columns\n",
    "    gdf[\"Native_Fish_Richness\"] = gdf[sum_columns].sum(axis=1)\n",
    "    \n",
    "    # Create a new GeoDataFrame with only the sum and geometry\n",
    "    return gdf[[\"Native_Fish_Richness\", \"geometry\"]]\n",
    "\n",
    "def spatial_join_with_nearest(poly_gdf: gpd.GeoDataFrame, point_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    # Ensure both GeoDataFrames have the same CRS\n",
    "    if poly_gdf.crs != point_gdf.crs:\n",
    "        poly_gdf = poly_gdf.to_crs(point_gdf.crs)\n",
    "\n",
    "    # Perform a nearest spatial join\n",
    "    joined_gdf = gpd.sjoin_nearest(poly_gdf, point_gdf, how=\"left\", distance_col=\"distance\")\n",
    "\n",
    "    return joined_gdf\n",
    "\n",
    "def export_inv_richness(inv_rich_gdf, output_path):\n",
    "    # Convert input data to EPSG:5070 BEFORE getting bounds\n",
    "    inv_rich_gdf = inv_rich_gdf.to_crs(my_crs)\n",
    "    data_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "\n",
    "    if inv_rich_gdf.empty:\n",
    "        raise ValueError(\"Error: The input GeoDataFrame is empty!\")\n",
    "\n",
    "    # Get bounding box in CRS 5070\n",
    "    xmin, ymin, xmax, ymax = inv_rich_gdf.total_bounds  \n",
    "    pixel_size = 100  # 1000 meters per pixel\n",
    "\n",
    "    # Ensure bounds are valid\n",
    "    if xmin == xmax:\n",
    "        xmin -= pixel_size\n",
    "        xmax += pixel_size\n",
    "    if ymin == ymax:\n",
    "        ymin -= pixel_size\n",
    "        ymax += pixel_size\n",
    "\n",
    "    # Recalculate width and height\n",
    "    width = max(1, int((xmax - xmin) / pixel_size))\n",
    "    height = max(1, int((ymax - ymin) / pixel_size))\n",
    "\n",
    "    # Correct transform in CRS 5070\n",
    "    transform = Affine(pixel_size, 0, xmin, 0, -pixel_size, ymax)\n",
    "    num_bands = len(data_columns) + 1  # Extra band for sum\n",
    "    raster = np.zeros((num_bands, height, width), dtype=np.float32)\n",
    "\n",
    "    # Debug: Check valid geometries\n",
    "    valid_geom_count = inv_rich_gdf.geometry.notnull().sum()\n",
    "    print(f\"Valid geometries count: {valid_geom_count}\")\n",
    "\n",
    "    # Rasterize each data column\n",
    "    for i, column in enumerate(data_columns):\n",
    "        shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column]) if geom is not None and not np.isnan(value)]\n",
    "        if not shapes:\n",
    "            print(f\"Skipping {column}: No valid geometries!\")\n",
    "            continue  # Skip empty bands\n",
    "        raster_band = rasterize(\n",
    "            shapes,\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype='float32',\n",
    "            default_value=1,  # Ensure full coverage of waterbody\n",
    "            all_touched=True  # Ensures every pixel within the geometry gets a value\n",
    "        )\n",
    "        shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column])]\n",
    "        raster_band = rasterize(\n",
    "            shapes, out_shape=(height, width), transform=transform, fill=0, dtype='float32', all_touched=True\n",
    "        )\n",
    "        raster[i] = raster_band  # Keep original values\n",
    "        print(f\"Rasterized {column}: Min={raster_band.min()}, Max={raster_band.max()}, Non-zero pixels={np.count_nonzero(raster_band)}\")\n",
    "\n",
    "    # Compute the sum band\n",
    "    raster[-1] = np.sum(raster[:-1], axis=0)\n",
    "    print(f\"Final Sum Band: Min={raster[-1].min()}, Max={raster[-1].max()}, Non-zero pixels={np.count_nonzero(raster[-1])}\")\n",
    "\n",
    "    # Save as a multi-band GeoTIFF in EPSG:5070\n",
    "    with rasterio.open(\n",
    "        output_path, 'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=num_bands,\n",
    "        dtype=raster.dtype,\n",
    "        crs=my_crs,\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        for band in range(num_bands):\n",
    "            dst.write(raster[band], band + 1)\n",
    "        band_names = data_columns + [\"Inv_Richness\"]\n",
    "        for band, name in enumerate(band_names, start=1):\n",
    "            dst.set_band_description(band, name)\n",
    "\n",
    "    # Debug: Plot overlay of geometries on raster\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.imshow(raster[-1], cmap='viridis', extent=[xmin, xmax, ymin, ymax], origin=\"upper\")\n",
    "    gpd.GeoSeries(inv_rich_gdf.geometry).plot(ax=ax, facecolor=\"none\", edgecolor=\"red\")\n",
    "    plt.title(\"Overlaying Geometries on Raster\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot bands\n",
    "    fig, axes = plt.subplots(1, num_bands, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(raster[i], cmap='viridis')\n",
    "        ax.set_title(band_names[i])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Raster file saved as '{output_path}' in CRS {my_crs} with band names: {band_names}\")\n",
    "\n",
    "def export_native_raster(joined_gdf: gpd.GeoDataFrame, resolution: int = 100):\n",
    "    bounds = joined_gdf.total_bounds\n",
    "    transform = rasterio.transform.from_origin(bounds[0], bounds[3], resolution, resolution)\n",
    "    out_shape = (\n",
    "        int(np.ceil((bounds[3] - bounds[1]) / resolution)),  \n",
    "        int(np.ceil((bounds[2] - bounds[0]) / resolution))\n",
    "    )\n",
    "    \n",
    "    column_name = \"Native_Fish_Richness\"  # Change this to dynamically select the column if needed\n",
    "    raster = rasterize(\n",
    "        [(geom, value) for geom, value in zip(joined_gdf.geometry, joined_gdf[column_name])],\n",
    "        out_shape=out_shape,\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype=rasterio.float32\n",
    "    )\n",
    "    \n",
    "    output_filename = f\"{my_path}{my_state}_{column_name}_richness.tif\"\n",
    "    \n",
    "    with rasterio.open(\n",
    "        output_filename, \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=out_shape[0],\n",
    "        width=out_shape[1],\n",
    "        count=1,\n",
    "        dtype=rasterio.float32,\n",
    "        crs=my_crs,\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "        dst.set_band_description(1, column_name)  # Set band name\n",
    "    \n",
    "    # Plot the raster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(raster, cmap='viridis', extent=[bounds[0], bounds[2], bounds[1], bounds[3]])\n",
    "    plt.colorbar(label=f'{column_name} Richness')\n",
    "    plt.title('Rasterized GeoDataFrame')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def combine_geotiffs(input_files, output_file, state):\n",
    "    band_data = []\n",
    "    band_names = []\n",
    "    \n",
    "    # ✅ Use the first raster as a reference grid\n",
    "    with rasterio.open(input_files[5]) as ref_src:\n",
    "        ref_transform = ref_src.transform\n",
    "        ref_crs = ref_src.crs\n",
    "        ref_shape = (ref_src.height, ref_src.width)\n",
    "        ref_bounds = ref_src.bounds\n",
    "        \n",
    "    state_crs = state.crs\n",
    "    \n",
    "    for file in input_files:\n",
    "        with rasterio.open(file) as src:\n",
    "            src_crs = src.crs\n",
    "            src_data = []\n",
    "            \n",
    "            # ✅ Reproject RSD if CRS is different\n",
    "            if src_crs != state_crs:\n",
    "                transform, width, height = calculate_default_transform(\n",
    "                    src_crs, state_crs, ref_shape[1], ref_shape[0], *ref_bounds\n",
    "                )\n",
    "                meta = src.meta.copy()\n",
    "                meta.update({\"crs\": state_crs, \"transform\": transform, \"width\": width, \"height\": height})\n",
    "                \n",
    "                for i in range(1, src.count + 1):\n",
    "                    data = np.empty((height, width), dtype=src.dtypes[i - 1])\n",
    "                    reproject(\n",
    "                        source=src.read(i),\n",
    "                        destination=data,\n",
    "                        src_transform=src.transform,\n",
    "                        src_crs=src.crs,\n",
    "                        dst_transform=transform,\n",
    "                        dst_crs=state_crs,\n",
    "                        resampling=Resampling.nearest\n",
    "                    )\n",
    "                    src_data.append(data)\n",
    "            else:\n",
    "                src_data = [src.read(i) for i in range(1, src.count + 1)]\n",
    "            \n",
    "            # ✅ Preserve band names\n",
    "            for i in range(len(src_data)):\n",
    "                band_name = src.descriptions[i] if src.descriptions and src.descriptions[i] else f\"{file.split('/')[-1].split('.')[0]}_Band{i+1}\"\n",
    "                band_names.append(band_name)\n",
    "\n",
    "            band_data.extend(src_data)\n",
    "\n",
    "    meta.update(count=len(band_data))\n",
    "\n",
    "    with rasterio.open(output_file, 'w', **meta) as dst:\n",
    "        for i, data in enumerate(band_data):\n",
    "            dst.write(data, i + 1)\n",
    "            dst.set_band_description(i + 1, band_names[i])\n",
    "    \n",
    "    print(f\"Successfully created {output_file} with {len(band_data)} bands.\")\n",
    "\n",
    "    # ✅ Check band names\n",
    "    print(\"Band Names:\")\n",
    "    for i, name in enumerate(band_names, start=1):\n",
    "        print(f\"Band {i}: {name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fef787-a519-437c-a806-9919adadc396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download state boundaries shapefile\n",
    "state_boundary_url = 'http://www2.census.gov/geo/tiger/TIGER2012/STATE/tl_2012_us_state.zip'\n",
    "local_path = my_path\n",
    "print('Downloading shapefile...')\n",
    "r = requests.get(state_boundary_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "print(\"Done\")\n",
    "z.extractall(path=local_path) # extract to folder\n",
    "filenames = [y for y in sorted(z.namelist()) for ending in ['dbf', 'prj', 'shp', 'shx'] if y.endswith(ending)] \n",
    "print(filenames)\n",
    "# Get USGS monitoring stations from their API\n",
    "my_stations = get_station_data(state = state_fip)\n",
    "#Filter and convert to a Geodataframe\n",
    "stations = my_stations[['MonitoringLocationIdentifier', 'LatitudeMeasure', 'LongitudeMeasure']].rename(columns = {'MonitoringLocationIdentifier': 'station_id',\n",
    "                        'LatitudeMeasure' : 'latitude', 'LongitudeMeasure' : 'longitude'})  \n",
    "stations_gdf = gpd.GeoDataFrame(\n",
    "    stations, geometry=gpd.points_from_xy(stations.longitude, stations.latitude)).set_crs(4269, allow_override=True)\n",
    "stations_gdf.to_file(my_path + 'usgs_monitoring_stations.shp') # Saving and importing saved files later allows you to import later without remaking variables\n",
    "print('Station and State data saved')\n",
    "stations_gdf = gpd.read_file(my_path + 'usgs_monitoring_stations.shp').set_crs(5070, allow_override=True)\n",
    "state_boundary = gpd.read_file(my_path + 'tl_2012_us_state.shp').dropna().set_crs(5070, allow_override=True)\n",
    "\n",
    "state = state_boundary[state_boundary['STUSPS'] == my_state]\n",
    "my_state_stations = gpd.sjoin(stations_gdf, state, predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd206fc-a6ae-4ad7-a9eb-910b8f011a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = get_data(state = state_fip, my_format = 'csv', characteristic = 'Calcium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b87d24-c9ac-4473-aad4-bc7a2045686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note this takes a while and sometimes the water quality portal API will not respond.  If you get an error, try running again.\n",
    "# A common error when the API is not responding is: IncompleteRead(35814946 bytes read)\n",
    "ca = get_data(state = state_fip, my_format = 'csv', characteristic = 'Calcium')\n",
    "ca_resultsFile = pd.DataFrame(ca).dropna(subset=['ResultMeasureValue'])\n",
    "ca_resultsFile.ResultMeasureValue = pd.to_numeric(ca_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "ca_fixed = ca_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "# Change units for those that are equivalent to mg/L and make ug/L units consistent\n",
    "ca_clean_1 = ca_fixed.replace({'mg/l' : 'mg/L', 'mg/kg' : 'mg/L', 'mg/l CaCO3': 'mg/L', 'ug/l' : 'ug/L'} )\n",
    "# convert ug/L to mg/L\n",
    "ca_clean_1['value'] = ca_clean_1.apply(\n",
    "    lambda row: row['value'] / 1000 if row['unit'] == 'ug/L' else row['value'],\n",
    "    axis=1)\n",
    "#Change unit for converted values to mg/L\n",
    "ca_clean_2 = ca_clean_1.replace({'ug/L': 'mg/L'})\n",
    "#Get rid of other records not in mg/L\n",
    "my_ca = ca_clean_2[ca_clean_2['unit'] == 'mg/L']\n",
    "#Filter to reasonable values\n",
    "ca_fixed_2 = my_ca[my_ca['value'] < 300]\n",
    "ca_fixed_3 = ca_fixed_2[ca_fixed_2['value'] > 0]\n",
    "#Calculate summary stats by station_id\n",
    "ca_result = pd.DataFrame(ca_fixed_3.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "stations_w_ca = pd.merge(my_state_stations, ca_result, on = 'station_id').dropna()\n",
    "ca_station_list = stations_w_ca['station_id'].tolist()\n",
    "stations_no_ca = my_state_stations[~my_state_stations['station_id'].isin(ca_station_list)]\n",
    "ca = add_nearest(stations_no_ca, stations_w_ca)\n",
    "my_ca_result = pd.concat([ca, stations_w_ca], axis = 0).drop(columns = {\"nearest_id\",\"latitude\",\"longitude\"})\n",
    "my_ca_result.to_file(my_path + 'usgs_ca.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f372ffa-4d05-4a17-87a1-d6b0eb5e73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pH = get_data(state = state_fip, my_format = 'csv', characteristic = 'pH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cf75a-b560-40bb-8c66-f23bb6e46331",
   "metadata": {},
   "outputs": [],
   "source": [
    "pH_resultsFile =pd.DataFrame(pH).dropna(subset=['ResultMeasureValue'])\n",
    "pH_resultsFile.ResultMeasureValue = pd.to_numeric(pH_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "pH_fixed = pH_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "my_pH = pH_fixed.loc[(pH_fixed['unit'].isna()) | (pH_fixed['unit']== 'std units')]\n",
    "pH_result = pd.DataFrame(my_pH.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "pH_fixed_2 = pH_result[pH_result['median'] < 14] \n",
    "pH_fixed_3 = pH_fixed_2[pH_fixed_2['median'] > 4]\n",
    "stations_w_pH = pd.merge(my_state_stations, pH_fixed_3, on = 'station_id').dropna()\n",
    "pH_station_list = stations_w_pH['station_id'].tolist()\n",
    "stations_no_pH = my_state_stations[~my_state_stations['station_id'].isin(pH_station_list)]\n",
    "pH = add_nearest(stations_no_pH, stations_w_pH)\n",
    "my_pH_result = pd.concat([pH, stations_w_pH], axis = 0).drop(columns = {\"nearest_id\", \"latitude\", \"longitude\"})\n",
    "my_pH_result.to_file(my_path + 'usgs_pH.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775a8fa-6fe1-44c1-ac0e-22191f3c5651",
   "metadata": {},
   "outputs": [],
   "source": [
    "di_N = get_data(state = state_fip, my_format = 'csv', characteristic = 'Nitrogen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4add793-1159-4f07-a559-47904ab0b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "di_N_resultsFile = pd.DataFrame(di_N).dropna(subset=['ResultMeasureValue'])\n",
    "di_N_resultsFile.ResultMeasureValue = pd.to_numeric(di_N_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "di_N_fixed = di_N_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "di_N_fixed.loc[(di_N_fixed['unit'] == 'ug/L') | \n",
    "       (di_N_fixed['unit'] == 'ppb') | (di_N_fixed['unit'] == 'mg/g'), 'value'] /= 1000\n",
    "di_N_clean_1 = di_N_fixed.replace({'mg/l' : 'mg/L', 'mg/kg' : 'mg/L', 'ppb': 'mg/L', 'ug/L' : 'mg/L'})\n",
    "di_fixed_2 = di_N_clean_1[di_N_clean_1['unit'] == 'mg/L']\n",
    "di_fixed_3 = di_fixed_2[di_fixed_2['value']<= 500]\n",
    "di_fixed_4 = di_fixed_3[di_fixed_3['value'] > 0]\n",
    "di_N_result = pd.DataFrame(di_fixed_4.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "stations_w_di_N = pd.merge(my_state_stations, di_N_result, on = 'station_id')\n",
    "di_N_station_list = stations_w_di_N['station_id'].tolist()\n",
    "stations_no_di_N = my_state_stations[~my_state_stations['station_id'].isin(di_N_station_list)]\n",
    "di_N = add_nearest(stations_no_di_N, stations_w_di_N)\n",
    "my_di_N_result = pd.concat([di_N, stations_w_di_N], axis = 0).drop(columns = {\"nearest_id\", \"latitude\",\"longitude\"})\n",
    "my_di_N_result.to_file(my_path + 'usgs_N.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b1fc4-a47e-4626-aa3a-16f1b97503bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "do = get_data(state = state_fip, my_format = 'csv', characteristic = 'Oxygen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf9dca-0184-413b-9343-23719f7ba27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "do_resultsFile = pd.DataFrame(do).dropna(subset=['ResultMeasureValue'])\n",
    "do_resultsFile.ResultMeasureValue = pd.to_numeric(do_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "do_fixed = do_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "do_fixed.loc[(do_fixed['unit'] == 'mg/L'), 'value'] *= 12.67\n",
    "do_clean_1 = do_fixed.replace({'mg/L' : '% saturatn', '% by vol' : '% saturatn'})\n",
    "do_fixed_2 = do_clean_1[do_clean_1['value']<= 1]\n",
    "do_fixed_3 = do_fixed_2[do_fixed_2['value']> 0]\n",
    "do_result = pd.DataFrame(do_fixed_3.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "stations_w_do = pd.merge(my_state_stations, do_result, on = 'station_id')\n",
    "do_station_list = stations_w_do['station_id'].tolist()\n",
    "stations_no_do = my_state_stations[~my_state_stations['station_id'].isin(do_station_list)]\n",
    "do = add_nearest(stations_no_do, stations_w_do)\n",
    "my_do_result = pd.concat([do, stations_w_do], axis = 0).drop(columns = {\"nearest_id\", \"latitude\",\"longitude\"})\n",
    "my_do_result.to_file(my_path + 'usgs_do.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aaa277-97cb-4e96-b87e-70e15ccceec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "phos = get_data(state = state_fip, my_format = 'csv', characteristic = 'Phosphorus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26519e-aa8f-4ba0-b884-386bdcb1cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "phos_resultsFile = pd.DataFrame(phos).dropna(subset=['ResultMeasureValue'])\n",
    "phos_resultsFile.ResultMeasureValue = pd.to_numeric(phos_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "phos_fixed = phos_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "phos_fixed.loc[(phos_fixed['unit'] == 'ug/L') | \n",
    "       (phos_fixed['unit'] == 'ppb') | (phos_fixed['unit'] == 'mg/g'), 'value'] /= 1000\n",
    "phos_clean_1 = phos_fixed.replace({'mg/l' : 'mg/L', 'mg/l PO4' : 'mg/L', 'mg/l as P' : 'mg/L', 'mg/kg' : 'mg/L', 'mg/kg as P' : 'mg/L',\n",
    "                                  'ug/L' : 'mg/L', 'ppb' : 'mg/L', 'mg/g' : 'mg/L'} )\n",
    "my_phos = phos_clean_1[phos_clean_1['unit'] == 'mg/L']\n",
    "phos_fixed_2 = my_phos[my_phos['value']<= 1]\n",
    "phos_result = pd.DataFrame(phos_fixed_2.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "stations_w_phos = pd.merge(my_state_stations, phos_result, on = 'station_id')\n",
    "phos_station_list = stations_w_phos['station_id'].tolist()\n",
    "stations_no_phos = my_state_stations[~my_state_stations['station_id'].isin(phos_station_list)]\n",
    "phos = add_nearest(stations_no_phos, stations_w_phos)\n",
    "my_phos_result = pd.concat([phos, stations_w_phos], axis = 0).drop(columns = {\"nearest_id\", \"latitude\",\"longitude\"})\n",
    "my_phos_result.to_file(my_path + 'usgs_phos.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10750f-473e-4d16-9335-13702b340562",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ca_result = gpd.read_file(my_path + 'usgs_ca.shp')\n",
    "interpolate(my_ca_result, 'ca.tif', 'Ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f62f2b-8f7a-4595-b90a-9b9c39cc5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pH_result = gpd.read_file(my_path + 'usgs_pH.shp')\n",
    "interpolate(my_pH_result, 'pH.tif', 'pH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebd38d-ab62-4dfd-b26d-013b984c8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_di_N_result = gpd.read_file(my_path + 'usgs_N.shp')\n",
    "interpolate(my_di_N_result, 'di_N.tif', 'Nitrogen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833a24d-7038-4b02-9d3b-94f40d9b7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_do_result = gpd.read_file(my_path + 'usgs_do.shp')\n",
    "interpolate(my_do_result, 'do.tif', 'DO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cce07c-fcda-4652-befb-ad4f1cd729a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_phos_result = gpd.read_file(my_path + 'usgs_phos.shp')\n",
    "interpolate(my_phos_result, 'phos.tif', 'Phos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994216e-871c-4035-9c98-d7e3b837dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of Biological Variables\n",
    "# Download waterbody shapefiles by state from NHD \n",
    "URL_BASE_NHD = 'https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHD/State/Shape/'\n",
    "NHD_url = f\"{URL_BASE_NHD}NHD_H_{state_name}_State_Shape.zip\"\n",
    "local_path = my_path\n",
    "print('Downloading shapefile...')\n",
    "r = requests.get(NHD_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "print(\"Done\")\n",
    "z.extractall(path=local_path) # extract to folder\n",
    "filenames = [y for y in sorted(z.namelist()) for ending in ['dbf', 'prj', 'shp', 'shx'] if y.endswith(ending)] \n",
    "print(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c25749e1-573e-4404-bba9-751956e8a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined variables\n",
    "my_state = 'IA' # State USPS abbreviation\n",
    "state_name = 'Iowa'\n",
    "# IA = 19; ID = 16; IL = 17; MN = 27; MO = 29; MT = 30; OR = 41;  WA = 53; WI = 55\n",
    "state_fip = 'US%3A19' # Replace last 2 digits with your state's FIP code\n",
    "my_path = 'data/' + my_state + '/' # leave this alone   \n",
    "my_crs = 5070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcb2a4d-5d1a-412c-bf08-3b46976ddc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged shapefiles into a single GeoDataFrame.\n",
      "Imported 1 shapefiles\n"
     ]
    }
   ],
   "source": [
    "# Import stream files\n",
    "# Find all shapefiles that include \"NHDFlowline_\" in the filename\n",
    "shapefiles = glob.glob(os.path.join(my_path + \"/shape/\", \"*NHDFlowline_*.shp\")) + glob.glob(os.path.join(my_path + \"/shape/\", \"NHDFlowline.shp\"))\n",
    "\n",
    "# Ensure shapefiles were found\n",
    "if not shapefiles:\n",
    "    print(\"No shapefiles found matching the pattern.\")\n",
    "\n",
    "# Load all shapefiles into a list of GeoDataFrames\n",
    "gdfs = [gpd.read_file(shp) for shp in shapefiles]\n",
    "\n",
    "# Optionally, concatenate all shapefiles into a single GeoDataFrame\n",
    "if gdfs:  # Only concatenate if the list is not empty\n",
    "    stream_gdf = gpd.pd.concat(gdfs, ignore_index=True)\n",
    "    print(\"Successfully merged shapefiles into a single GeoDataFrame.\")\n",
    "else:\n",
    "    stream_gdf = None\n",
    "    print(\"No valid shapefiles to merge.\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Imported {len(gdfs)} shapefiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70bb37-527d-4a7c-beda-3b86f7af2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lakes and rivers filter and set coordinate reference system (crs)\n",
    "lakes = gpd.read_file(my_path + \"/shape/NHDWaterbody.shp\").to_crs(4269)\n",
    "rivers = gpd.read_file(my_path + \"/shape/NHDArea.shp\").to_crs(4269)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59138f96-0ff1-4638-b61a-a73b994b8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge lakes and rivers\n",
    "lakes_rivers = pd.concat([lakes, rivers]).set_crs(lakes.crs, allow_override=True)\n",
    "# Filter lakes and rivers by size\n",
    "my_lakes_rivers = lakes_rivers[lakes_rivers['areasqkm'] >= 0.25].copy()\n",
    "# Load streams and filter by length\n",
    "my_streams = stream_gdf[stream_gdf['lengthkm'] >= 5].set_crs(lakes.crs, allow_override=True)\n",
    "# Remove streams that intersect lakes/rivers\n",
    "my_streams = my_streams.sjoin(lakes_rivers, predicate='intersects', how='left', rsuffix='lake', lsuffix='stream')\n",
    "# Keep only non-intersecting streams\n",
    "my_streams = my_streams[my_streams['index_lake'].isna()].drop(columns=['index_lake'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734a56c-6fba-4755-8ced-43b68d31e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lakes_rivers = my_lakes_rivers[['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283d0568-ec2e-4f28-b717-214833148d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_streams = my_streams[['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08e228-7733-4a1e-b1b4-deea1d7ead25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Apply Buffering**\n",
    "my_lakes_rivers['buffer'] = my_lakes_rivers.buffer(0.01)  # 1,110 meters buffer\n",
    "my_streams['buffer'] = my_streams.buffer(0.01)  # 1,110 meters buffer\n",
    "\n",
    "# **Merge Water Layers**\n",
    "my_water = pd.concat([my_lakes_rivers, my_streams]).set_crs(lakes_rivers.crs, allow_override=True)\n",
    "my_water['waterID'] = range(1, len(my_water) + 1)\n",
    "# **Replace Geometry with Buffered One**\n",
    "buffered_water = my_water.drop(columns=['geometry']).set_geometry('buffer').rename_geometry('geometry')\n",
    "\n",
    "# **Save and Plot**\n",
    "buffered_water.to_file(my_path + my_state + \"_buffered_water.shp\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "buffered_water.plot(ax=ax, color=\"blue\", linewidth=0.5)\n",
    "\n",
    "ax.set_title(\"Buffered Water with Unique waterbody_ID\", fontsize=14)\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a38b88-6cea-4cf9-bb18-f258ccc1b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_df = nas_api_call(my_state)\n",
    "all_nas_df = pd.DataFrame(nas_df)\n",
    "all_nas_data = all_nas_df[[\"speciesID\", \"commonName\", \"group\", \"state\", \"decimalLatitude\", \"decimalLongitude\", \"year\", \"status\"]]\n",
    "all_nas_data_fltr = all_nas_data[(all_nas_data['status'] == 'established')].dropna()\n",
    "nas_gdf = gpd.GeoDataFrame(\n",
    "    all_nas_data_fltr, geometry=gpd.points_from_xy(all_nas_data_fltr.decimalLongitude, all_nas_data_fltr.decimalLatitude))#.set_crs(my_crs)\n",
    "nas_gdf.to_file(my_path + my_state + \"_nas.shp\") # Save file so you can start at the next block next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5abf5-8798-40e7-8c10-714a74d708bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Invasive Predictors\n",
    "nas_gdf = gpd.read_file(my_path + my_state + \"_nas.shp\")\n",
    "nas_gdf = nas_gdf.set_crs(4269, allow_override=True)\n",
    "for col in nas_gdf.select_dtypes(include=['int64']).columns:\n",
    "    nas_gdf[col] = nas_gdf[col].astype('float64')\n",
    "\n",
    "buffered_water = gpd.read_file(my_path + my_state + \"_buffered_water.shp\")\n",
    "\n",
    "NAS_ais_obs_df = gpd.sjoin(nas_gdf, buffered_water, how=\"inner\")\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Algae', 'Inv_Algae')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Plants', 'Inv_Plants')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Fishes', 'Inv_Fish')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Crustaceans-Cladocerans', 'Inv_Crustaceans')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Crustaceans-Cladocerans', 'Inv_Crustaceans')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Crustaceans-Amphipods', 'Inv_Crustaceans')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Mollusks-Bivalves', 'Inv_Mollusks')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Mollusks-Gastropods', 'Inv_Mollusks')\n",
    "NAS_ais_df = NAS_ais_obs_df[['waterID', 'commonName', 'group']]\n",
    "\n",
    "unique_commonnames = NAS_ais_df.groupby(['waterID', 'group'])['commonName'].nunique().reset_index()\n",
    "pivot_df = unique_commonnames.pivot(index='waterID', columns='group', values='commonName').reset_index().fillna(0)\n",
    "lakes_w_invasives = pd.merge(buffered_water, pivot_df, on = 'waterID', how = 'left')\n",
    "\n",
    "# Define the required columns\n",
    "species_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants', 'geometry']\n",
    "\n",
    "# Add missing columns and fill with 0\n",
    "for col in species_columns:\n",
    "    if col not in lakes_w_invasives.columns:\n",
    "        lakes_w_invasives[col] = 0  # Add missing column with default value 0\n",
    "\n",
    "# Now safely select the columns and fill NaN values with 0\n",
    "inv_rich = lakes_w_invasives[species_columns].fillna(0)\n",
    "for col in inv_rich.select_dtypes(include=['int64']).columns:\n",
    "    inv_rich[col] = inv_rich[col].astype('float64')\n",
    "export_inv_richness(inv_rich, my_path + my_state + '_inv_richness.tif')\n",
    "# Create your background data input and export\n",
    "bg_gdf = make_bg_data(inv_rich)  # Generate random points only for selected polygons\n",
    "bg_gdf.to_file(my_path + my_state +'_bg.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27b59c-daaf-4afc-b1a7-15f6a2e7129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Native Fish \n",
    "# Download data from USGS native fish presence/absence dataset\n",
    "URL_BASE = \"https://www.sciencebase.gov/catalog/file/get/6086df60d34eadd49d31b04a?f=__disk__ad%2F21%2Ffc%2Fad21fc677379f4e45caa4bd506ca1c587d5f01f7\"\n",
    "\n",
    "response = requests.get(URL_BASE)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Convert response content to DataFrame\n",
    "    csv_data = StringIO(response.text)\n",
    "    fish_df = pd.read_csv(csv_data)\n",
    "\n",
    "    # Convert DataFrame to GeoDataFrame\n",
    "    fish_gdf = gpd.GeoDataFrame(\n",
    "        fish_df, geometry=gpd.points_from_xy(fish_df.longitude, fish_df.latitude)\n",
    "    ).set_crs(4269).to_crs(5070)\n",
    "else:\n",
    "    print(f\"Error: Failed to download data (status code {response.status_code})\")\n",
    "\n",
    "\n",
    "state_boundary = gpd.read_file(my_path + 'tl_2012_us_state.shp').dropna().to_crs(5070)\n",
    "state = state_boundary[state_boundary['STUSPS'] == my_state]\n",
    "# Clip the points by the polygon\n",
    "clipped_native_fish = clip_points_by_polygon(fish_gdf, state)\n",
    "# Save to shapefile\n",
    "clipped_native_fish.to_file(my_path + my_state + '_native_fish_gdf.shp')\n",
    "#clipped_native_fish = gpd.read_file(my_path + my_state + '_native_fish_gdf.shp')\n",
    "native_fish_gdf = sum_numeric_columns(clipped_native_fish)#.to_crs(4269)\n",
    "water_w_native_fish = spatial_join_with_nearest(buffered_water, native_fish_gdf)\n",
    "for col in water_w_native_fish.select_dtypes(include=['int64']).columns:\n",
    "    water_w_native_fish[col] = water_w_native_fish[col].astype('float64')\n",
    "export_native_raster(water_w_native_fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42df5dc-c332-47fc-952f-065fe5c5e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files = [\n",
    "    my_path + \"ca.tif\", my_path + \"pH.tif\", my_path + \"di_N.tif\", my_path + \"do.tif\", my_path + \"phos.tif\", \n",
    "    my_path + my_state + \"_rsd.tif\", my_path + my_state + \"_inv_richness.tif\", \n",
    "    my_path + my_state + \"_Native_Fish_Richness.tif\"\n",
    "]\n",
    "\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file = my_path + my_state + \"_combined.tif\"\n",
    "combine_geotiffs(input_files, output_file, state)\n",
    "\n",
    "print(f\"✅ Multi-band raster saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3ac17-3d7c-43af-ab4c-b689bc39d120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e61629-1f62-44bd-90bb-ef8f30142f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

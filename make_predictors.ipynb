{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc18a25-f28e-419e-adb0-99f9ea9048ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined variables\n",
    "my_state = 'ID' # State USPS abbreviation\n",
    "state_name = 'Idaho'\n",
    "# IA = 19; ID = 16; IL = 17; MN = 27; MO = 29; MT = 30; OR = 41;  WA = 53; WI = 55\n",
    "state_fip = 'US%3A16' # Replace last 2 digits with your state's FIP code\n",
    "my_path = 'data/' + my_state + '/' # leave this alone   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52cac097-3ddf-4747-9f20-2f7415faa655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import requests\n",
    "from shapely.geometry import Point\n",
    "from functools import reduce\n",
    "import folium\n",
    "import io\n",
    "import os\n",
    "import zipfile\n",
    "from matplotlib import pyplot as plt\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import skgstat as skg\n",
    "import gstools as gs\n",
    "from skgstat import models\n",
    "from skgstat.util.likelihood import get_likelihood\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "import pykrige.kriging_tools as kt\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import random\n",
    "from rasterio.features import rasterize\n",
    "import scipy.ndimage\n",
    "from rasterio.transform import from_origin\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from rasterio.mask import mask\n",
    "import rasterio.plot\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import glob\n",
    "\n",
    "#Helper functions\n",
    "def get_station_data(state):\n",
    "    URL_BASE_2 = 'https://www.waterqualitydata.us/data/Station/search?'\n",
    "    url_request_2 = f\"{URL_BASE_2}countrycode=US&statecode={state}\"\n",
    "    response_2 = pd.read_csv(url_request_2)\n",
    "    return response_2\n",
    "def find_nearest(row, other_gdf):\n",
    "    # Find the index of the nearest geometry\n",
    "    nearest_idx = other_gdf.distance(row.geometry).idxmin()\n",
    "    return other_gdf.loc[nearest_idx]\n",
    "\n",
    "def add_nearest(gdf1, gdf2):\n",
    "    nearest_neighbors = gdf1.apply(lambda row: find_nearest(row, gdf2), axis=1)\n",
    "    # Add nearest neighbor information to the first GeoDataFrame\n",
    "    gdf1['nearest_id'] = nearest_neighbors['station_id']\n",
    "    gdf1['median'] = nearest_neighbors['median']\n",
    "    parameter_added = gdf1\n",
    "    return parameter_added\n",
    "\n",
    "def get_data(state, characteristic, my_format):\n",
    "    URL_BASE = 'https://www.waterqualitydata.us/data/Result/search?'\n",
    "    url_request = f\"{URL_BASE}countrycode=US&statecode={state}&characteristicName={characteristic}&mimeType={my_format}\"\n",
    "    response = pd.read_csv(url_request)\n",
    "    return response\n",
    "\n",
    "def interpolate(stations_w_data, filename, bandname=\"Interpolated_Band\", max_grid_size=500):\n",
    "    # Extract coordinates and values\n",
    "    stations_w_data = stations_w_data.to_crs(epsg=5070)\n",
    "    # Extract coordinates *after* transformation\n",
    "    stations_w_data['x'] = stations_w_data.geometry.x\n",
    "    stations_w_data['y'] = stations_w_data.geometry.y\n",
    "    samples_df = stations_w_data[['x', 'y', 'median']]\n",
    "    x_point = samples_df['x']\n",
    "    y_point = samples_df['y']\n",
    "    coords = np.column_stack((x_point, y_point))\n",
    "    vals = samples_df['median']\n",
    "    \n",
    "    # Scatter plot of original data\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    art = ax.scatter(coords[:, 0], coords[:, 1], s=10, c=vals, cmap='plasma', vmin=0, vmax=vals.max())\n",
    "    plt.colorbar(art)\n",
    "    \n",
    "    # Adaptive grid sizing to prevent memory overflow\n",
    "    xmin, xmax = x_point.min(), x_point.max()\n",
    "    ymin, ymax = y_point.min(), y_point.max()\n",
    "    \n",
    "    grid_x = min(max_grid_size, int((xmax - xmin) / 500))\n",
    "    grid_y = min(max_grid_size, int((ymax - ymin) / 500))\n",
    "\n",
    "    if grid_x * grid_y > 1e6:  # If interpolation grid is too large, skip Kriging\n",
    "        print(\"‚ö†Ô∏è Grid too large for Kriging, switching to Nearest Neighbor interpolation.\")\n",
    "        use_kriging = False\n",
    "    else:\n",
    "        use_kriging = True\n",
    "    \n",
    "    try:\n",
    "        if use_kriging:\n",
    "            print(\"üîπ Attempting Ordinary Kriging interpolation...\")\n",
    "            V = skg.Variogram(coords, vals, model='exponential', maxlag='median', n_lags=30, normalize=False)\n",
    "            V.plot(show=False)\n",
    "            print(f'Exponential RMSE: {V.rmse:.2f}')\n",
    "            print(f'Exponential effective range: {V.describe().get(\"effective_range\"):.1f}')\n",
    "            \n",
    "            ok = skg.OrdinaryKriging(V, min_points=1, max_points=8, mode='exact')\n",
    "            xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]  # Grid definition\n",
    "            field = ok.transform(xx.flatten(), yy.flatten()).reshape(xx.shape)\n",
    "            s2 = ok.sigma.reshape(xx.shape)\n",
    "            print(\"‚úÖ Kriging completed successfully.\")\n",
    "        else:\n",
    "            raise MemoryError  # Manually trigger fallback\n",
    "\n",
    "    except (MemoryError, ValueError, np.linalg.LinAlgError):\n",
    "        print(\"‚ö†Ô∏è Kriging failed due to memory constraints or numerical issues. Switching to Nearest Neighbor interpolation...\")\n",
    "\n",
    "        # Nearest Neighbor Interpolation as fallback\n",
    "        xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]  # Ensure the same grid as before\n",
    "        knn = KNeighborsRegressor(n_neighbors=8, weights='distance', algorithm='kd_tree', n_jobs=-1)\n",
    "        knn.fit(coords, vals)\n",
    "\n",
    "        query_points = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "        nn_values = knn.predict(query_points).reshape(xx.shape)\n",
    "        field = nn_values.astype(np.float32)\n",
    "        s2 = np.zeros_like(field)  # No error estimate for Nearest Neighbor interpolation\n",
    "        print(\"‚úÖ Nearest Neighbor interpolation completed successfully.\")\n",
    "    \n",
    "    # Fill NaN values using nearest neighbor interpolation\n",
    "    def fill_missing_values(field):\n",
    "        mask = np.isnan(field)  # Identify NaN locations\n",
    "        if np.all(mask):\n",
    "            raise ValueError(\"All values are NaN; interpolation cannot be performed.\")\n",
    "    \n",
    "        # Compute nearest neighbor indices for NaN positions\n",
    "        nearest_indices = scipy.ndimage.distance_transform_edt(mask, return_distances=False, return_indices=True)\n",
    "    \n",
    "        # Assign nearest valid values to NaN locations\n",
    "        field[mask] = field[tuple(nearest_indices[i][mask] for i in range(field.ndim))]\n",
    "    \n",
    "        return field\n",
    "\n",
    "    field = fill_missing_values(field)\n",
    "\n",
    "    # Export interpolated raster\n",
    "    nrows, ncols = np.shape(field.T)\n",
    "    xres = (xmax - xmin) / float(ncols)\n",
    "    yres = (ymax - ymin) / float(nrows)\n",
    "    arr = np.abs(field.T).astype(np.float32)\n",
    "\n",
    "    transform = from_origin(xmin, ymin, xres, -yres)\n",
    "    export_path = my_path + filename  # Assuming filename is already the full path\n",
    "\n",
    "    with rasterio.open(export_path, 'w', driver='GTiff',\n",
    "                       height=arr.shape[0], width=arr.shape[1],\n",
    "                       count=1, dtype=str(arr.dtype),\n",
    "                       crs=\"EPSG:5070\",\n",
    "                       transform=transform) as new_dataset:\n",
    "        new_dataset.write(arr, 1)\n",
    "        new_dataset.set_band_description(1, bandname)\n",
    "    \n",
    "    print(f\"Raster saved: {filename}\")\n",
    "# This function requests and retrieves the AIS records from the USGS NAS database API\n",
    "def nas_api_call(nas_id, state):\n",
    "    responses = []\n",
    "    URL_BASE = 'http://nas.er.usgs.gov/api/v2/'\n",
    "    url_request = f\"{URL_BASE}/occurrence/search?species_ID={nas_id}&state={my_state}\"\n",
    "    response = requests.get(url_request, timeout=None)\n",
    "    responses.append(response.json())\n",
    "    return responses\n",
    "\n",
    "def make_bg_data(geo_df):    \n",
    "    species_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "    existing_columns = [col for col in species_columns if col in geo_df.columns]  # Only use existing columns\n",
    "    random_points = []\n",
    "\n",
    "    for _, row in geo_df.iterrows():\n",
    "        if not existing_columns or row[existing_columns].sum() == 0:  # Proceed if columns don't exist OR sum is 0\n",
    "            polygon = row.geometry\n",
    "            if polygon.is_empty or not polygon.is_valid:\n",
    "                random_points.append(None)\n",
    "                continue\n",
    "\n",
    "            minx, miny, maxx, maxy = polygon.bounds\n",
    "\n",
    "            while True:\n",
    "                random_point = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "                if polygon.contains(random_point):\n",
    "                    random_points.append(random_point)\n",
    "                    break\n",
    "        else:\n",
    "            random_points.append(None)  # Keep None for rows that don't match the condition\n",
    "\n",
    "    return gpd.GeoDataFrame(geometry=random_points, crs=\"EPSG:5070\")\n",
    "\n",
    "# Function to clip point geometries by polygon geometries\n",
    "def clip_points_by_polygon(points_gdf, polygon_gdf):\n",
    "    # Ensure that both GeoDataFrames are in the same CRS\n",
    "    if points_gdf.crs != polygon_gdf.crs:\n",
    "        points_gdf = points_gdf.to_crs(polygon_gdf.crs)\n",
    "\n",
    "    # Clip the points with the polygon(s)\n",
    "    clipped_points = gpd.sjoin(points_gdf, polygon_gdf, how='inner')\n",
    "\n",
    "    # Drop the geometry from polygon_gdf that was added during the join (if needed)\n",
    "    clipped_points = clipped_points.drop(columns=polygon_gdf.columns.difference(['geometry']))\n",
    "\n",
    "    return clipped_points\n",
    "\n",
    "def sum_numeric_columns(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    # Identify columns with six-digit string headers\n",
    "    sum_columns = [col for col in gdf.columns if col.isdigit() and len(col) == 6]\n",
    "    \n",
    "    # Sum across these columns\n",
    "    gdf[\"Native_Fish_Richness\"] = gdf[sum_columns].sum(axis=1)\n",
    "    \n",
    "    # Create a new GeoDataFrame with only the sum and geometry\n",
    "    return gdf[[\"Native_Fish_Richness\", \"geometry\"]]\n",
    "\n",
    "def spatial_join_with_nearest(poly_gdf: gpd.GeoDataFrame, point_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    # Ensure both GeoDataFrames have the same CRS\n",
    "    if poly_gdf.crs != point_gdf.crs:\n",
    "        poly_gdf = poly_gdf.to_crs(point_gdf.crs)\n",
    "\n",
    "    # Perform a nearest spatial join\n",
    "    joined_gdf = gpd.sjoin_nearest(poly_gdf, point_gdf, how=\"left\", distance_col=\"distance\")\n",
    "\n",
    "    return joined_gdf\n",
    "\n",
    "\n",
    "\n",
    "def combine_geotiffs(input_files, output_file, state):\n",
    "    band_data = []\n",
    "    band_names = []\n",
    "    \n",
    "    # ‚úÖ Use the first raster as a reference grid\n",
    "    with rasterio.open(input_files[0]) as ref_src:\n",
    "        ref_transform = ref_src.transform\n",
    "        ref_crs = ref_src.crs\n",
    "        ref_shape = (ref_src.height, ref_src.width)\n",
    "        ref_bounds = ref_src.bounds\n",
    "        \n",
    "    state_crs = state.crs\n",
    "    \n",
    "    for file in input_files:\n",
    "        with rasterio.open(file) as src:\n",
    "            src_crs = src.crs\n",
    "            src_data = []\n",
    "            \n",
    "            # ‚úÖ Reproject RSD if CRS is different\n",
    "            if src_crs != state_crs:\n",
    "                transform, width, height = calculate_default_transform(\n",
    "                    src_crs, state_crs, ref_shape[1], ref_shape[0], *ref_bounds\n",
    "                )\n",
    "                meta = src.meta.copy()\n",
    "                meta.update({\"crs\": state_crs, \"transform\": transform, \"width\": width, \"height\": height})\n",
    "                \n",
    "                for i in range(1, src.count + 1):\n",
    "                    data = np.empty((height, width), dtype=src.dtypes[i - 1])\n",
    "                    reproject(\n",
    "                        source=src.read(i),\n",
    "                        destination=data,\n",
    "                        src_transform=src.transform,\n",
    "                        src_crs=src.crs,\n",
    "                        dst_transform=transform,\n",
    "                        dst_crs=state_crs,\n",
    "                        resampling=Resampling.nearest\n",
    "                    )\n",
    "                    src_data.append(data)\n",
    "            else:\n",
    "                src_data = [src.read(i) for i in range(1, src.count + 1)]\n",
    "            \n",
    "            # ‚úÖ Preserve band names\n",
    "            for i in range(len(src_data)):\n",
    "                band_name = src.descriptions[i] if src.descriptions and src.descriptions[i] else f\"{file.split('/')[-1].split('.')[0]}_Band{i+1}\"\n",
    "                band_names.append(band_name)\n",
    "\n",
    "            band_data.extend(src_data)\n",
    "\n",
    "    meta.update(count=len(band_data))\n",
    "\n",
    "    with rasterio.open(output_file, 'w', **meta) as dst:\n",
    "        for i, data in enumerate(band_data):\n",
    "            dst.write(data, i + 1)\n",
    "            dst.set_band_description(i + 1, band_names[i])\n",
    "    \n",
    "    print(f\"Successfully created {output_file} with {len(band_data)} bands.\")\n",
    "\n",
    "    # ‚úÖ Check band names\n",
    "    print(\"Band Names:\")\n",
    "    for i, name in enumerate(band_names, start=1):\n",
    "        print(f\"Band {i}: {name}\")\n",
    "\n",
    "def export_inv_richness(inv_rich_gdf, output_path):\n",
    "    # Convert input data to EPSG:5070 BEFORE getting bounds\n",
    "    inv_rich_gdf = inv_rich_gdf.to_crs(5070)\n",
    "    data_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "\n",
    "    if inv_rich_gdf.empty:\n",
    "        raise ValueError(\"Error: The input GeoDataFrame is empty!\")\n",
    "\n",
    "    # Get bounding box in CRS 5070\n",
    "    xmin, ymin, xmax, ymax = inv_rich_gdf.total_bounds  \n",
    "    pixel_size = 100  # 1000 meters per pixel\n",
    "\n",
    "    # Ensure bounds are valid\n",
    "    if xmin == xmax:\n",
    "        xmin -= pixel_size\n",
    "        xmax += pixel_size\n",
    "    if ymin == ymax:\n",
    "        ymin -= pixel_size\n",
    "        ymax += pixel_size\n",
    "\n",
    "    # Recalculate width and height\n",
    "    width = max(1, int((xmax - xmin) / pixel_size))\n",
    "    height = max(1, int((ymax - ymin) / pixel_size))\n",
    "\n",
    "    # Correct transform in CRS 5070\n",
    "    transform = from_origin(xmin, ymax, pixel_size, pixel_size)\n",
    "    num_bands = len(data_columns) + 1  # Extra band for sum\n",
    "    raster = np.zeros((num_bands, height, width), dtype=np.float32)\n",
    "\n",
    "    # Debug: Check valid geometries\n",
    "    valid_geom_count = inv_rich_gdf.geometry.notnull().sum()\n",
    "    print(f\"Valid geometries count: {valid_geom_count}\")\n",
    "\n",
    "    # Rasterize each data column\n",
    "    for i, column in enumerate(data_columns):\n",
    "        shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column]) if geom is not None and not np.isnan(value)]\n",
    "        if not shapes:\n",
    "            print(f\"Skipping {column}: No valid geometries!\")\n",
    "            continue  # Skip empty bands\n",
    "        raster_band = rasterize(\n",
    "            shapes,\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype='float32',\n",
    "            default_value=1,  # Ensure full coverage of waterbody\n",
    "            all_touched=True  # Ensures every pixel within the geometry gets a value\n",
    "        )\n",
    "        raster[i] = raster_band * inv_rich_gdf[column].mean()  # Assign mean value inside each geometry\n",
    "        print(f\"Rasterized {column}: Min={raster_band.min()}, Max={raster_band.max()}, Non-zero pixels={np.count_nonzero(raster_band)}\")\n",
    "\n",
    "    # Compute the sum band\n",
    "    raster[-1] = np.sum(raster[:-1], axis=0)\n",
    "    print(f\"Final Sum Band: Min={raster[-1].min()}, Max={raster[-1].max()}, Non-zero pixels={np.count_nonzero(raster[-1])}\")\n",
    "\n",
    "    # Save as a multi-band GeoTIFF in EPSG:5070\n",
    "    with rasterio.open(\n",
    "        output_path, 'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=num_bands,\n",
    "        dtype=raster.dtype,\n",
    "        crs=\"EPSG:5070\",\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        for band in range(num_bands):\n",
    "            dst.write(raster[band], band + 1)\n",
    "        band_names = data_columns + [\"Inv_Richness\"]\n",
    "        for band, name in enumerate(band_names, start=1):\n",
    "            dst.set_band_description(band, name)\n",
    "\n",
    "    # Debug: Plot overlay of geometries on raster\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.imshow(raster[-1], cmap='viridis', extent=[xmin, xmax, ymin, ymax], origin=\"upper\")\n",
    "    gpd.GeoSeries(inv_rich_gdf.geometry).plot(ax=ax, facecolor=\"none\", edgecolor=\"red\")\n",
    "    plt.title(\"Overlaying Geometries on Raster\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot bands\n",
    "    fig, axes = plt.subplots(1, num_bands, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(raster[i], cmap='viridis')\n",
    "        ax.set_title(band_names[i])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Raster file saved as '{output_path}' in CRS 5070 with band names: {band_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1fef787-a519-437c-a806-9919adadc396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading shapefile...\n",
      "Done\n",
      "['tl_2012_us_state.dbf', 'tl_2012_us_state.prj', 'tl_2012_us_state.shp', 'tl_2012_us_state.shx']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leif.howard\\AppData\\Local\\Temp\\ipykernel_11660\\489319793.py:53: DtypeWarning: Columns (8,10,14,21,27,29,30,31,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  response_2 = pd.read_csv(url_request_2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station and State data saved\n"
     ]
    }
   ],
   "source": [
    "# Download state boundaries shapefile\n",
    "state_boundary_url = 'http://www2.census.gov/geo/tiger/TIGER2012/STATE/tl_2012_us_state.zip'\n",
    "local_path = my_path\n",
    "print('Downloading shapefile...')\n",
    "r = requests.get(state_boundary_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "print(\"Done\")\n",
    "z.extractall(path=local_path) # extract to folder\n",
    "filenames = [y for y in sorted(z.namelist()) for ending in ['dbf', 'prj', 'shp', 'shx'] if y.endswith(ending)] \n",
    "print(filenames)\n",
    "# Get USGS monitoring stations from their API\n",
    "my_stations = get_station_data(state = state_fip)\n",
    "#Filter and convert to a Geodataframe\n",
    "stations = my_stations[['MonitoringLocationIdentifier', 'LatitudeMeasure', 'LongitudeMeasure']].rename(columns = {'MonitoringLocationIdentifier': 'station_id',\n",
    "                        'LatitudeMeasure' : 'latitude', 'LongitudeMeasure' : 'longitude'})  \n",
    "stations_gdf = gpd.GeoDataFrame(\n",
    "    stations, geometry=gpd.points_from_xy(stations.longitude, stations.latitude)).set_crs(4269, allow_override=True)\n",
    "stations_gdf.to_file(my_path + 'usgs_monitoring_stations.shp') # Saving and importing saved files later allows you to import later without remaking variables\n",
    "print('Station and State data saved')\n",
    "# stations_gdf = gpd.read_file(my_path + 'usgs_monitoring_stations.shp').set_crs(4269, allow_override=True)\n",
    "state_boundary = gpd.read_file(my_path + 'tl_2012_us_state.shp').dropna().set_crs(4269, allow_override=True)\n",
    "\n",
    "state = state_boundary[state_boundary['STUSPS'] == my_state]\n",
    "my_state_stations = gpd.sjoin(stations_gdf, state, predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b87d24-c9ac-4473-aad4-bc7a2045686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note this takes a while....\n",
    "# ca = get_data(state = state_fip, my_format = 'csv', characteristic = 'Calcium')\n",
    "# ca_resultsFile = pd.DataFrame(ca).dropna(subset=['ResultMeasureValue'])\n",
    "# ca_resultsFile.ResultMeasureValue = pd.to_numeric(ca_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "# ca_fixed = ca_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "# # Change units for those that are equivalent to mg/L and make ug/L units consistent\n",
    "# ca_clean_1 = ca_fixed.replace({'mg/l' : 'mg/L', 'mg/kg' : 'mg/L', 'mg/l CaCO3': 'mg/L', 'ug/l' : 'ug/L'} )\n",
    "# # convert ug/L to mg/L\n",
    "# ca_clean_1['value'] = ca_clean_1.apply(\n",
    "#     lambda row: row['value'] / 1000 if row['unit'] == 'ug/L' else row['value'],\n",
    "#     axis=1)\n",
    "# #Change unit for converted values to mg/L\n",
    "# ca_clean_2 = ca_clean_1.replace({'ug/L': 'mg/L'})\n",
    "# #Get rid of other records not in mg/L\n",
    "# my_ca = ca_clean_2[ca_clean_2['unit'] == 'mg/L']\n",
    "# #Filter to reasonable values\n",
    "# ca_fixed_2 = my_ca[my_ca['value'] < 300]\n",
    "# ca_fixed_3 = ca_fixed_2[ca_fixed_2['value'] > 0]\n",
    "# #Calculate summary stats by station_id\n",
    "# ca_result = pd.DataFrame(ca_fixed_3.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "# stations_w_ca = pd.merge(my_state_stations, ca_result, on = 'station_id').dropna()\n",
    "# ca_station_list = stations_w_ca['station_id'].tolist()\n",
    "# stations_no_ca = my_state_stations[~my_state_stations['station_id'].isin(ca_station_list)]\n",
    "# ca = add_nearest(stations_no_ca, stations_w_ca)\n",
    "# my_ca_result = pd.concat([ca, stations_w_ca], axis = 0).drop(columns = {\"nearest_id\",\"latitude\",\"longitude\"})\n",
    "# my_ca_result.to_file(my_path + 'usgs_ca.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cf75a-b560-40bb-8c66-f23bb6e46331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pH = get_data(state = state_fip, my_format = 'csv', characteristic = 'pH')\n",
    "# pH_resultsFile =pd.DataFrame(pH).dropna(subset=['ResultMeasureValue'])\n",
    "# pH_resultsFile.ResultMeasureValue = pd.to_numeric(pH_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "# pH_fixed = pH_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "# my_pH = pH_fixed.loc[(pH_fixed['unit'].isna()) | (pH_fixed['unit']== 'std units')]\n",
    "# pH_result = pd.DataFrame(my_pH.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "# pH_fixed_2 = pH_result[pH_result['median'] < 14]\n",
    "# pH_fixed_3 = pH_fixed_2[pH_fixed_2['median'] > 4]\n",
    "# stations_w_pH = pd.merge(my_state_stations, pH_fixed_3, on = 'station_id').dropna()\n",
    "# pH_station_list = stations_w_pH['station_id'].tolist()\n",
    "# stations_no_pH = my_state_stations[~my_state_stations['station_id'].isin(pH_station_list)]\n",
    "# pH = add_nearest(stations_no_pH, stations_w_pH)\n",
    "# my_pH_result = pd.concat([pH, stations_w_pH], axis = 0).drop(columns = {\"nearest_id\", \"latitude\", \"longitude\"})\n",
    "# my_pH_result.to_file(my_path + 'usgs_pH.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4add793-1159-4f07-a559-47904ab0b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# di_N = get_data(state = state_fip, my_format = 'csv', characteristic = 'Nitrogen')\n",
    "# di_N_resultsFile = pd.DataFrame(di_N).dropna(subset=['ResultMeasureValue'])\n",
    "# di_N_resultsFile.ResultMeasureValue = pd.to_numeric(di_N_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "# di_N_fixed = di_N_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "# di_N_fixed.loc[(di_N_fixed['unit'] == 'ug/L') | \n",
    "#        (di_N_fixed['unit'] == 'ppb') | (di_N_fixed['unit'] == 'mg/g'), 'value'] /= 1000\n",
    "# di_N_clean_1 = di_N_fixed.replace({'mg/l' : 'mg/L', 'mg/kg' : 'mg/L', 'ppb': 'mg/L', 'ug/L' : 'mg/L'})\n",
    "# di_fixed_2 = di_N_clean_1[di_N_clean_1['unit'] == 'mg/L']\n",
    "# di_fixed_3 = di_fixed_2[di_fixed_2['value']<= 500]\n",
    "# di_fixed_4 = di_fixed_3[di_fixed_3['value'] > 0]\n",
    "# di_N_result = pd.DataFrame(di_fixed_4.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "# stations_w_di_N = pd.merge(my_state_stations, di_N_result, on = 'station_id')#.to_crs(26915)\n",
    "# di_N_station_list = stations_w_di_N['station_id'].tolist()\n",
    "# stations_no_di_N = my_state_stations[~my_state_stations['station_id'].isin(di_N_station_list)]#.to_crs(26915)\n",
    "# di_N = add_nearest(stations_no_di_N, stations_w_di_N)\n",
    "# my_di_N_result = pd.concat([di_N, stations_w_di_N], axis = 0).drop(columns = {\"nearest_id\", \"latitude\",\"longitude\"})\n",
    "# my_di_N_result.to_file(my_path + 'usgs_N.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf9dca-0184-413b-9343-23719f7ba27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do = get_data(state = state_fip, my_format = 'csv', characteristic = 'Oxygen')\n",
    "# do_resultsFile = pd.DataFrame(do).dropna(subset=['ResultMeasureValue'])\n",
    "# do_resultsFile.ResultMeasureValue = pd.to_numeric(do_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "# do_fixed = do_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "# do_fixed.loc[(do_fixed['unit'] == 'mg/L'), 'value'] *= 12.67\n",
    "# do_clean_1 = do_fixed.replace({'mg/L' : '% saturatn', '% by vol' : '% saturatn'})\n",
    "# do_fixed_2 = do_clean_1[do_clean_1['value']<= 1]\n",
    "# do_fixed_3 = do_fixed_2[do_fixed_2['value']> 0]\n",
    "# do_result = pd.DataFrame(do_fixed_3.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "# stations_w_do = pd.merge(my_state_stations, do_result, on = 'station_id')#.to_crs(26915)\n",
    "# do_station_list = stations_w_do['station_id'].tolist()\n",
    "# stations_no_do = my_state_stations[~my_state_stations['station_id'].isin(do_station_list)]#.to_crs(26915)\n",
    "# do = add_nearest(stations_no_do, stations_w_do)\n",
    "# my_do_result = pd.concat([do, stations_w_do], axis = 0).drop(columns = {\"nearest_id\", \"latitude\",\"longitude\"})\n",
    "# my_do_result.to_file(my_path + 'usgs_do.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26519e-aa8f-4ba0-b884-386bdcb1cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phos = get_data(state = state_fip, my_format = 'csv', characteristic = 'Phosphorus')\n",
    "# phos_resultsFile = pd.DataFrame(phos).dropna(subset=['ResultMeasureValue'])\n",
    "# phos_resultsFile.ResultMeasureValue = pd.to_numeric(phos_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "# phos_fixed = phos_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "# phos_fixed.loc[(phos_fixed['unit'] == 'ug/L') | \n",
    "#        (phos_fixed['unit'] == 'ppb') | (phos_fixed['unit'] == 'mg/g'), 'value'] /= 1000\n",
    "# phos_clean_1 = phos_fixed.replace({'mg/l' : 'mg/L', 'mg/l PO4' : 'mg/L', 'mg/l as P' : 'mg/L', 'mg/kg' : 'mg/L', 'mg/kg as P' : 'mg/L',\n",
    "#                                   'ug/L' : 'mg/L', 'ppb' : 'mg/L', 'mg/g' : 'mg/L'} )\n",
    "# my_phos = phos_clean_1[phos_clean_1['unit'] == 'mg/L']\n",
    "# phos_fixed_2 = my_phos[my_phos['value']<= 1]\n",
    "# phos_result = pd.DataFrame(phos_fixed_2.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "# stations_w_phos = pd.merge(my_state_stations, phos_result, on = 'station_id')#.to_crs(4269)\n",
    "# phos_station_list = stations_w_phos['station_id'].tolist()\n",
    "# stations_no_phos = my_state_stations[~my_state_stations['station_id'].isin(phos_station_list)]#.to_crs(4269)\n",
    "# phos = add_nearest(stations_no_phos, stations_w_phos)\n",
    "# my_phos_result = pd.concat([phos, stations_w_phos], axis = 0).drop(columns = {\"nearest_id\", \"latitude\",\"longitude\"})\n",
    "# my_phos_result.to_file(my_path + 'usgs_phos.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736cb808-d1b0-4736-9f3a-e3ab6912118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ca_result = gpd.read_file(my_path + 'usgs_ca.shp')\n",
    "interpolate(my_ca_result, 'ca.tif', 'Ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f62f2b-8f7a-4595-b90a-9b9c39cc5331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Attempting Ordinary Kriging interpolation...\n"
     ]
    }
   ],
   "source": [
    "my_pH_result = gpd.read_file(my_path + 'usgs_pH.shp')\n",
    "interpolate(my_pH_result, 'pH.tif', 'pH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebd38d-ab62-4dfd-b26d-013b984c8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_di_N_result = gpd.read_file(my_path + 'usgs_N.shp')\n",
    "interpolate(my_di_N_result, 'di_N.tif', 'Nitrogen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833a24d-7038-4b02-9d3b-94f40d9b7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_do_result = gpd.read_file(my_path + 'usgs_do.shp')\n",
    "interpolate(my_do_result, 'do.tif', 'DO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cce07c-fcda-4652-befb-ad4f1cd729a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_phos_result = gpd.read_file(my_path + 'usgs_phos.shp')\n",
    "interpolate(my_phos_result, 'phos.tif', 'Phos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994216e-871c-4035-9c98-d7e3b837dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of Biological Variables\n",
    "# Download waterbody shapefiles by state from NHD \n",
    "URL_BASE_NHD = 'https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHD/State/Shape/'\n",
    "NHD_url = f\"{URL_BASE_NHD}NHD_H_{state_name}_State_Shape.zip\"\n",
    "local_path = my_path\n",
    "print('Downloading shapefile...')\n",
    "r = requests.get(NHD_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "print(\"Done\")\n",
    "z.extractall(path=local_path) # extract to folder\n",
    "filenames = [y for y in sorted(z.namelist()) for ending in ['dbf', 'prj', 'shp', 'shx'] if y.endswith(ending)] \n",
    "print(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb2a4d-5d1a-412c-bf08-3b46976ddc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stream files\n",
    "# Find all shapefiles that include \"NHDFlowline_\" in the filename\n",
    "shapefiles = glob.glob(os.path.join(my_path + \"/shape/\", \"*NHDFlowline_*.shp\")) + glob.glob(os.path.join(my_path + \"/shape/\", \"NHDFlowline.shp\"))\n",
    "\n",
    "# Ensure shapefiles were found\n",
    "if not shapefiles:\n",
    "    print(\"No shapefiles found matching the pattern.\")\n",
    "\n",
    "# Load all shapefiles into a list of GeoDataFrames\n",
    "gdfs = [gpd.read_file(shp) for shp in shapefiles]\n",
    "\n",
    "# Optionally, concatenate all shapefiles into a single GeoDataFrame\n",
    "if gdfs:  # Only concatenate if the list is not empty\n",
    "    stream_gdf = gpd.pd.concat(gdfs, ignore_index=True)\n",
    "    print(\"Successfully merged shapefiles into a single GeoDataFrame.\")\n",
    "else:\n",
    "    stream_gdf = None\n",
    "    print(\"No valid shapefiles to merge.\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Imported {len(gdfs)} shapefiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1383c-d78a-462e-9fb2-481f66d2f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lake and river shapefiles produced earlier in this script for your state\n",
    "lakes = gpd.read_file(my_path + \"/shape/NHDWaterbody.shp\").set_crs(4269, allow_override=True)\n",
    "rivers = gpd.read_file(my_path + \"/shape/NHDArea.shp\").set_crs(4269, allow_override=True)\n",
    "lakes_rivers = pd.concat([lakes, rivers])\n",
    "my_lakes_rivers = lakes_rivers[lakes_rivers['areasqkm'] >= 0.25]\n",
    "my_streams = stream_gdf[stream_gdf['lengthkm'] >= 10]\n",
    "\n",
    "# Create 100-meter buffer so we are certain to join points to water and create raster later\n",
    "my_lakes_rivers['buffer'] = my_lakes_rivers.buffer(0.001)\n",
    "my_streams['buffer'] = my_streams.buffer(0.001)\n",
    "\n",
    "my_water = pd.concat([my_lakes_rivers, my_streams])\n",
    "# Drop the original geometry and setting the new geometry\n",
    "buffered_water = my_water.drop(columns=['geometry']).set_geometry('buffer')\n",
    "buffered_water = buffered_water.rename_geometry('geometry')\n",
    "buffered_water.to_file(my_path + my_state + \"_buffered_water.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20c8ca-e8f3-46dd-8bbd-6f2af388d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "buffered_water.plot(ax=ax, color=\"blue\", linewidth=0.5)\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_title(\"Buffered Water\", fontsize=14)\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5080c-028c-4619-8f41-2dcff218ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the NAS database species_ids for some of the nastiest AIS\n",
    "my_nas_ids = [3294,1047,1012,551,730,1107,118,931,1045,3229,4,2937,1130,1134,\n",
    "2856,237,293,987,1100,92,714,211,538,514,2679,3100,3598,1115,235,239,95,796,910,\n",
    "217,1259,2938,1120,713,7,214,836,549,162,1688,761,714,1099,229,777,2682,5, 910, 939, 931,\n",
    "6, 1168, 1110,\n",
    "2942,\n",
    "1118,\n",
    "2698,\n",
    "243,\n",
    "3012,\n",
    "2681,\n",
    "2680,\n",
    "3020,\n",
    "1116,\n",
    "2686,\n",
    "2676,\n",
    "2943,\n",
    "2674\n",
    "]\n",
    "\n",
    "# This runs the NAS API function and stores the data in a list\n",
    "nas_api_result = []\n",
    "for id in my_nas_ids:\n",
    "    nas_api_result.append(nas_api_call(id, my_state))\n",
    "\n",
    "# This formats that list into a dataframe for easier manipulation (filtering)\n",
    "all_nas_records = []\n",
    "# Iterate through each data block and append records from 'results' to all_records\n",
    "for block in nas_api_result:\n",
    "    for record in block[0]['results']:\n",
    "        all_nas_records.append(record)\n",
    "\n",
    "# Create a pandas DataFrame from the list of records\n",
    "all_nas_df = pd.DataFrame(all_nas_records)\n",
    "all_nas_data = all_nas_df[[\"speciesID\", \"commonName\", \"group\", \"state\", \"decimalLatitude\", \"decimalLongitude\", \"year\", \"status\"]]\n",
    "all_nas_data_fltr = all_nas_data[(all_nas_data['status'] == 'established')].dropna()\n",
    "nas_gdf = gpd.GeoDataFrame(\n",
    "    all_nas_data_fltr, geometry=gpd.points_from_xy(all_nas_data_fltr.decimalLongitude, all_nas_data_fltr.decimalLatitude)).set_crs(4269)\n",
    "nas_gdf.to_file(my_path + my_state + \"_nas.shp\") # Save file so you can start at the next block next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e3cd16-4ca8-4fc7-bdc0-b36efa06995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_inv_richness(inv_rich_gdf, output_path):\n",
    "    # Convert input data to EPSG:5070 BEFORE getting bounds\n",
    "    inv_rich_gdf = inv_rich_gdf.to_crs(\"EPSG:5070\")\n",
    "    data_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "\n",
    "    if inv_rich_gdf.empty:\n",
    "        raise ValueError(\"Error: The input GeoDataFrame is empty!\")\n",
    "\n",
    "    # Get bounding box in CRS 5070\n",
    "    xmin, ymin, xmax, ymax = inv_rich_gdf.total_bounds  \n",
    "    pixel_size = 100  # 1000 meters per pixel\n",
    "\n",
    "    # Ensure bounds are valid\n",
    "    if xmin == xmax:\n",
    "        xmin -= pixel_size\n",
    "        xmax += pixel_size\n",
    "    if ymin == ymax:\n",
    "        ymin -= pixel_size\n",
    "        ymax += pixel_size\n",
    "\n",
    "    # Recalculate width and height\n",
    "    width = max(1, int((xmax - xmin) / pixel_size))\n",
    "    height = max(1, int((ymax - ymin) / pixel_size))\n",
    "\n",
    "    # Correct transform in CRS 5070\n",
    "    transform = from_origin(xmin, ymax, pixel_size, pixel_size)\n",
    "    num_bands = len(data_columns) + 1  # Extra band for sum\n",
    "    raster = np.zeros((num_bands, height, width), dtype=np.float32)\n",
    "\n",
    "    # Debug: Check valid geometries\n",
    "    valid_geom_count = inv_rich_gdf.geometry.notnull().sum()\n",
    "    print(f\"Valid geometries count: {valid_geom_count}\")\n",
    "\n",
    "    # Rasterize each data column\n",
    "    for i, column in enumerate(data_columns):\n",
    "        shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column]) if geom is not None and not np.isnan(value)]\n",
    "        if not shapes:\n",
    "            print(f\"Skipping {column}: No valid geometries!\")\n",
    "            continue  # Skip empty bands\n",
    "        raster_band = rasterize(\n",
    "            shapes,\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype='float32',\n",
    "            default_value=1,  # Ensure full coverage of waterbody\n",
    "            all_touched=True  # Ensures every pixel within the geometry gets a value\n",
    "        )\n",
    "        raster[i] = raster_band * inv_rich_gdf[column].mean()  # Assign mean value inside each geometry\n",
    "        print(f\"Rasterized {column}: Min={raster_band.min()}, Max={raster_band.max()}, Non-zero pixels={np.count_nonzero(raster_band)}\")\n",
    "\n",
    "    # Compute the sum band\n",
    "    raster[-1] = np.sum(raster[:-1], axis=0)\n",
    "    print(f\"Final Sum Band: Min={raster[-1].min()}, Max={raster[-1].max()}, Non-zero pixels={np.count_nonzero(raster[-1])}\")\n",
    "\n",
    "    # Save as a multi-band GeoTIFF in EPSG:5070\n",
    "    with rasterio.open(\n",
    "        output_path, 'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=num_bands,\n",
    "        dtype=raster.dtype,\n",
    "        crs=\"EPSG:4269\",\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        for band in range(num_bands):\n",
    "            dst.write(raster[band], band + 1)\n",
    "        band_names = data_columns + [\"Inv_Richness\"]\n",
    "        for band, name in enumerate(band_names, start=1):\n",
    "            dst.set_band_description(band, name)\n",
    "\n",
    "    # Debug: Plot overlay of geometries on raster\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.imshow(raster[-1], cmap='viridis', extent=[xmin, xmax, ymin, ymax], origin=\"upper\")\n",
    "    gpd.GeoSeries(inv_rich_gdf.geometry).plot(ax=ax, facecolor=\"none\", edgecolor=\"red\")\n",
    "    plt.title(\"Overlaying Geometries on Raster\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot bands\n",
    "    fig, axes = plt.subplots(1, num_bands, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(raster[i], cmap='viridis')\n",
    "        ax.set_title(band_names[i])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Raster file saved as '{output_path}' in CRS 5070 with band names: {band_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc97a9-d9ea-463a-94ee-39c3129439eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_gdf = gpd.read_file(my_path + my_state + \"_nas.shp\").set_crs(4269, allow_override=True)\n",
    "for col in nas_gdf.select_dtypes(include=['int64']).columns:\n",
    "    nas_gdf[col] = nas_gdf[col].astype('float64')\n",
    "\n",
    "buffered_water = gpd.read_file(my_path + my_state + \"_buffered_water.shp\").set_crs(4269, allow_override=True)\n",
    "NAS_ais_obs_df = gpd.sjoin(nas_gdf, buffered_water, how=\"inner\")\n",
    "\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Fishes', 'Inv_Fish')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Crustaceans-Cladocerans', 'Inv_Crustaceans')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Crustaceans-Crayfish', 'Inv_Crustaceans')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Mollusks-Bivalves', 'Inv_Mollusks')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Mollusks-Gastropods', 'Inv_Mollusks')\n",
    "NAS_ais_df = NAS_ais_obs_df[['ObjectID', 'commonName', 'group']]\n",
    "unique_commonnames = NAS_ais_df.groupby(['ObjectID', 'group'])['commonName'].nunique().reset_index()\n",
    "pivot_df = unique_commonnames.pivot(index='ObjectID', columns='group', values='commonName').reset_index().fillna(0)\n",
    "lakes_w_invasives = pd.merge(buffered_water, pivot_df, on = 'ObjectID', how = 'left')\n",
    "\n",
    "# Define the required columns\n",
    "species_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants', 'geometry']\n",
    "\n",
    "# Add missing columns and fill with 0\n",
    "for col in species_columns:\n",
    "    if col not in lakes_w_invasives.columns:\n",
    "        lakes_w_invasives[col] = 0  # Add missing column with default value 0\n",
    "\n",
    "# Now safely select the columns and fill NaN values with 0\n",
    "inv_rich = lakes_w_invasives[species_columns].fillna(0)\n",
    "for col in inv_rich.select_dtypes(include=['int64']).columns:\n",
    "    inv_rich[col] = inv_rich[col].astype('float64')\n",
    "\n",
    "# Create your background data input and export\n",
    "bg_gdf = make_bg_data(inv_rich)  # Generate random points only for selected polygons\n",
    "bg_gdf.to_file(my_path + my_state +'_bg.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdbd75c-40ff-4ba7-a460-2429a1d9cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_native_raster(joined_gdf: gpd.GeoDataFrame, resolution: int = 1000):\n",
    "    bounds = joined_gdf.total_bounds\n",
    "    transform = rasterio.transform.from_origin(bounds[0], bounds[3], resolution, resolution)\n",
    "    out_shape = (\n",
    "        int(np.ceil((bounds[3] - bounds[1]) / resolution)),  \n",
    "        int(np.ceil((bounds[2] - bounds[0]) / resolution))\n",
    "    )\n",
    "    \n",
    "    column_name = \"Native_Fish_Richness\"  # Change this to dynamically select the column if needed\n",
    "    raster = rasterize(\n",
    "        [(geom, value) for geom, value in zip(joined_gdf.geometry, joined_gdf[column_name])],\n",
    "        out_shape=out_shape,\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype=rasterio.float32\n",
    "    )\n",
    "    \n",
    "    output_filename = f\"{my_path}{my_state}_{column_name}_richness.tif\"\n",
    "    \n",
    "    with rasterio.open(\n",
    "        output_filename, \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=out_shape[0],\n",
    "        width=out_shape[1],\n",
    "        count=1,\n",
    "        dtype=rasterio.float32,\n",
    "        crs=\"EPSG:5070\",\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "        dst.set_band_description(1, column_name)  # Set band name\n",
    "    \n",
    "    # Plot the raster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(raster, cmap='viridis', extent=[bounds[0], bounds[2], bounds[1], bounds[3]])\n",
    "    plt.colorbar(label=f'{column_name} Richness')\n",
    "    plt.title('Rasterized GeoDataFrame')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27b59c-daaf-4afc-b1a7-15f6a2e7129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Native Fish \n",
    "# Download data from USGS native fish presence/absence dataset\n",
    "URL_BASE = \"https://www.sciencebase.gov/catalog/file/get/6086df60d34eadd49d31b04a?f=__disk__ad%2F21%2Ffc%2Fad21fc677379f4e45caa4bd506ca1c587d5f01f7\"\n",
    "\n",
    "response = requests.get(URL_BASE)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Convert response content to DataFrame\n",
    "    csv_data = StringIO(response.text)\n",
    "    fish_df = pd.read_csv(csv_data)\n",
    "\n",
    "    # Convert DataFrame to GeoDataFrame\n",
    "    fish_gdf = gpd.GeoDataFrame(\n",
    "        fish_df, geometry=gpd.points_from_xy(fish_df.longitude, fish_df.latitude)\n",
    "    ).set_crs(4269).to_crs(5070)\n",
    "else:\n",
    "    print(f\"Error: Failed to download data (status code {response.status_code})\")\n",
    "\n",
    "\n",
    "state_boundary = gpd.read_file(my_path + 'tl_2012_us_state.shp').dropna().to_crs(5070)\n",
    "state = state_boundary[state_boundary['STUSPS'] == my_state]\n",
    "# Clip the points by the polygon\n",
    "clipped_native_fish = clip_points_by_polygon(fish_gdf, state)\n",
    "# Save to shapefile\n",
    "clipped_native_fish.to_file(my_path + my_state + '_native_fish_gdf.shp')\n",
    "#clipped_native_fish = gpd.read_file(my_path + my_state + '_native_fish_gdf.shp')\n",
    "native_fish_gdf = sum_numeric_columns(clipped_native_fish)#.to_crs(4269)\n",
    "water_w_native_fish = spatial_join_with_nearest(buffered_water, native_fish_gdf)\n",
    "for col in water_w_native_fish.select_dtypes(include=['int64']).columns:\n",
    "    water_w_native_fish[col] = water_w_native_fish[col].astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b198e3-1e99-4fd4-9106-61acb10ccfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_inv_richness(inv_rich, my_path + my_state + '_inv_richness.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd79fd-678f-42fe-848a-2acd33fb5f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_native_raster(water_w_native_fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42df5dc-c332-47fc-952f-065fe5c5e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input files\n",
    "input_files = [\n",
    "    my_path + \"ca.tif\", my_path + \"pH.tif\", my_path + \"di_N.tif\", my_path + \"do.tif\", my_path + \"phos.tif\", \n",
    "    my_path + my_state + \"_rsd.tif\", my_path + my_state + \"_inv_richness.tif\", \n",
    "    my_path + my_state + \"_Native_Fish_Richness.tif\"\n",
    "]\n",
    "\n",
    "\n",
    "# Step 2: Combine aligned rasters into a multi-band GeoTIFF\n",
    "output_file = my_path + my_state + \"_combined.tif\"\n",
    "combine_geotiffs(input_files, output_file, state)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0566b02-8aba-4a40-ad82-d8c0871578cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3ac17-3d7c-43af-ab4c-b689bc39d120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e61629-1f62-44bd-90bb-ef8f30142f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

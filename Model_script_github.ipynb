{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "360d20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import geemap\n",
    "import json\n",
    "import requests\n",
    "from shapely.geometry import Point\n",
    "from functools import reduce\n",
    "import geemap.foliumap as foliumap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e339021-410b-48bb-9eb0-6705638e3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined Variables\n",
    "my_cloud_project = 'ee-shopvac18' # your GEE cloud project ID\n",
    "background_asset = 'conus_background' # name of your GEE background asset \n",
    "\n",
    "# Each NAS has a unique species ID that tells the database which taxa you want records from.\n",
    "#Get species ID from https://nas.er.usgs.gov/api/v2/species \n",
    "#use ctrl f to search for your NAS. Change my_species_id to match\n",
    "# zebra mussels = 5 ; emf = 237; ebt = 939\n",
    "my_species_id = 237 \n",
    "\n",
    "# Set Up geometry, background and import yearly covariate rasters.\n",
    "MY_state_abbrev = 'ID' # change to your state of interest\n",
    "MY_state = 'Idaho' # change to your state of interest\n",
    "MY_scale = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate() # You should already be authenticated from making your covariate rasters, but run just in case\n",
    "# Leave the read-only box un-checked\n",
    "# To force re-authentication run ee.Authenticate(force = True) instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7213ceaf-c77a-41b3-9c8d-9e713d713a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ee.Initialize(project=my_cloud_project) # your GEE cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa15410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get state geometry\n",
    "conus_states = ee.FeatureCollection('TIGER/2018/States')\n",
    "MY_geo = conus_states.filter(ee.Filter.equals('STUSPS', MY_state_abbrev)).geometry() \n",
    "\n",
    "# You should have uploaded the background shapefile by now.\n",
    "background = ee.FeatureCollection(\"projects/\" + my_cloud_project +\"/assets/\" + background_asset) # Change to the path where you stored the background csv\n",
    "MY_background = background.filter(ee.Filter.eq('states', MY_state_abbrev))\n",
    "\n",
    "## Import the yearly covariate rasters you made here. These are examples for MN. Change paths here to your gee assets.  \n",
    "Image_1 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2003\") \n",
    "Image_2 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2004\")\n",
    "Image_3 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2005\") \n",
    "Image_4 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2006\")\n",
    "Image_5 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2007\") \n",
    "Image_6 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2008\")\n",
    "Image_7 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2009\") \n",
    "Image_8 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2010\")\n",
    "Image_9 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2011\") \n",
    "Image_10 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2012\")\n",
    "Image_11 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2013\") \n",
    "Image_12 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2014\")\n",
    "Image_13 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2015\") \n",
    "Image_14 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2016\")\n",
    "Image_15 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2017\") \n",
    "Image_16 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2018\")\n",
    "Image_17 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2019\") \n",
    "Image_18 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2020\")\n",
    "Image_19 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2021\") \n",
    "Image_20 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2022\") \n",
    "\n",
    "## If you want a different timeframe, remove images from within parentheses before creating ImageCollection.\n",
    "my_env_mean = ee.ImageCollection([Image_1, Image_2, Image_3, Image_4, Image_5, Image_6, Image_7, Image_8, Image_9, Image_10,\n",
    "                                Image_11, Image_12, Image_13, Image_14, Image_15, Image_16, Image_17, Image_18, Image_19, \n",
    "                                 Image_20]).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582ba48-35bb-4554-afec-bd356f8938ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to reduce the extent being modeled\n",
    "## Import your shapefile and convert it to a geometry asset\n",
    "# my_local_shape = ee.FeatureCollection('projects/ee-GEE-Cloud-ID/assets/My_Shapefile_Asset').geometry()\n",
    "## Then use the geometry to clip you environmental data\n",
    "# my_local_env = my_env_mean.clip(my_local_shape)\n",
    "## Now change my_env_mean to my_local_env below.\n",
    "## You will also need to filter the occurence and background data sets... See commented out script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d130342-0697-43c3-ac7e-ff9572eb5ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to expand the extent to multiple states\n",
    "## Make covariate rasters for your second state \n",
    "## Then follow the same proceedure to as for your first state\n",
    "## Import yearly raster images for state 2.  This example is for WI\n",
    "## These variables need to be unique, so continue the numbering from above\n",
    "#Image_21 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2003\") \n",
    "#Image_22 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2004\")\n",
    "#Image_23 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2005\") \n",
    "#Image_24 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2006\")\n",
    "#Image_25 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2007\") \n",
    "#Image_26 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2008\")\n",
    "#Image_27 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2009\") \n",
    "#Image_28 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2010\")\n",
    "#Image_29 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2011\") \n",
    "#Image_30 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2012\")\n",
    "#Image_31 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2013\") \n",
    "#Image_32 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2014\")\n",
    "#Image_33 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2015\") \n",
    "#Image_34 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2016\")\n",
    "#Image_35 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2017\") \n",
    "#Image_36 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2018\")\n",
    "#Image_37 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2019\") \n",
    "#Image_38 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2020\")\n",
    "#Image_39 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2021\") \n",
    "#Image_40 = ee.Image(\"projects/ee-GEE-Cloud-ID/assets/covariates_WI2022\")\n",
    "\n",
    "## If you commented out years above, you need to remove them from the list of images here.\n",
    "## Put these yearly rasters into an Image Collection and take the mean\n",
    "#my_env_mean_2 = ee.ImageCollection([Image_21, Image_22, Image_23, Image_24, Image_25, Image_26, Image_27, Image_28, Image_29, Image_30,\n",
    "                                #Image_31, Image_32, Image_33, Image_34, Image_35, Image_36, Image_37, Image_38, Image_39, Image_40]).mean()\n",
    "\n",
    "## Now merge mean covariate rasters together using the .mosaic() functions\n",
    "#multi_state_env = ee.ImageCollection([my_env_mean, my_env_mean_2]).mosaic()\n",
    "## Now change my_env_mean to multi_state_env below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36126b4-d0b9-4d97-86c9-27f6d7f300f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Currently, all environmental parameters from your covariate rasters will be used to build the model\n",
    "## You can comment out parameters you don't want used to train your model by inserting # in front of the band name in this list.\n",
    "\n",
    "parameter_selector = [\n",
    "    #'SurfaceWaterOccurrence',\n",
    "    'elevation',\n",
    "    'Heat_Insolation_Load',\n",
    "    'Topographic_Diversity',\n",
    "    'gHM',\n",
    "    'Max_LST_Annual',\n",
    "    'Mean_GPP',\n",
    "    'Max_NDVI',\n",
    "    'Max_EVI', \n",
    "    'winter_totalPrecip', \n",
    "    'spring_totalPrecip',\n",
    "    'summer_totalPrecip', \n",
    "    'fall_totalPrecip', \n",
    "    'Flashiness']\n",
    "\n",
    "MY_collections = my_env_mean.select(parameter_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db887910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for importing/formatting data from NAS API\n",
    "def _get_col_rename(df, dftype):\n",
    "    \"\"\"Returns a dictionary of columns to rename based on the dataframe and type('csv' or 'api')\"\"\"\n",
    "    \n",
    "    # Build a dictionary of column renamings for use in pandas rename function\n",
    "    renamed_columns = {}\n",
    "    column_names = list(df.columns)\n",
    "    lower_columns = [name.lower().replace(' ','').replace('_','') for name in column_names]\n",
    "    for i in range(len(column_names)):\n",
    "        renamed_columns[column_names[i]] = lower_columns[i]\n",
    "\n",
    "    if dftype == 'csv':\n",
    "        # build csv rename dictionary\n",
    "        renamed_columns['museumcatno'] = 'museumcatnumber'\n",
    "        renamed_columns['huc8number']  = 'huc8'\n",
    "    elif dftype == 'api':\n",
    "        # build api rename dictionary\n",
    "        renamed_columns['key']              = 'specimennumber'\n",
    "        renamed_columns['decimallatitude']  = 'latitude'\n",
    "        renamed_columns['decimallongitude'] = 'longitude'\n",
    "        renamed_columns['latlongsource']    = 'source'\n",
    "        renamed_columns['latlongaccuracy']  = 'accuracy'\n",
    "    else:\n",
    "        raise ValueError(f\"Dataframe type '{dftype}' invalid - Accepted inputs are 'csv' or 'api'\")\n",
    "\n",
    "    return renamed_columns\n",
    "\n",
    "def _manage_cols(df, drop_list=[], name_dict={}):\n",
    "    \"\"\"Private method for dropping and renaming columns in a dataframe, as well as creating one standard table from two different forms.\"\"\"\n",
    "\n",
    "    for colname in drop_list:\n",
    "        if colname not in df:\n",
    "            raise ValueError(f\"Can't drop column '{colname}' - '{colname}' does not exist in dataframe\")\n",
    "    for colname in list(name_dict.keys()):\n",
    "        if colname not in df:\n",
    "            raise ValueError(f\"Can't rename '{colname}' to '{name_dict[colname]}' - '{colname}' does not exist in dataframe\")\n",
    "        if colname in drop_list:\n",
    "            raise ValueError(f\"Can't rename '{colname}' to '{name_dict[colname]}' - '{colname}' in drop_list\")\n",
    "\n",
    "    column_names = np.setdiff1d(list(df.columns), list(name_dict.keys()))\n",
    "    lower_columns = [name.lower().replace(' ','').replace('_','') for name in column_names]\n",
    "    for i in range(len(column_names)):\n",
    "        name_dict[column_names[i]] = lower_columns[i]\n",
    "    \n",
    "    df = df.drop(drop_list, axis=1)\n",
    "    df = df.rename(columns=name_dict)\n",
    "    \n",
    "    return df\n",
    "\n",
    "URL_BASE = 'http://nas.er.usgs.gov/api/v2/'\n",
    "\n",
    "\n",
    "def api_df(species_id, limit, api_key):\n",
    "    \"\"\"Returns a pandas dataframe containing records about a species from the NAS database using their API\"\"\"\n",
    "    \n",
    "    # Check for API key\n",
    "    if api_key is not None:\n",
    "        url_request = f\"{URL_BASE}/occurrence/search?species_ID={species_id}&api_key={api_key}\"\n",
    "    else:\n",
    "        url_request = f\"{URL_BASE}/occurrence/search?species_ID={species_id}\"\n",
    "    \n",
    "    # Get dataframe from API request\n",
    "    request_json = requests.get(url_request, params={'limit':limit}).json()\n",
    "    api_df = pd.json_normalize(request_json, 'results')\n",
    "    api_df = _manage_cols(api_df)\n",
    "\n",
    "    # Add columns that are in a CSV dataframe but not an API dataframe\n",
    "    api_df['country']      = np.nan\n",
    "    api_df['drainagename'] = np.nan\n",
    "\n",
    "    # Rename columns\n",
    "    renamed_columns = _get_col_rename(api_df, 'api')\n",
    "    api_df = api_df.rename(columns=renamed_columns)\n",
    "\n",
    "    # Reorder columns\n",
    "    cols = list(api_df.columns)\n",
    "    cols = cols[0:8] + cols[33:34] + cols[8:33] + cols[34:] # country\n",
    "    cols = cols[0:16] + cols[34:] + cols[16:34] # drainagename\n",
    "    api_df = api_df[cols]\n",
    "    \n",
    "    return api_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09ae10-e03c-40b7-bf24-8dadffeec380",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_NAS = api_df(my_species_id, limit = 10000, api_key = {\"speciesID\": my_species_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4591ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab just the columns we need for filtering the occurence records\n",
    "my_data = my_NAS[[\"state\", \"latitude\", \"longitude\", \"year\", \"status\", \"accuracy\"]]\n",
    "## Note: There are many other columns which contain additional metadata for each record\n",
    "## To view all columns paste the name of the object created above and click run....\n",
    "#my_NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter the pandas dataframe to get only accurate presences for your state.\n",
    "\n",
    "## If you are increasing the extent of your model, you can get NAS data for multiple states by replacing\n",
    "# & (my_data['state'] == My_state) with & (my_data['state'].isin([MY_state, 'State_2_name'])\n",
    "\n",
    "my_data = my_NAS[[\"state\", \"latitude\", \"longitude\", \"year\", \"status\", \"accuracy\"]]\n",
    "my_data_fltr = my_data[(my_data['status'] == 'established') & (my_data['accuracy'] == 'Accurate')\n",
    "& (my_data['state'] == MY_state)] \n",
    "my_data_fixed = my_data_fltr.dropna()\n",
    "#turn it into a geo datafrome\n",
    "user_data_gdf = gpd.GeoDataFrame(\n",
    "    my_data_fixed, geometry=gpd.points_from_xy(my_data_fixed.longitude, my_data_fixed.latitude))\n",
    "#add Coordinate Reference System (CRS)\n",
    "user_data_gdf.crs = \"EPSG:4326\"\n",
    "#convert geopandas object into ee object\n",
    "fc = geemap.geopandas_to_ee(user_data_gdf)\n",
    "#add present property \n",
    "def addPresent (property):\n",
    "    return property.set('Present', 1);\n",
    "\n",
    "user_data = fc.map(addPresent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98a79d-3d99-4b69-9056-345166840e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add your own data (w/ latitude and Longitude columns)\n",
    "# Import CSV\n",
    "# my_presence_data = pd.read_csv('C:/YourFilename.csv')\n",
    "# user_gdf = gpd.GeoDataFrame(\n",
    "    #my_presence_data, geometry=gpd.points_from_xy(my_presence_data.longitude, my_presence_data.latitude))\n",
    "# user_gdf.crs = \"EPSG:4326\"\n",
    "# my_gdf = geemap.geopandas_to_ee(user_gdf)\n",
    "# my_presence_data = my_gdf.map(addPresent)\n",
    "# Replace user_data with your info in the next block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b36e3-2789-48c2-86c4-78911ab08ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add your own background data (w/ latitude and Longitude columns)\n",
    "# Import CSV\n",
    "# my_background_data = pd.read_csv('C:/YourFilename.csv')\n",
    "# user_background = gpd.GeoDataFrame(\n",
    "    #my_background_data, geometry=gpd.points_from_xy(my_background_data.longitude, my_background_data.latitude))\n",
    "# user_background.crs = \"EPSG:4326\"\n",
    "# my_background_gdf = geemap.geopandas_to_ee(user_background)\n",
    "#add present property \n",
    "#def addBackground (property):\n",
    "    #return property.set('Present', 0);\n",
    "# my_presence_data = my_background_gdf.map(addBackground)\n",
    "# Replace MY_background with your background info in the next block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f371d-c7a7-4f9c-97fd-516bed68b857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2f866-53d4-413c-b8a7-45569f35c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you are increasing your extent, you will need bacground points from multiple states\n",
    "# My_background = background.filter(ee.Filter.eq('states', [MY_state_abbrev, 'WI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40feb9-e13e-44e0-8582-e70636ac1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you only want a specific year or timeframe, you need to filter the data you pulled from the NAS database\n",
    "# user_data_year = user_data.filter(ee.Filter.eq(\"year\", 2003)\n",
    "## replace user_data in the next block with user_data_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split for training and testing\n",
    "data_split = user_data.randomColumn()\n",
    "training_data = data_split.filter(ee.Filter.lte('random', 0.75)).merge(MY_background)\n",
    "testing_data = data_split.filter(ee.Filter.gt('random', 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa05c42-f18a-4e0d-bd05-a6c890bd3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you are reducing the extent clip the training and testing data using filter.bounds()\n",
    "# training_data_local = training_data.filter(ee.Filter.bounds(my_local_shape))\n",
    "# testing_data_local = testing_data.filter(ee.Filter.bounds(my_local_shape))\n",
    "## Replace training_data and testing data below with these new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ab723",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_points = MY_collections.reduceRegions(**{\n",
    "                              'collection': training_data,\n",
    "                              'reducer': ee.Reducer.mean(),\n",
    "                              'crs': 'EPSG:4326',\n",
    "                              'scale': MY_scale,\n",
    "                              'tileScale': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Classifier and setup for MaxEnt Parameters \n",
    "classifier = ee.Classifier.amnhMaxent().train(**{\n",
    "    'features': training_points,\n",
    "    'classProperty': 'Present',\n",
    "    'inputProperties': MY_collections.bandNames()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0deae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_results = classifier.explain();\n",
    "classifier_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504acc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_input_classified = MY_collections.classify(classifier);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoother = ee.Kernel.square(**{\n",
    "    'radius': 3,\n",
    "    'magnitude': 1\n",
    "})\n",
    "smoothed = MY_input_classified.convolve(smoother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61faed83-7398-4940-a4fd-19acc75a3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create color paramaters and map classified image\n",
    "Probability_PARAMS = {\"opacity\":1,\"bands\":[\"probability\"],\n",
    "\"palette\":[\"2b83ba\",\"6ab0af\",\"abdda4\",\"cdebaf\",\"ffffbf\", \"fed790\", \"fdae61\", \"d7191c\",\"d7191c\"]}\n",
    "Map = foliumap.Map(center=[45,-95], zoom=6)\n",
    "Map.addLayer(smoothed, Probability_PARAMS, 'Habitat Suitability')\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_points = MY_input_classified.reduceRegions(**{\n",
    "                              'collection': testing_data,\n",
    "                              'reducer': ee.Reducer.mean(),\n",
    "                              'crs': 'EPSG:4326',\n",
    "                              'scale': MY_scale,\n",
    "                              'tileScale': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select get the predictions from the testing locations\n",
    "false_neg_testing = testing_points.select('probability');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4078c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## False negatives = Predictions < 0.30 at testing locations (known positive locations withheld for testing earlier in the script)\n",
    "# Count the false negatives\n",
    "my_false_negs =  false_neg_testing.filter(ee.Filter.lessThan('probability', 0.30)).size();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "## True positives = Predictions > 0.70 at testing locations\n",
    "# Count True positives\n",
    "my_true_pos = false_neg_testing.filter(ee.Filter.greaterThan('probability', 0.70)).size();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adceb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate False Negative Rate by dividing the number of false negatives by the sum of false negatives and true positives\n",
    "my_fn_rate =  my_false_negs.divide(my_false_negs.add(my_true_pos));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37732b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check False Negative Rate\n",
    "my_fn_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ebf38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the testing prediction values into a pandas dataframe\n",
    "false_neg_df = geemap.ee_to_df(false_neg_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fdedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create histogram of distribution for testing predictions values.\n",
    "n_bins = 20\n",
    "x = false_neg_df\n",
    "plt.hist(x, n_bins, density = True, \n",
    "         histtype ='bar',\n",
    "         color = 'green')\n",
    "\n",
    "plt.title('Testing Predictions\\n\\n',\n",
    "          fontweight =\"bold\")\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6599df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Map as GeoTiff\n",
    "export = ee.batch.Export.image.toDrive(image = MY_input_classified,\n",
    "                    description = 'MY_prediction',\n",
    "                    region = MY_geo,\n",
    "                    scale =  MY_scale,\n",
    "                    maxPixels = 1e13)\n",
    "export.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

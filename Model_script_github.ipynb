{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import geemap\n",
    "import json\n",
    "import requests\n",
    "from shapely.geometry import Point\n",
    "from functools import reduce\n",
    "import geemap.foliumap as foliumap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e339021-410b-48bb-9eb0-6705638e3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined Variables\n",
    "my_cloud_project = 'ee-YourCloudProject' # your GEE cloud project ID\n",
    "background_asset = 'conus_background' # name of your GEE background asset \n",
    "\n",
    "# Each NAS has a unique species ID that tells the database which taxa you want records from.\n",
    "#Get species ID from https://nas.er.usgs.gov/api/v2/species \n",
    "#use ctrl f to search for your NAS. Change my_species_id to match\n",
    "# zebra mussels = 5 ; emf = 237; ebt = 939\n",
    "my_species_id = 5 \n",
    "\n",
    "# Set Up geometry, background and import yearly covariate rasters.\n",
    "MY_state_abbrev = 'MN' # change to your state of interest\n",
    "MY_state = 'Minnesota' # change to your state of interest\n",
    "MY_scale = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate() # You should already be authenticated from making your covariate rasters, but run just in case\n",
    "# Leave the read-only box un-checked\n",
    "# To force re-authentication run ee.Authenticate(force = True) instead \n",
    "ee.Initialize(project=my_cloud_project) # your GEE cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa15410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get state geometry\n",
    "conus_states = ee.FeatureCollection('TIGER/2018/States')\n",
    "MY_geo = conus_states.filter(ee.Filter.equals('STUSPS', MY_state_abbrev)).geometry() \n",
    "\n",
    "# You should have uploaded the background shapefile by now.\n",
    "background = ee.FeatureCollection(\"projects/\" + my_cloud_project +\"/assets/\" + background_asset) # Change to the path where you stored the background csv\n",
    "MY_background = background.filter(ee.Filter.eq('states', MY_state_abbrev))\n",
    "\n",
    "## Import the yearly covariate rasters you made here. These are examples for MN. Change paths here to your gee assets.  \n",
    "Image_1 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2003\") \n",
    "Image_2 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2004\")\n",
    "Image_3 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2005\") \n",
    "Image_4 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2006\")\n",
    "Image_5 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2007\") \n",
    "Image_6 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2008\")\n",
    "Image_7 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2009\") \n",
    "Image_8 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2010\")\n",
    "Image_9 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2011\") \n",
    "Image_10 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2012\")\n",
    "Image_11 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2013\") \n",
    "Image_12 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2014\")\n",
    "Image_13 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2015\") \n",
    "Image_14 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2016\")\n",
    "Image_15 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2017\") \n",
    "Image_16 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2018\")\n",
    "Image_17 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2019\") \n",
    "Image_18 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2020\")\n",
    "Image_19 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev + \"_2021\") \n",
    "\n",
    "\n",
    "## If you want a different timeframe, remove images from within parentheses before creating ImageCollection.\n",
    "my_env_data = ee.ImageCollection([Image_1, Image_2, Image_3, Image_4, Image_5, Image_6, Image_7, Image_8, Image_9, Image_10,\n",
    "                                Image_11, Image_12, Image_13, Image_14, Image_15, Image_16, Image_17, Image_18, Image_19]).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582ba48-35bb-4554-afec-bd356f8938ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to reduce the extent being modeled\n",
    "## Import your shapefile and convert it to a geometry asset\n",
    "# my_local_shape = ee.FeatureCollection('projects/ee-GEE-Cloud-ID/assets/My_Shapefile_Asset').geometry()\n",
    "## Then use the geometry to clip you environmental data\n",
    "# my_local_env = my_env_mean.clip(my_local_shape)\n",
    "## Now change my_env_mean to my_local_env below.\n",
    "## You will also need to filter the occurence and background data sets... See commented out script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d130342-0697-43c3-ac7e-ff9572eb5ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hint: Highlight multiple lines and use ctrl / to uncomment all highlighted lines\n",
    "\n",
    "# ## If you want to predict to another state or increase the area modeled (after making rasters for an additional state) uncomment this block of code.\n",
    "# MY_state_abbrev_2 = 'ID'\n",
    "# MY_state_2 = 'Idaho'\n",
    "# ## You can comment out years to change the timeframe you model\n",
    "# Image_21 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2003\") \n",
    "# Image_22 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2004\")\n",
    "# Image_23 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2005\") \n",
    "# Image_24 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2006\")\n",
    "# Image_25 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2007\") \n",
    "# Image_26 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2008\")\n",
    "# Image_27 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2009\") \n",
    "# Image_28 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2010\")\n",
    "# Image_29 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2011\") \n",
    "# Image_30 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2012\")\n",
    "# Image_31 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2013\") \n",
    "# Image_32 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2014\")\n",
    "# Image_33 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2015\") \n",
    "# Image_34 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2016\")\n",
    "# Image_35 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2017\") \n",
    "# Image_36 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2018\")\n",
    "# Image_37 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2019\") \n",
    "# Image_38 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2020\")\n",
    "# Image_39 = ee.Image(\"projects/\" + my_cloud_project +\"/assets/covariates_\" + MY_state_abbrev_2 + \"_2021\") \n",
    "\n",
    "# ## If you commented out years above, you need to remove them from the list of images here.\n",
    "# my_env_data_2 = ee.ImageCollection([Image_21, Image_22, Image_23, Image_24, Image_25, Image_26, Image_27, Image_28, Image_29, Image_30,\n",
    "#                                Image_31, Image_32, Image_33, Image_34, Image_35, Image_36, Image_37, Image_38, Image_39, \n",
    "#                                 ]).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36126b4-d0b9-4d97-86c9-27f6d7f300f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Currently, all environmental parameters from your covariate rasters will be used to build the model\n",
    "## You can comment out parameters you don't want used to train your model by inserting # in front of the band name in this list.\n",
    "parameter_selector = [\n",
    "    'elevation', # for example, you might comment out elevation when using a model trained with MN data to project to ID.\n",
    "    'Heat_Insolation_Load',\n",
    "    'Topographic_Diversity',\n",
    "    'gHM',\n",
    "    'Max_LST_Annual',\n",
    "    'Mean_GPP',\n",
    "    'Max_NDVI',\n",
    "    'Max_EVI', \n",
    "    'winter_totalPrecip', \n",
    "    'spring_totalPrecip',\n",
    "    'summer_totalPrecip', \n",
    "    'fall_totalPrecip', \n",
    "    'Flashiness']\n",
    "MY_collections = my_env_data.select(parameter_selector)\n",
    "# MY_collections_2 = my_env_data_2.select(parameter_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db887910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for importing/formatting data from NAS API\n",
    "def _get_col_rename(df, dftype):\n",
    "    \"\"\"Returns a dictionary of columns to rename based on the dataframe and type('csv' or 'api')\"\"\"\n",
    "# Build a dictionary of column renamings for use in pandas rename function\n",
    "    renamed_columns = {}\n",
    "    column_names = list(df.columns)\n",
    "    lower_columns = [name.lower().replace(' ','').replace('_','') for name in column_names]\n",
    "    for i in range(len(column_names)):\n",
    "        renamed_columns[column_names[i]] = lower_columns[i]\n",
    "\n",
    "    if dftype == 'csv':\n",
    "        # build csv rename dictionary\n",
    "        renamed_columns['museumcatno'] = 'museumcatnumber'\n",
    "        renamed_columns['huc8number']  = 'huc8'\n",
    "    elif dftype == 'api':\n",
    "        # build api rename dictionary\n",
    "        renamed_columns['key']              = 'specimennumber'\n",
    "        renamed_columns['decimallatitude']  = 'latitude'\n",
    "        renamed_columns['decimallongitude'] = 'longitude'\n",
    "        renamed_columns['latlongsource']    = 'source'\n",
    "        renamed_columns['latlongaccuracy']  = 'accuracy'\n",
    "    else:\n",
    "        raise ValueError(f\"Dataframe type '{dftype}' invalid - Accepted inputs are 'csv' or 'api'\")\n",
    "\n",
    "    return renamed_columns\n",
    "\n",
    "def _manage_cols(df, drop_list=[], name_dict={}):\n",
    "    \"\"\"Private method for dropping and renaming columns in a dataframe, as well as creating one standard table from two different forms.\"\"\"\n",
    "\n",
    "    for colname in drop_list:\n",
    "        if colname not in df:\n",
    "            raise ValueError(f\"Can't drop column '{colname}' - '{colname}' does not exist in dataframe\")\n",
    "    for colname in list(name_dict.keys()):\n",
    "        if colname not in df:\n",
    "            raise ValueError(f\"Can't rename '{colname}' to '{name_dict[colname]}' - '{colname}' does not exist in dataframe\")\n",
    "        if colname in drop_list:\n",
    "            raise ValueError(f\"Can't rename '{colname}' to '{name_dict[colname]}' - '{colname}' in drop_list\")\n",
    "\n",
    "    column_names = np.setdiff1d(list(df.columns), list(name_dict.keys()))\n",
    "    lower_columns = [name.lower().replace(' ','').replace('_','') for name in column_names]\n",
    "    for i in range(len(column_names)):\n",
    "        name_dict[column_names[i]] = lower_columns[i]\n",
    "    \n",
    "    df = df.drop(drop_list, axis=1)\n",
    "    df = df.rename(columns=name_dict)\n",
    "    \n",
    "    return df\n",
    "\n",
    "URL_BASE = 'http://nas.er.usgs.gov/api/v2/'\n",
    "\n",
    "def api_df(species_id, limit, api_key):\n",
    "    \"\"\"Returns a pandas dataframe containing records about a species from the NAS database using their API\"\"\"\n",
    "    \n",
    "    # Check for API key\n",
    "    if api_key is not None:\n",
    "        url_request = f\"{URL_BASE}/occurrence/search?species_ID={species_id}&api_key={api_key}\"\n",
    "    else:\n",
    "        url_request = f\"{URL_BASE}/occurrence/search?species_ID={species_id}\"\n",
    "    \n",
    "    # Get dataframe from API request\n",
    "    request_json = requests.get(url_request, params={'limit':limit}).json()\n",
    "    api_df = pd.json_normalize(request_json, 'results')\n",
    "    api_df = _manage_cols(api_df)\n",
    "\n",
    "    # Add columns that are in a CSV dataframe but not an API dataframe\n",
    "    api_df['country']      = np.nan\n",
    "    api_df['drainagename'] = np.nan\n",
    "\n",
    "    # Rename columns\n",
    "    renamed_columns = _get_col_rename(api_df, 'api')\n",
    "    api_df = api_df.rename(columns=renamed_columns)\n",
    "\n",
    "    # Reorder columns\n",
    "    cols = list(api_df.columns)\n",
    "    cols = cols[0:8] + cols[33:34] + cols[8:33] + cols[34:] # country\n",
    "    cols = cols[0:16] + cols[34:] + cols[16:34] # drainagename\n",
    "    api_df = api_df[cols]\n",
    "    \n",
    "    return api_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4591ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get records by species ID from NAS database.\n",
    "my_NAS = api_df(my_species_id, limit = 10000, api_key = {\"speciesID\": my_species_id})\n",
    "## Grab just the columns we need for filtering the occurence records\n",
    "my_data = my_NAS[[\"state\", \"latitude\", \"longitude\", \"year\", \"status\", \"accuracy\"]]\n",
    "## Filter the pandas dataframe to get only accurate presences for your state.\n",
    "## If you are increasing the extent of your model, you can get NAS data for multiple states by replacing\n",
    "# & (my_data['state'] == My_state) with & (my_data['state'].isin([MY_state, 'State_2_name'])\n",
    "my_data = my_NAS[[\"state\", \"latitude\", \"longitude\", \"year\", \"status\", \"accuracy\"]]\n",
    "my_data_fltr = my_data[(my_data['status'] == 'established') & (my_data['accuracy'] == 'Accurate')\n",
    "& (my_data['state'] == MY_state)] \n",
    "my_data_fixed = my_data_fltr.dropna()\n",
    "#turn it into a geo datafrome\n",
    "user_data_gdf = gpd.GeoDataFrame(\n",
    "    my_data_fixed, geometry=gpd.points_from_xy(my_data_fixed.longitude, my_data_fixed.latitude))\n",
    "#add Coordinate Reference System (CRS)\n",
    "user_data_gdf.crs = \"EPSG:4326\"\n",
    "#convert geopandas object into ee object\n",
    "fc = geemap.geopandas_to_ee(user_data_gdf)\n",
    "#add present property \n",
    "def addPresent (property):\n",
    "    return property.set('Present', 1);\n",
    "user_data = fc.map(addPresent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98a79d-3d99-4b69-9056-345166840e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add your own data (using a CSV w/ latitude and Longitude columns)\n",
    "# Import CSV\n",
    "# my_presence_data = pd.read_csv('C:/YourFilename.csv')\n",
    "# user_gdf = gpd.GeoDataFrame(\n",
    "    #my_presence_data, geometry=gpd.points_from_xy(my_presence_data.longitude, my_presence_data.latitude))\n",
    "# user_gdf.crs = \"EPSG:4326\"\n",
    "# my_gdf = geemap.geopandas_to_ee(user_gdf)\n",
    "# my_presence_data = my_gdf.map(addPresent)\n",
    "# Replace user_data with your info in the next block of code or use .merge() to cobine your data with NAS database records.\n",
    "\n",
    "## If you are increasing your extent, you will need background points from multiple states. This is not necessary for predicting to another state.\n",
    "# My_background = background.filter(ee.Filter.eq('states', [MY_state_abbrev, 'MY_state_abbrev_2'])\n",
    "\n",
    "## If you only want a specific year or timeframe, you need to filter the data you pulled from the NAS database\n",
    "# user_data_year = user_data.filter(ee.Filter.eq(\"year\", 2003)\n",
    "## replace user_data in the next block with user_data_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a25a53-5090-41f5-bc5d-28b443aef044",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and test your model\n",
    "## Run 10-fold cross validation to check model performance\n",
    "results_df = []\n",
    "importance_df = []\n",
    "for x in range(10):\n",
    "    data_split = user_data.randomColumn(seed= x)\n",
    "    training_data = data_split.filter(ee.Filter.lte('random', 0.75)).merge(MY_background)\n",
    "    testing_data = data_split.filter(ee.Filter.gt('random', 0.75))\n",
    "            \n",
    "# split for training and testing with eDNA\n",
    "    training_points = MY_collections.reduceRegions(**{\n",
    "                              'collection': training_data,\n",
    "                              'reducer': ee.Reducer.mean(),\n",
    "                              'crs': 'EPSG:4326',\n",
    "                              'scale': MY_scale,\n",
    "                              'tileScale': 16})\n",
    "#Define Classifier and setup for MaxEnt Parameters \n",
    "    classifier = ee.Classifier.amnhMaxent().train(**{\n",
    "        'features': training_points,\n",
    "        'classProperty': 'Present',\n",
    "        'inputProperties': MY_collections.bandNames()\n",
    "    })\n",
    "    \n",
    "    MY_input_classified = MY_collections.classify(classifier);\n",
    "    \n",
    "    testing_points = MY_input_classified.reduceRegions(**{\n",
    "                              'collection': testing_data,\n",
    "                              'reducer': ee.Reducer.mean(),\n",
    "                              'crs': 'EPSG:4326',\n",
    "                              'scale': MY_scale,\n",
    "                              'tileScale': 16})\n",
    "    \n",
    "\n",
    "#Select get the predictions from the testing locations\n",
    "    false_neg_testing = testing_points.select('probability');\n",
    "    my_false_negs =  false_neg_testing.filter(ee.Filter.lessThan('probability', 0.30)).size();\n",
    "    my_true_pos = false_neg_testing.filter(ee.Filter.greaterThan('probability', 0.70)).size();\n",
    "    my_fn_rate =  my_false_negs.divide(my_false_negs.add(my_true_pos));\n",
    "    results = ee.List([my_fn_rate]).getInfo()\n",
    "    classifier_results = classifier.explain();\n",
    "    importance = classifier_results.get('Contributions')\n",
    "    imp = importance.getInfo()\n",
    "    results_df.append(results)\n",
    "    importance_df.append(imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4102def0-0b97-44c7-a6df-6a15f2f8ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot results of cross validation\n",
    "all_data = pd.DataFrame(results_df)\n",
    "fig, ax1 = plt.subplots(figsize=(4, 4))\n",
    "labels = ['All Data']\n",
    "# rectangular box plot\n",
    "bplot1 = ax1.boxplot(all_data, vert=True) \n",
    "plt.hlines(0.2, xmin=0, xmax=2, color='r', linestyles='--')\n",
    "ax1.set_ylabel('False Negative Rate')\n",
    "ax1.set_xticklabels(\"\")\n",
    "ax1.set_xlabel('Results')\n",
    "ax1.set_title('Cross Validation Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaafe26-3134-456b-b334-a4ea5d00f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check parameter contributions/importance\n",
    "imp_df = pd.DataFrame(importance_df)\n",
    "imp_df_mean = pd.DataFrame(imp_df.mean(axis=0))\n",
    "imp_df_se = pd.DataFrame(imp_df.sem(axis=0))\n",
    "imp_results = imp_df_mean.merge(imp_df_se, left_index=True, right_index=True).reset_index()\n",
    "imp_formatted = imp_results.rename(columns={\"index\": \"Parameter\", \"0_x\": \"Contribution\", \"0_y\": \"std_er\"})\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "params = np.array(imp_formatted['Parameter'])\n",
    "importance = np.array(imp_formatted['Contribution'])\n",
    "err_bars = np.array(imp_formatted['std_er'])\n",
    "ax.bar(params, importance)\n",
    "plt.errorbar(params, importance, yerr=err_bars, fmt=\".\", color=\"r\")\n",
    "plt.xticks(rotation = 75)\n",
    "ax.set_ylabel('Change in AUC when parameter is removed')\n",
    "ax.set_title('Parameter Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92e85f-10c4-45d5-910d-ce6d2446c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the model without withholding the 25% for testing, and classify image to get heatmap produced with all data\n",
    "map_data = user_data.merge(MY_background)\n",
    "\n",
    "training_points_2 = MY_collections.reduceRegions(**{\n",
    "                              'collection': map_data,\n",
    "                              'reducer': ee.Reducer.mean(),\n",
    "                              'crs': 'EPSG:4326',\n",
    "                              'scale': MY_scale,\n",
    "                              'tileScale': 16})\n",
    "classifier_2 = ee.Classifier.amnhMaxent().train(**{\n",
    "    'features': training_points_2,\n",
    "    'classProperty': 'Present',\n",
    "    'inputProperties': MY_collections.bandNames()\n",
    "})\n",
    "\n",
    "#Use the new model to classify the environmental data\n",
    "MY_input_classified = MY_collections.classify(classifier_2);\n",
    "# MY_input_classified_2 = MY_collections_2.classify(classifier_2);\n",
    "\n",
    "# Use the Surface Water Occurence Band to mask the prediction so only pixels with water are visible\n",
    "water_pixels = my_env_data.select('SurfaceWaterOccurrence')\n",
    "waterMask = water_pixels.gte(0.2)\n",
    "prediction_masked = MY_input_classified.mask(waterMask)\n",
    "\n",
    "# water_pixels_2 = my_env_data_2.select('SurfaceWaterOccurrence')\n",
    "# waterMask_2 = water_pixels_2.gte(0.2)\n",
    "# prediction_masked_2 = MY_input_classified_2.mask(waterMask_2)\n",
    "\n",
    "# Apply smoothing function\n",
    "smoother = ee.Kernel.square(**{\n",
    "    'radius': 8,\n",
    "    'magnitude': 1\n",
    "})\n",
    "smoothed = prediction_masked.convolve(smoother)\n",
    "# smoothed_2 = prediction_masked_2.convolve(smoother)\n",
    "\n",
    "#Create color paramaters and map classified image\n",
    "Probability_PARAMS = {\"opacity\":1,\"bands\":[\"probability\"],\n",
    "\"palette\":[\"2b83ba\",\"6ab0af\",\"abdda4\",\"cdebaf\",\"ffffbf\", \"fed790\", \"fdae61\", \"d7191c\",\"d7191c\"]}\n",
    "Map = foliumap.Map(center=[45,-95], zoom=6)\n",
    "Map.add_colorbar(Probability_PARAMS)\n",
    "Map.addLayer(smoothed, Probability_PARAMS, 'Habitat Suitability')\n",
    "# Map.addLayer(smoothed_2, Probability_PARAMS, 'Habitat Suitability')\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6599df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export your heatmap to your GEE assets\n",
    "task = ee.batch.Export.image.toAsset(\n",
    "    image = prediction_masked,\n",
    "    description='my_heatmap',\n",
    "    assetId=\"projects/\" + my_cloud_project +\"/assets/my_heatmap\",  # <> modify these\n",
    "    region=MY_geo,\n",
    "    scale=100)\n",
    "task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b40ac59-538b-45ff-90aa-9ece5528c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ee.batch.Export.table.toAsset(\n",
    "    collection = user_data,\n",
    "    description='NAS Records',\n",
    "    assetId=\"projects/\" + my_cloud_project +\"/assets/my_nas_records\")\n",
    "task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d0ab5-4ca8-4db7-a774-ea12fae20976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

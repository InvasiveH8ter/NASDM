{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213ceaf-c77a-41b3-9c8d-9e713d713a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize(project='ee-Your-GEE-Cloud-ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import geemap\n",
    "import json\n",
    "import requests\n",
    "from shapely.geometry import Point\n",
    "from functools import reduce\n",
    "import geemap.foliumap as foliumap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d1497a",
   "metadata": {},
   "source": [
    "# Get species ID from https://nas.er.usgs.gov/api/v2/species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46193049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each NAS has a unique species ID that tells the API which taxa you want records from.\n",
    "# Navigate to the above link and use ctrl f to search for your NAS. Change my_species_id to match\n",
    "my_species_id = 5 # 5 = zebra mussels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa15410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up geometry, background and import yearly covariate rasters.\n",
    "MY_state_abbrev = 'MN' # change to your state of interest\n",
    "MY_state = 'Minnesota' # change to your state of interest\n",
    "MY_scale = 1000\n",
    "\n",
    "conus_states = ee.FeatureCollection('TIGER/2018/States')\n",
    "MY_geo = conus_states.filter(ee.Filter.equals('STUSPS', MY_state_abbrev)).geometry() \n",
    "\n",
    "# You should have uploaded the background shapefile by now.\n",
    "background = ee.FeatureCollection(\"projects/ee-Your-GEE-Cloud-ID/background\") # Change to the path where you stored the background csv\n",
    "MY_background = background.filter(ee.Filter.eq('states', MY_state_abbrev))\n",
    "\n",
    "## Add yearly covariate rasters you made here. These are examples for MN\n",
    "Image_1 = ee.Image(\"projects/ee-Your-GEE-Cloud-ID/covariates_\") # add year to the en\n",
    "Image_2 = ee.Image(\"projects/ee-Your-GEE-Cloud-ID/covariates_\")\n",
    "\n",
    "MY_collections = ee.ImageCollection([Image_1, Image_2]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db887910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for importing/formatting data from NAS API\n",
    "def _get_col_rename(df, dftype):\n",
    "    \"\"\"Returns a dictionary of columns to rename based on the dataframe and type('csv' or 'api')\"\"\"\n",
    "    \n",
    "    # Build a dictionary of column renamings for use in pandas rename function\n",
    "    renamed_columns = {}\n",
    "    column_names = list(df.columns)\n",
    "    lower_columns = [name.lower().replace(' ','').replace('_','') for name in column_names]\n",
    "    for i in range(len(column_names)):\n",
    "        renamed_columns[column_names[i]] = lower_columns[i]\n",
    "\n",
    "    if dftype == 'csv':\n",
    "        # build csv rename dictionary\n",
    "        renamed_columns['museumcatno'] = 'museumcatnumber'\n",
    "        renamed_columns['huc8number']  = 'huc8'\n",
    "    elif dftype == 'api':\n",
    "        # build api rename dictionary\n",
    "        renamed_columns['key']              = 'specimennumber'\n",
    "        renamed_columns['decimallatitude']  = 'latitude'\n",
    "        renamed_columns['decimallongitude'] = 'longitude'\n",
    "        renamed_columns['latlongsource']    = 'source'\n",
    "        renamed_columns['latlongaccuracy']  = 'accuracy'\n",
    "    else:\n",
    "        raise ValueError(f\"Dataframe type '{dftype}' invalid - Accepted inputs are 'csv' or 'api'\")\n",
    "\n",
    "    return renamed_columns\n",
    "\n",
    "def _manage_cols(df, drop_list=[], name_dict={}):\n",
    "    \"\"\"Private method for dropping and renaming columns in a dataframe, as well as creating one standard table from two different forms.\"\"\"\n",
    "\n",
    "    for colname in drop_list:\n",
    "        if colname not in df:\n",
    "            raise ValueError(f\"Can't drop column '{colname}' - '{colname}' does not exist in dataframe\")\n",
    "    for colname in list(name_dict.keys()):\n",
    "        if colname not in df:\n",
    "            raise ValueError(f\"Can't rename '{colname}' to '{name_dict[colname]}' - '{colname}' does not exist in dataframe\")\n",
    "        if colname in drop_list:\n",
    "            raise ValueError(f\"Can't rename '{colname}' to '{name_dict[colname]}' - '{colname}' in drop_list\")\n",
    "\n",
    "    column_names = np.setdiff1d(list(df.columns), list(name_dict.keys()))\n",
    "    lower_columns = [name.lower().replace(' ','').replace('_','') for name in column_names]\n",
    "    for i in range(len(column_names)):\n",
    "        name_dict[column_names[i]] = lower_columns[i]\n",
    "    \n",
    "    df = df.drop(drop_list, axis=1)\n",
    "    df = df.rename(columns=name_dict)\n",
    "    \n",
    "    return df\n",
    "\n",
    "URL_BASE = 'http://nas.er.usgs.gov/api/v2/'\n",
    "\n",
    "\n",
    "def api_df(species_id, limit, api_key):\n",
    "    \"\"\"Returns a pandas dataframe containing records about a species from the NAS database using their API\"\"\"\n",
    "    \n",
    "    # Check for API key\n",
    "    if api_key is not None:\n",
    "        url_request = f\"{URL_BASE}/occurrence/search?species_ID={species_id}&api_key={api_key}\"\n",
    "    else:\n",
    "        url_request = f\"{URL_BASE}/occurrence/search?species_ID={species_id}\"\n",
    "    \n",
    "    # Get dataframe from API request\n",
    "    request_json = requests.get(url_request, params={'limit':limit}).json()\n",
    "    api_df = pd.json_normalize(request_json, 'results')\n",
    "    api_df = _manage_cols(api_df)\n",
    "\n",
    "    # Add columns that are in a CSV dataframe but not an API dataframe\n",
    "    api_df['country']      = np.nan\n",
    "    api_df['drainagename'] = np.nan\n",
    "\n",
    "    # Rename columns\n",
    "    renamed_columns = _get_col_rename(api_df, 'api')\n",
    "    api_df = api_df.rename(columns=renamed_columns)\n",
    "\n",
    "    # Reorder columns\n",
    "    cols = list(api_df.columns)\n",
    "    cols = cols[0:8] + cols[33:34] + cols[8:33] + cols[34:] # country\n",
    "    cols = cols[0:16] + cols[34:] + cols[16:34] # drainagename\n",
    "    api_df = api_df[cols]\n",
    "    \n",
    "    return api_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09ae10-e03c-40b7-bf24-8dadffeec380",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_NAS = api_df(my_species_id = 5, limit = 10000, api_key = {\"speciesID\": my_species_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4591ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = my_NAS[[\"state\", \"latitude\", \"longitude\", \"year\", \"status\", \"accuracy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_fltr = my_data[(my_data['status'] == 'established') & (my_data['accuracy'] == 'Accurate')\n",
    "& (my_data['state'] == MY_state)] # Change to your state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_fixed = my_data_fltr.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn it into a geo datafrome\n",
    "user_data_gdf = gpd.GeoDataFrame(\n",
    "    my_data_fixed, geometry=gpd.points_from_xy(my_data_fixed.longitude, my_data_fixed.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f863b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Coordinate Reference System (CRS)\n",
    "user_data_gdf.crs = \"EPSG:4326\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert geopandas object into ee object\n",
    "fc = geemap.geopandas_to_ee(user_data_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add present property \n",
    "def addPresent (property):\n",
    "    return property.set('Present', 1);\n",
    "\n",
    "user_data = fc.map(addPresent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split for training and testing\n",
    "data_split = user_data.randomColumn()\n",
    "training_data = data_split.filter(ee.Filter.lte('random', 0.75)).merge(MY_background)\n",
    "testing_data = data_split.filter(ee.Filter.gt('random', 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f72155",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_choices = [\n",
    "  'Heat_Insolation_Load', \n",
    "  'Topographic_Diversity', \n",
    "  'gHM', \n",
    "  'Max_LST_Annual',\n",
    "  'Mean_GPP', \n",
    "  'Mean_NDVI', \n",
    "  'winter_totalPrecip', \n",
    "  'summer_totalPrecip', \n",
    "  'Flashiness',\n",
    "  'elevation'\n",
    "];\n",
    "\n",
    "MY_input = MY_collections.select(covariate_choices);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ab723",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_points = MY_input.reduceRegions(**{\n",
    "                              'collection': training_data,\n",
    "                              'reducer': ee.Reducer.mean(),\n",
    "                              'crs': 'EPSG:4326',\n",
    "                              'scale': MY_scale,\n",
    "                              'tileScale': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Classifier and setup for MaxEnt Parameters \n",
    "classifier = ee.Classifier.amnhMaxent().train(**{\n",
    "    'features': training_points,\n",
    "    'classProperty': 'Present',\n",
    "    'inputProperties': MY_input.bandNames()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0deae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_results = classifier.explain();\n",
    "classifier_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504acc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_input_classified = MY_input.classify(classifier);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create color paramaters and map classified image\n",
    "Probability_PARAMS = {\"opacity\":1,\"bands\":[\"probability\"],\n",
    "\"palette\":[\"2b83ba\",\"6ab0af\",\"abdda4\",\"cdebaf\",\"ffffbf\", \"fed790\", \"fdae61\", \"d7191c\",\"d7191c\"]}\n",
    "Map = foliumap.Map(center=[40,-100], zoom=4)\n",
    "Map.addLayer(MY_input_classified, Probability_PARAMS, 'Habitat Suitability')\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_points = MY_input_classified.reduceRegions(**{\n",
    "                              'collection': testing_data,\n",
    "                              'reducer': ee.Reducer.mean(),\n",
    "                              'crs': 'EPSG:4326',\n",
    "                              'scale': MY_scale,\n",
    "                              'tileScale': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_testing = testing_points.select('probability');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4078c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_false_negs =  false_neg_testing.filter(ee.Filter.lessThan('probability', 0.30)).size();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_true_pos = false_neg_testing.filter(ee.Filter.greaterThan('probability', 0.70)).size();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adceb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fn_rate =  my_false_negs.divide(my_false_negs.add(my_true_pos));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37732b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fn_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ebf38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_df = geemap.ee_to_df(false_neg_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fdedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 20\n",
    "x = false_neg_df\n",
    "plt.hist(x, n_bins, density = True, \n",
    "         histtype ='bar',\n",
    "         color = 'green')\n",
    "\n",
    "plt.title('Testing Predictions\\n\\n',\n",
    "          fontweight =\"bold\")\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6599df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Map as GeoTiff\n",
    "export = ee.batch.Export.image.toDrive(image = MY_input_classified,\n",
    "                    description = 'MY_prediction',\n",
    "                    region = MY_geo,\n",
    "                    scale =  MY_scale,\n",
    "                    maxPixels = 1e13)\n",
    "export.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3e887-5166-45d7-af6b-85456e602504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import requests\n",
    "from shapely.geometry import Point\n",
    "from functools import reduce\n",
    "import folium\n",
    "import io\n",
    "import os\n",
    "import zipfile\n",
    "from matplotlib import pyplot as plt\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import skgstat as skg\n",
    "import gstools as gs\n",
    "from skgstat import models\n",
    "from skgstat.util.likelihood import get_likelihood\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "import pykrige.kriging_tools as kt\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import random\n",
    "from rasterio.features import rasterize\n",
    "import scipy.ndimage\n",
    "from rasterio.transform import from_origin\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from rasterio.mask import mask\n",
    "import rasterio.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc18a25-f28e-419e-adb0-99f9ea9048ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined variables\n",
    "my_state = 'WI' # State USPS abbreviation\n",
    "state_name = 'Wisconsin'\n",
    "# IA = 19; ID = 16; IL = 17; MN = 27; MO = 29; MT = 30; OR = 41;  WA = 53; WI = 55\n",
    "state_fip = 'US%3A55' # Replace last 2 digits with your state's FIP code\n",
    "my_crs = 5070 # change to recommended projected crs for your state\n",
    "my_path = 'data/' + my_state + '/' # leave this alone   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cac097-3ddf-4747-9f20-2f7415faa655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def get_station_data(state):\n",
    "    URL_BASE_2 = 'https://www.waterqualitydata.us/data/Station/search?'\n",
    "    url_request_2 = f\"{URL_BASE_2}countrycode=US&statecode={state}\"\n",
    "    response_2 = pd.read_csv(url_request_2)\n",
    "    return response_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fef787-a519-437c-a806-9919adadc396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download state boundaries shapefile\n",
    "state_boundary_url = 'http://www2.census.gov/geo/tiger/TIGER2012/STATE/tl_2012_us_state.zip'\n",
    "local_path = my_path\n",
    "print('Downloading shapefile...')\n",
    "r = requests.get(state_boundary_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "print(\"Done\")\n",
    "z.extractall(path=local_path) # extract to folder\n",
    "filenames = [y for y in sorted(z.namelist()) for ending in ['dbf', 'prj', 'shp', 'shx'] if y.endswith(ending)] \n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c0fb8-e1a2-4d8a-aef9-c164675c8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get USGS monitoring stations from their API\n",
    "my_stations = get_station_data(state = state_fip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee65a44f-d0e7-4547-bd22-1669e8f38019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter and convert to a Geodataframe\n",
    "stations = my_stations[['MonitoringLocationIdentifier', 'LatitudeMeasure', 'LongitudeMeasure']].rename(columns = {'MonitoringLocationIdentifier': 'station_id',\n",
    "                        'LatitudeMeasure' : 'latitude', 'LongitudeMeasure' : 'longitude'})  \n",
    "stations_gdf = gpd.GeoDataFrame(\n",
    "    stations, geometry=gpd.points_from_xy(stations.longitude, stations.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a07096-1f77-4e3c-acf9-9ae266f9bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_gdf.to_file(my_path + 'usgs_monitoring_stations.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c9598-c0b5-4337-ad64-2e0c2401a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_gdf = gpd.read_file(my_path + 'usgs_monitoring_stations.shp').set_crs(my_crs, allow_override=True)\n",
    "state_boundary = gpd.read_file(my_path + 'tl_2012_us_state.shp').dropna().set_crs(my_crs, allow_override=True)\n",
    "\n",
    "state = state_boundary[state_boundary['STUSPS'] == my_state]\n",
    "my_state_stations = gpd.sjoin(stations_gdf, state, predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d1a8f-76cd-45bc-ac6c-7c0c94a68be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(row, other_gdf):\n",
    "    # Find the index of the nearest geometry\n",
    "    nearest_idx = other_gdf.distance(row.geometry).idxmin()\n",
    "    return other_gdf.loc[nearest_idx]\n",
    "\n",
    "def add_nearest(gdf1, gdf2):\n",
    "    nearest_neighbors = gdf1.apply(lambda row: find_nearest(row, gdf2), axis=1)\n",
    "    # Add nearest neighbor information to the first GeoDataFrame\n",
    "    gdf1['nearest_id'] = nearest_neighbors['station_id']\n",
    "    gdf1['median'] = nearest_neighbors['median']\n",
    "    parameter_added = gdf1\n",
    "    return parameter_added\n",
    "\n",
    "def get_data(state, characteristic, my_format):\n",
    "    URL_BASE = 'https://www.waterqualitydata.us/data/Result/search?'\n",
    "    url_request = f\"{URL_BASE}countrycode=US&statecode={state}&characteristicName={characteristic}&mimeType={my_format}\"\n",
    "    response = pd.read_csv(url_request)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57f72b-5eb6-4ac2-89db-2cef840fe45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = get_data(state = state_fip, my_format = 'csv', characteristic = 'Calcium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fbcaad-3c5f-418c-a1d9-ef503a803a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pH = get_data(state = state_fip, my_format = 'csv', characteristic = 'pH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f493a4-911c-46a0-9187-3f6230adfe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "di_N = get_data(state = state_fip, my_format = 'csv', characteristic = 'Nitrogen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4288b11-c664-48f0-b1cf-83e7dac2ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "do = get_data(state = state_fip, my_format = 'csv', characteristic = 'Oxygen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fb1d5-20f1-45d3-afbe-101398a2785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "phos = get_data(state = state_fip, my_format = 'csv', characteristic = 'Phosphorus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b87d24-c9ac-4473-aad4-bc7a2045686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this takes a while....\n",
    "ca_resultsFile = pd.DataFrame(ca).dropna(subset=['ResultMeasureValue'])\n",
    "ca_resultsFile.ResultMeasureValue = pd.to_numeric(ca_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "ca_fixed = ca_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "# Change units for those that are equivalent to mg/L and make ug/L units consistent\n",
    "ca_clean_1 = ca_fixed.replace({'mg/l' : 'mg/L', 'mg/kg' : 'mg/L', 'mg/l CaCO3': 'mg/L', 'ug/l' : 'ug/L'} )\n",
    "# convert ug/L to mg/L\n",
    "ca_clean_1['value'] = ca_clean_1.apply(\n",
    "    lambda row: row['value'] / 1000 if row['unit'] == 'ug/L' else row['value'],\n",
    "    axis=1)\n",
    "#Change unit for converted values to mg/L\n",
    "ca_clean_2 = ca_clean_1.replace({'ug/L': 'mg/L'})\n",
    "#Get rid of other records not in mg/L\n",
    "my_ca = ca_clean_2[ca_clean_2['unit'] == 'mg/L']\n",
    "#Filter to reasonable values\n",
    "ca_fixed_2 = my_ca[my_ca['value'] < 300]\n",
    "ca_fixed_3 = ca_fixed_2[ca_fixed_2['value'] > 0]\n",
    "#Calculate summary stats by station_id\n",
    "ca_result = pd.DataFrame(ca_fixed_3.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "stations_w_ca = pd.merge(my_state_stations, ca_result, on = 'station_id').dropna()\n",
    "\n",
    "ca_station_list = stations_w_ca['station_id'].tolist()\n",
    "stations_no_ca = my_state_stations[~my_state_stations['station_id'].isin(ca_station_list)]\n",
    "ca = add_nearest(stations_no_ca, stations_w_ca)\n",
    "my_ca_result = pd.concat([ca, stations_w_ca], axis = 0).drop(columns = {\"nearest_id\",\"latitude\",\"longitude\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b69cb-c934-4403-aef9-61b7784b9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ca_result.to_file(my_path + 'usgs_ca.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cf75a-b560-40bb-8c66-f23bb6e46331",
   "metadata": {},
   "outputs": [],
   "source": [
    "pH_resultsFile =pd.DataFrame(pH).dropna(subset=['ResultMeasureValue'])\n",
    "pH_resultsFile.ResultMeasureValue = pd.to_numeric(pH_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "pH_fixed = pH_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "my_pH = pH_fixed.loc[(pH_fixed['unit'].isna()) | (pH_fixed['unit']== 'std units')]\n",
    "pH_result = pd.DataFrame(my_pH.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "pH_fixed_2 = pH_result[pH_result['median'] < 14]\n",
    "pH_fixed_3 = pH_fixed_2[pH_fixed_2['median'] > 4]\n",
    "stations_w_pH = pd.merge(my_state_stations, pH_fixed_3, on = 'station_id').dropna()\n",
    "\n",
    "pH_station_list = stations_w_pH['station_id'].tolist()\n",
    "stations_no_pH = my_state_stations[~my_state_stations['station_id'].isin(pH_station_list)]\n",
    "pH = add_nearest(stations_no_pH, stations_w_pH)\n",
    "my_pH_result = pd.concat([pH, stations_w_pH], axis = 0).drop(columns = {\"nearest_id\", \"latitude\", \"longitude\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cb362-63a5-414a-a1ed-9658d74a1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pH_result.to_file(my_path + 'usgs_pH.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4add793-1159-4f07-a559-47904ab0b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#di_N = get_data(state = state_fip, my_format = 'csv', characteristic = 'Nitrogen')\n",
    "di_N_resultsFile = pd.DataFrame(di_N).dropna(subset=['ResultMeasureValue'])\n",
    "di_N_resultsFile.ResultMeasureValue = pd.to_numeric(di_N_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "di_N_fixed = di_N_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "\n",
    "di_N_fixed.loc[(di_N_fixed['unit'] == 'ug/L') | \n",
    "       (di_N_fixed['unit'] == 'ppb') | (di_N_fixed['unit'] == 'mg/g'), 'value'] /= 1000\n",
    "di_N_clean_1 = di_N_fixed.replace({'mg/l' : 'mg/L', 'mg/kg' : 'mg/L', 'ppb': 'mg/L', 'ug/L' : 'mg/L'})\n",
    "di_fixed_2 = di_N_clean_1[di_N_clean_1['unit'] == 'mg/L']\n",
    "di_fixed_3 = di_fixed_2[di_fixed_2['value']<= 500]\n",
    "di_fixed_4 = di_fixed_3[di_fixed_3['value'] > 0]\n",
    "di_N_result = pd.DataFrame(di_fixed_4.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "stations_w_di_N = pd.merge(my_state_stations, di_N_result, on = 'station_id')#.to_crs(26915)\n",
    "di_N_station_list = stations_w_di_N['station_id'].tolist()\n",
    "stations_no_di_N = my_state_stations[~my_state_stations['station_id'].isin(di_N_station_list)]#.to_crs(26915)\n",
    "di_N = add_nearest(stations_no_di_N, stations_w_di_N)\n",
    "my_di_N_result = pd.concat([di_N, stations_w_di_N], axis = 0).drop(columns = {\"nearest_id\", \"latitude\",\"longitude\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0737d-b169-4126-a47c-182103a5c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_di_N_result.to_file(my_path + 'usgs_N.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf9dca-0184-413b-9343-23719f7ba27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_resultsFile = pd.DataFrame(do).dropna(subset=['ResultMeasureValue'])\n",
    "do_resultsFile.ResultMeasureValue = pd.to_numeric(do_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "do_fixed = do_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "do_fixed.loc[(do_fixed['unit'] == 'mg/L'), 'value'] *= 12.67\n",
    "do_clean_1 = do_fixed.replace({'mg/L' : '% saturatn', '% by vol' : '% saturatn'})\n",
    "do_fixed_2 = do_clean_1[do_clean_1['value']<= 1]\n",
    "do_fixed_3 = do_fixed_2[do_fixed_2['value']> 0]\n",
    "do_result = pd.DataFrame(do_fixed_3.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "stations_w_do = pd.merge(my_state_stations, do_result, on = 'station_id')#.to_crs(26915)\n",
    "do_station_list = stations_w_do['station_id'].tolist()\n",
    "stations_no_do = my_state_stations[~my_state_stations['station_id'].isin(do_station_list)]#.to_crs(26915)\n",
    "do = add_nearest(stations_no_do, stations_w_do)\n",
    "my_do_result = pd.concat([do, stations_w_do], axis = 0).drop(columns = {\"nearest_id\", \"latitude\",\"longitude\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7084f86b-ab0b-4a97-941c-120c2916e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_do_result.to_file(my_path + 'usgs_do.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26519e-aa8f-4ba0-b884-386bdcb1cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "phos_resultsFile = pd.DataFrame(phos).dropna(subset=['ResultMeasureValue'])\n",
    "phos_resultsFile.ResultMeasureValue = pd.to_numeric(phos_resultsFile.ResultMeasureValue, errors='coerce')\n",
    "phos_fixed = phos_resultsFile[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier', 'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\"CharacteristicName\": \"Characteristic\", \"ActivityStartDate\": \"date\", 'MonitoringLocationIdentifier' : \"station_id\", 'ResultMeasureValue' : \"value\", 'ResultMeasure/MeasureUnitCode': \"unit\"})\n",
    "phos_fixed.loc[(phos_fixed['unit'] == 'ug/L') | \n",
    "       (phos_fixed['unit'] == 'ppb') | (phos_fixed['unit'] == 'mg/g'), 'value'] /= 1000\n",
    "phos_clean_1 = phos_fixed.replace({'mg/l' : 'mg/L', 'mg/l PO4' : 'mg/L', 'mg/l as P' : 'mg/L', 'mg/kg' : 'mg/L', 'mg/kg as P' : 'mg/L',\n",
    "                                  'ug/L' : 'mg/L', 'ppb' : 'mg/L', 'mg/g' : 'mg/L'} )\n",
    "my_phos = phos_clean_1[phos_clean_1['unit'] == 'mg/L']\n",
    "phos_fixed_2 = my_phos[my_phos['value']<= 1]\n",
    "phos_result = pd.DataFrame(phos_fixed_2.groupby(['station_id'])['value'].aggregate(['median'])).reset_index()\n",
    "stations_w_phos = pd.merge(my_state_stations, phos_result, on = 'station_id')#.to_crs(26915)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f512ae-df59-47dd-b2d3-1b961386deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "phos_station_list = stations_w_phos['station_id'].tolist()\n",
    "stations_no_phos = my_state_stations[~my_state_stations['station_id'].isin(phos_station_list)]#.to_crs(26915)\n",
    "phos = add_nearest(stations_no_phos, stations_w_phos)\n",
    "my_phos_result = pd.concat([phos, stations_w_phos], axis = 0).drop(columns = {\"nearest_id\", \"latitude\",\"longitude\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eac31d-f007-4d0d-aade-77f27b227a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_phos_result.to_file(my_path + 'usgs_phos.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba4861-8db4-4814-a97f-5ddac91bda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(stations_w_data, filename, bandname=\"Interpolated_Band\", max_grid_size=500):\n",
    "    # Extract coordinates and values\n",
    "    stations_w_data['x'] = stations_w_data.geometry.x\n",
    "    stations_w_data['y'] = stations_w_data.geometry.y\n",
    "    samples_df = stations_w_data[['x', 'y', 'median']]\n",
    "    x_point = samples_df['x']\n",
    "    y_point = samples_df['y']\n",
    "    coords = np.column_stack((x_point, y_point))\n",
    "    vals = samples_df['median']\n",
    "    \n",
    "    # Scatter plot of original data\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    art = ax.scatter(coords[:, 0], coords[:, 1], s=10, c=vals, cmap='plasma', vmin=0, vmax=vals.max())\n",
    "    plt.colorbar(art)\n",
    "    \n",
    "    # Adaptive grid sizing to prevent memory overflow\n",
    "    xmin, xmax = x_point.min(), x_point.max()\n",
    "    ymin, ymax = y_point.min(), y_point.max()\n",
    "    \n",
    "    grid_x = min(max_grid_size, int((xmax - xmin) / 500))\n",
    "    grid_y = min(max_grid_size, int((ymax - ymin) / 500))\n",
    "\n",
    "    if grid_x * grid_y > 1e6:  # If interpolation grid is too large, skip Kriging\n",
    "        print(\"⚠️ Grid too large for Kriging, switching to Nearest Neighbor interpolation.\")\n",
    "        use_kriging = False\n",
    "    else:\n",
    "        use_kriging = True\n",
    "    \n",
    "    try:\n",
    "        if use_kriging:\n",
    "            print(\"🔹 Attempting Ordinary Kriging interpolation...\")\n",
    "            V = skg.Variogram(coords, vals, model='exponential', maxlag='median', n_lags=30, normalize=False)\n",
    "            V.plot(show=False)\n",
    "            print(f'Exponential RMSE: {V.rmse:.2f}')\n",
    "            print(f'Exponential effective range: {V.describe().get(\"effective_range\"):.1f}')\n",
    "            \n",
    "            ok = skg.OrdinaryKriging(V, min_points=1, max_points=8, mode='exact')\n",
    "            xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]  # Grid definition\n",
    "            field = ok.transform(xx.flatten(), yy.flatten()).reshape(xx.shape)\n",
    "            s2 = ok.sigma.reshape(xx.shape)\n",
    "            print(\"✅ Kriging completed successfully.\")\n",
    "        else:\n",
    "            raise MemoryError  # Manually trigger fallback\n",
    "\n",
    "    except (MemoryError, ValueError, np.linalg.LinAlgError):\n",
    "        print(\"⚠️ Kriging failed due to memory constraints or numerical issues. Switching to Nearest Neighbor interpolation...\")\n",
    "\n",
    "        # Nearest Neighbor Interpolation as fallback\n",
    "        xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]  # Ensure the same grid as before\n",
    "        knn = KNeighborsRegressor(n_neighbors=8, weights='distance', algorithm='kd_tree', n_jobs=-1)\n",
    "        knn.fit(coords, vals)\n",
    "\n",
    "        query_points = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "        nn_values = knn.predict(query_points).reshape(xx.shape)\n",
    "        field = nn_values.astype(np.float32)\n",
    "        s2 = np.zeros_like(field)  # No error estimate for Nearest Neighbor interpolation\n",
    "        print(\"✅ Nearest Neighbor interpolation completed successfully.\")\n",
    "    \n",
    "    # Fill NaN values using nearest neighbor interpolation\n",
    "    def fill_missing_values(field):\n",
    "        mask = np.isnan(field)  # Identify NaN locations\n",
    "        if np.all(mask):\n",
    "            raise ValueError(\"All values are NaN; interpolation cannot be performed.\")\n",
    "    \n",
    "        # Compute nearest neighbor indices for NaN positions\n",
    "        nearest_indices = scipy.ndimage.distance_transform_edt(mask, return_distances=False, return_indices=True)\n",
    "    \n",
    "        # Assign nearest valid values to NaN locations\n",
    "        field[mask] = field[tuple(nearest_indices[i][mask] for i in range(field.ndim))]\n",
    "    \n",
    "        return field\n",
    "\n",
    "    field = fill_missing_values(field)\n",
    "\n",
    "    # Export interpolated raster\n",
    "    nrows, ncols = np.shape(field.T)\n",
    "    xres = (xmax - xmin) / float(ncols)\n",
    "    yres = (ymax - ymin) / float(nrows)\n",
    "    arr = np.abs(field.T).astype(np.float32)\n",
    "\n",
    "    transform = from_origin(xmin, ymin, xres, -yres)\n",
    "    export_path = my_path + filename  # Assuming filename is already the full path\n",
    "\n",
    "    with rasterio.open(export_path, 'w', driver='GTiff',\n",
    "                       height=arr.shape[0], width=arr.shape[1],\n",
    "                       count=1, dtype=str(arr.dtype),\n",
    "                       crs='+proj=utm +zone=15 +ellps=GRS80 +datum=NAD83 +units=m +no_defs',\n",
    "                       transform=transform) as new_dataset:\n",
    "        new_dataset.write(arr, 1)\n",
    "        new_dataset.set_band_description(1, bandname)\n",
    "    \n",
    "    print(f\"Raster saved: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f62f2b-8f7a-4595-b90a-9b9c39cc5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pH_result = gpd.read_file(my_path + 'usgs_pH.shp')\n",
    "interpolate(my_pH_result, 'pH.tif', 'pH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736cb808-d1b0-4736-9f3a-e3ab6912118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ca_result = gpd.read_file(my_path + 'usgs_ca.shp')\n",
    "interpolate(my_ca_result, 'ca.tif', 'Ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebd38d-ab62-4dfd-b26d-013b984c8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_di_N_result = gpd.read_file(my_path + 'usgs_N.shp')\n",
    "interpolate(my_di_N_result, 'di_N.tif', 'Nitrogen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833a24d-7038-4b02-9d3b-94f40d9b7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_do_result = gpd.read_file(my_path + 'usgs_do.shp')\n",
    "interpolate(my_do_result, 'do.tif', 'DO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cce07c-fcda-4652-befb-ad4f1cd729a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_phos_result = gpd.read_file(my_path + 'usgs_phos.shp')\n",
    "interpolate(my_phos_result, 'phos.tif', 'Phos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994216e-871c-4035-9c98-d7e3b837dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of Biological Variables\n",
    "# Download waterbody shapefiles by state from NHD \n",
    "URL_BASE_NHD = 'https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHD/State/Shape/'\n",
    "NHD_url = f\"{URL_BASE_NHD}NHD_H_{state_name}_State_Shape.zip\"\n",
    "local_path = my_path\n",
    "print('Downloading shapefile...')\n",
    "r = requests.get(NHD_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "print(\"Done\")\n",
    "z.extractall(path=local_path) # extract to folder\n",
    "filenames = [y for y in sorted(z.namelist()) for ending in ['dbf', 'prj', 'shp', 'shx'] if y.endswith(ending)] \n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5080c-028c-4619-8f41-2dcff218ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the NAS database species_ids for some of the nastiest AIS\n",
    "my_nas_ids = [3294,1047,1012,551,730,1107,118,931,1045,3229,4,2937,1130,1134,\n",
    "2856,237,293,987,1100,92,714,211,538,514,2679,3100,3598,1115,235,239,95,796,910,\n",
    "217,1259,2938,1120,713,7,214,836,549,162,1688,761,714,1099,229,777,2682,5, 910, 939, 931,\n",
    "6, 1168, 1110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f35e8e-b7a7-43af-ad5f-7b7eb8c62508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function requests and retrieves the AIS records from the USGS NAS database API\n",
    "def api_call(nas_id, state):\n",
    "    responses = []\n",
    "    URL_BASE = 'http://nas.er.usgs.gov/api/v2/'\n",
    "    url_request = f\"{URL_BASE}/occurrence/search?species_ID={nas_id}&state={my_state}\"\n",
    "    response = requests.get(url_request, timeout=None)\n",
    "    responses.append(response.json())\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d9966-cd22-4552-b928-5a0538372528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This runs the NAS API function and stores the data in a list\n",
    "api_result = []\n",
    "for id in my_nas_ids:\n",
    "    api_result.append(api_call(id, my_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0490e2e-5975-47f0-a659-0ca54f52c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This formats that list into a dataframe for easier manipulation (filtering)\n",
    "all_nas_records = []\n",
    "# Iterate through each data block and append records from 'results' to all_records\n",
    "for block in api_result:\n",
    "    for record in block[0]['results']:\n",
    "        all_nas_records.append(record)\n",
    "\n",
    "# Create a pandas DataFrame from the list of records\n",
    "all_nas_df = pd.DataFrame(all_nas_records)\n",
    "all_nas_data = all_nas_df[[\"speciesID\", \"commonName\", \"group\", \"state\", \"decimalLatitude\", \"decimalLongitude\", \"year\", \"status\"]]\n",
    "all_nas_data_fltr = all_nas_data[(all_nas_data['status'] == 'established')].dropna()\n",
    "nas_gdf = gpd.GeoDataFrame(\n",
    "    all_nas_data_fltr, geometry=gpd.points_from_xy(all_nas_data_fltr.decimalLongitude, all_nas_data_fltr.decimalLatitude)).set_crs(\"EPSG:4269\")\n",
    "nas_gdf.to_file(my_path + my_state + \"_nas.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2a58d-578a-4734-9a09-aab3ceb21aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lake and river shapefiles produced earlier in this script for your state\n",
    "lakes = gpd.read_file(my_path + \"/shape/NHDWaterbody.shp\").set_crs(\"EPSG:4269\", allow_override=True)\n",
    "rivers = gpd.read_file(my_path + \"/shape/NHDArea.shp\").set_crs(\"EPSG:4269\", allow_override=True)\n",
    "water = pd.concat([lakes, rivers])\n",
    "my_water = water[water['areasqkm'] >= 0.05]\n",
    "# Create 100-meter buffer so we are certain to join points to water and create raster later\n",
    "my_water['buffer'] = my_water.buffer(.001)\n",
    "# Drop the original geometry and setting the new geometry\n",
    "buffered_water = my_water.drop(columns=['geometry']).set_geometry('buffer')\n",
    "buffered_water = buffered_water.rename_geometry('geometry')\n",
    "buffered_water.to_file(my_path + my_state + \"_buffered_water.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc97a9-d9ea-463a-94ee-39c3129439eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_gdf = gpd.read_file(my_path + my_state + \"_nas.shp\").set_crs(\"EPSG:4269\", allow_override=True)\n",
    "for col in nas_gdf.select_dtypes(include=['int64']).columns:\n",
    "    nas_gdf[col] = nas_gdf[col].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d239a9-f534-484f-a761-0c5af193ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered_water = gpd.read_file(my_path + my_state + \"_buffered_water.shp\").set_crs(\"EPSG:4269\", allow_override=True)\n",
    "NAS_ais_obs_df = gpd.sjoin(nas_gdf, buffered_water, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4cc554-6037-4d44-982e-221df78d0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Fishes', 'Fish')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Crustaceans-Cladocerans', 'Crustaceans')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Crustaceans-Crayfish', 'Crustaceans')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Mollusks-Bivalves', 'Mollusks')\n",
    "NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace('Mollusks-Gastropods', 'Mollusks')\n",
    "NAS_ais_df = NAS_ais_obs_df[['ObjectID', 'commonName', 'group']]\n",
    "unique_commonnames = NAS_ais_df.groupby(['ObjectID', 'group'])['commonName'].nunique().reset_index()\n",
    "pivot_df = unique_commonnames.pivot(index='ObjectID', columns='group', values='commonName').reset_index().fillna(0)\n",
    "lakes_w_invasives = pd.merge(buffered_water, pivot_df, on = 'ObjectID', how = 'left')\n",
    "# Define the required columns\n",
    "species_columns = ['Algae', 'Crustaceans', 'Fish', 'Mollusks', 'Plants', 'geometry']\n",
    "\n",
    "# Add missing columns and fill with 0\n",
    "for col in species_columns:\n",
    "    if col not in lakes_w_invasives.columns:\n",
    "        lakes_w_invasives[col] = 0  # Add missing column with default value 0\n",
    "\n",
    "# Now safely select the columns and fill NaN values with 0\n",
    "inv_rich = lakes_w_invasives[species_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48605f-acfd-4ec0-9e5a-dacc9d0a08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in inv_rich.select_dtypes(include=['int64']).columns:\n",
    "    inv_rich[col] = inv_rich[col].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee99fd-ba4d-4c3f-b12d-d3a65916bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bg_data(geo_df):    \n",
    "    species_columns = ['Algae', 'Crustaceans', 'Fish', 'Mollusks', 'Plants']\n",
    "    existing_columns = [col for col in species_columns if col in geo_df.columns]  # Only use existing columns\n",
    "    random_points = []\n",
    "\n",
    "    for _, row in geo_df.iterrows():\n",
    "        if not existing_columns or row[existing_columns].sum() == 0:  # Proceed if columns don't exist OR sum is 0\n",
    "            polygon = row.geometry\n",
    "            if polygon.is_empty or not polygon.is_valid:\n",
    "                random_points.append(None)\n",
    "                continue\n",
    "\n",
    "            minx, miny, maxx, maxy = polygon.bounds\n",
    "\n",
    "            while True:\n",
    "                random_point = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "                if polygon.contains(random_point):\n",
    "                    random_points.append(random_point)\n",
    "                    break\n",
    "        else:\n",
    "            random_points.append(None)  # Keep None for rows that don't match the condition\n",
    "\n",
    "    return gpd.GeoDataFrame(geometry=random_points, crs=geo_df.crs)\n",
    "\n",
    "bg_gdf = make_bg_data(inv_rich)  # Generate random points only for selected polygons\n",
    "bg_gdf.to_file(my_path + my_state +'_bg.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cde9a8-38c9-4279-bbb3-554446ec4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to a multi-band raster and export as geotiff\n",
    "\n",
    "inv_rich_gdf = inv_rich.to_crs(\"EPSG:3857\")  # Convert to a projected CRS if necessary\n",
    "\n",
    "data_columns = ['Algae', 'Crustaceans', 'Fish', 'Mollusks', 'Plants']\n",
    "\n",
    "# Debug: Ensure GeoDataFrame is not empty\n",
    "if inv_rich_gdf.empty:\n",
    "    raise ValueError(\"Error: The input GeoDataFrame is empty!\")\n",
    "\n",
    "# Set raster parameters\n",
    "pixel_size = 1000  # 1000 meters per pixel\n",
    "xmin, ymin, xmax, ymax = inv_rich_gdf.total_bounds  # Get bounding box\n",
    "\n",
    "# Debug: Print bounding box\n",
    "print(f\"Initial Bounds: xmin={xmin}, ymin={ymin}, xmax={xmax}, ymax={ymax}\")\n",
    "\n",
    "# Ensure bounds are valid (expand if necessary)\n",
    "if xmin == xmax:\n",
    "    xmin -= pixel_size\n",
    "    xmax += pixel_size\n",
    "\n",
    "if ymin == ymax:\n",
    "    ymin -= pixel_size\n",
    "    ymax += pixel_size\n",
    "\n",
    "# Recalculate width and height\n",
    "width = max(1, int((xmax - xmin) / pixel_size))  # Ensure at least 1 pixel\n",
    "height = max(1, int((ymax - ymin) / pixel_size))\n",
    "\n",
    "# Debug: Print updated raster dimensions\n",
    "print(f\"Updated Raster Dimensions: width={width}, height={height}\")\n",
    "\n",
    "# Define the transform for the raster\n",
    "transform = from_origin(xmin, ymax, pixel_size, pixel_size)\n",
    "\n",
    "# Number of bands (data columns + 1 for sum)\n",
    "num_bands = len(data_columns) + 1\n",
    "\n",
    "# Initialize an empty raster array\n",
    "raster = np.zeros((num_bands, height, width), dtype=np.float32)\n",
    "\n",
    "# Rasterize each data column into a separate band\n",
    "for i, column in enumerate(data_columns):\n",
    "    shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column]) if geom is not None and not np.isnan(value)]\n",
    "    \n",
    "    if not shapes:\n",
    "        print(f\"Skipping {column}: No valid geometries!\")\n",
    "        continue  # Skip empty bands\n",
    "\n",
    "    print(f\"Rasterizing {column} with {len(shapes)} geometries\")\n",
    "\n",
    "    raster_band = rasterize(\n",
    "        shapes,\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,  # Background remains 0\n",
    "        dtype='float32'  # Preserve decimal values\n",
    "    )\n",
    "\n",
    "    print(f\"Rasterized {column}: Min={raster_band.min()}, Max={raster_band.max()}\")\n",
    "    raster[i] = raster_band\n",
    "\n",
    "# Compute the sum band (last band)\n",
    "raster[-1] = np.sum(raster[:-1], axis=0)\n",
    "print(f\"Final Sum Band: Min={raster[-1].min()}, Max={raster[-1].max()}\")\n",
    "\n",
    "# Save as a multi-band GeoTIFF\n",
    "output_path = my_path + my_state + '_inv_richness.tif'\n",
    "with rasterio.open(\n",
    "    output_path, 'w',\n",
    "    driver='GTiff',\n",
    "    height=height,\n",
    "    width=width,\n",
    "    count=num_bands,\n",
    "    dtype=raster.dtype,\n",
    "    crs=inv_rich_gdf.crs,\n",
    "    transform=transform\n",
    ") as dst:\n",
    "    # Write each band\n",
    "    for band in range(num_bands):\n",
    "        dst.write(raster[band], band + 1)\n",
    "    \n",
    "    # Set band names\n",
    "    band_names = data_columns + [\"Inv_Richness\"]\n",
    "    for band, name in enumerate(band_names, start=1):\n",
    "        dst.set_band_description(band, name)  # Assign column names to bands\n",
    "\n",
    "print(f\"Raster file has been saved as '{output_path}' with band names: {band_names}\")\n",
    "\n",
    "# --- Plot the first few bands in Jupyter Notebook ---\n",
    "fig, axes = plt.subplots(1, num_bands, figsize=(15, 5))\n",
    "titles = data_columns + [\"Inv_Richness\"]\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(raster[i], cmap='viridis')\n",
    "    ax.set_title(titles[i])\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27b59c-daaf-4afc-b1a7-15f6a2e7129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from USGS native fish presence/absence dataset\n",
    "URL_BASE = \"https://www.sciencebase.gov/catalog/file/get/6086df60d34eadd49d31b04a?f=__disk__ad%2F21%2Ffc%2Fad21fc677379f4e45caa4bd506ca1c587d5f01f7\"\n",
    "\n",
    "response = requests.get(URL_BASE)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Convert response content to DataFrame\n",
    "    csv_data = StringIO(response.text)\n",
    "    fish_df = pd.read_csv(csv_data)\n",
    "\n",
    "    # Convert DataFrame to GeoDataFrame\n",
    "    fish_gdf = gpd.GeoDataFrame(\n",
    "        fish_df, geometry=gpd.points_from_xy(fish_df.longitude, fish_df.latitude)\n",
    "    ).set_crs(4269)\n",
    "else:\n",
    "    print(f\"Error: Failed to download data (status code {response.status_code})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57ea54-5f65-40fc-81de-f3e1bf518147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clip point geometries by polygon geometries\n",
    "def clip_points_by_polygon(points_gdf, polygon_gdf):\n",
    "    # Ensure that both GeoDataFrames are in the same CRS\n",
    "    if points_gdf.crs != polygon_gdf.crs:\n",
    "        points_gdf = points_gdf.to_crs(polygon_gdf.crs)\n",
    "\n",
    "    # Clip the points with the polygon(s)\n",
    "    clipped_points = gpd.sjoin(points_gdf, polygon_gdf, how='inner')\n",
    "\n",
    "    # Drop the geometry from polygon_gdf that was added during the join (if needed)\n",
    "    clipped_points = clipped_points.drop(columns=polygon_gdf.columns.difference(['geometry']))\n",
    "\n",
    "    return clipped_points\n",
    "\n",
    "# state_boundary = gpd.read_file(my_path + 'tl_2012_us_state.shp').dropna().to_crs(5070)\n",
    "# state = state_boundary[state_boundary['STUSPS'] == my_state]\n",
    "# Clip the points by the polygon\n",
    "clipped_native_fish = clip_points_by_polygon(fish_gdf, state)\n",
    "# Save to shapefile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c7e2eb-35f7-4fb6-a6f2-ec2f00b24178",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_native_fish.to_file(my_path + my_state + '_native_fish_gdf.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4a58c-d7e8-4220-a97d-fe647a46be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_native_fish = gpd.read_file(my_path + my_state + '_native_fish_gdf.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b7f16-dbf7-4ba3-b297-57d041eaf8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_numeric_columns(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    # Identify columns with six-digit string headers\n",
    "    sum_columns = [col for col in gdf.columns if col.isdigit() and len(col) == 6]\n",
    "    \n",
    "    # Sum across these columns\n",
    "    gdf[\"Native_Fish\"] = gdf[sum_columns].sum(axis=1)\n",
    "    \n",
    "    # Create a new GeoDataFrame with only the sum and geometry\n",
    "    return gdf[[\"Native_Fish\", \"geometry\"]]\n",
    "native_fish_gdf = sum_numeric_columns(clipped_native_fish)#.to_crs(4269)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09813b4-5ca9-4f03-91dc-bbc9b053c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_water = buffered_water#.to_crs(4269)\n",
    "def spatial_join_with_nearest(poly_gdf: gpd.GeoDataFrame, point_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    # Ensure both GeoDataFrames have the same CRS\n",
    "    if poly_gdf.crs != point_gdf.crs:\n",
    "        poly_gdf = poly_gdf.to_crs(point_gdf.crs)\n",
    "\n",
    "    # Perform a nearest spatial join\n",
    "    joined_gdf = gpd.sjoin_nearest(poly_gdf, point_gdf, how=\"left\", distance_col=\"distance\")\n",
    "\n",
    "    return joined_gdf\n",
    "\n",
    "water_w_native_fish = spatial_join_with_nearest(my_water, native_fish_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a2df7-032a-4e26-926b-fa26f33bd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in water_w_native_fish.select_dtypes(include=['int64']).columns:\n",
    "    water_w_native_fish[col] = water_w_native_fish[col].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db3545-28ce-4011-862d-dfa5a734a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_raster(joined_gdf: gpd.GeoDataFrame, resolution: int = 1000):\n",
    "    bounds = joined_gdf.total_bounds  # (minx, miny, maxx, maxy)\n",
    "    transform = rasterio.transform.from_origin(bounds[0], bounds[3], resolution, resolution)\n",
    "    out_shape = (\n",
    "        int(np.ceil((bounds[3] - bounds[1]) / resolution)),  \n",
    "        int(np.ceil((bounds[2] - bounds[0]) / resolution))\n",
    "    )\n",
    "    \n",
    "    column_name = \"Native_Fish\"  # Change this to dynamically select the column if needed\n",
    "    raster = rasterize(\n",
    "        [(geom, value) for geom, value in zip(joined_gdf.geometry, joined_gdf[column_name])],\n",
    "        out_shape=out_shape,\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype=rasterio.float32\n",
    "    )\n",
    "    \n",
    "    output_filename = f\"{my_path}{my_state}_{column_name}_richness.tif\"\n",
    "    \n",
    "    with rasterio.open(\n",
    "        output_filename, \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=out_shape[0],\n",
    "        width=out_shape[1],\n",
    "        count=1,\n",
    "        dtype=rasterio.float32,\n",
    "        crs=joined_gdf.crs,\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "        dst.set_band_description(1, column_name)  # Set band name\n",
    "    \n",
    "    # Plot the raster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(raster, cmap='viridis', extent=[bounds[0], bounds[2], bounds[1], bounds[3]])\n",
    "    plt.colorbar(label=f'{column_name} Richness')\n",
    "    plt.title('Rasterized GeoDataFrame')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1131470d-4c22-4300-ac29-0984dd5c5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_to_raster(water_w_native_fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb417e8-4a8c-4166-b932-9b76f0e2019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_geotiffs(input_files, output_file, state):\n",
    "    \"\"\"\n",
    "    Combines multiple single-band or multi-band GeoTIFFs into a single multi-band image,\n",
    "    masks it using the provided state GeoDataFrame, and ensures it is upright in the northern hemisphere.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_files (list of str): List of file paths to input GeoTIFFs.\n",
    "    - output_file (str): Path to the output combined GeoTIFF.\n",
    "    - state (GeoDataFrame): GeoDataFrame containing the mask geometry.\n",
    "    \"\"\"\n",
    "    \n",
    "    band_data = []  # Stores all bands\n",
    "    band_names = []  # Stores band descriptions\n",
    "    meta = None  # Metadata reference\n",
    "    \n",
    "    for file in input_files:\n",
    "        with rasterio.open(file) as src:\n",
    "            # Read all bands from the file\n",
    "            for i in range(1, src.count + 1):\n",
    "                band_data.append(src.read(i))  # Append band data\n",
    "                band_name = src.descriptions[i - 1] if src.descriptions[i - 1] else f\"Band_{len(band_data)}\"\n",
    "                band_names.append(band_name)  # Store band name\n",
    "            \n",
    "            # Set metadata from the first file\n",
    "            if meta is None:\n",
    "                meta = src.meta.copy()\n",
    "    \n",
    "    # Update metadata for the new multi-band output\n",
    "    meta.update(count=len(band_data))\n",
    "    \n",
    "    # Write combined multi-band GeoTIFF\n",
    "    with rasterio.open(output_file, 'w', **meta) as dst:\n",
    "        for i, data in enumerate(band_data):\n",
    "            dst.write(data, i + 1)  # Write band\n",
    "            dst.set_band_description(i + 1, band_names[i])  # Preserve band names\n",
    "    \n",
    "    # Define a nodata value (use -9999 for integer or np.nan for floating-point rasters)\n",
    "    nodata_value = -9999 if meta[\"dtype\"].startswith(\"int\") else np.nan\n",
    "    \n",
    "    # Apply mask using the state GeoDataFrame\n",
    "    with rasterio.open(output_file) as src:\n",
    "        out_image, out_transform = mask(src, state.geometry, crop=True, nodata=nodata_value, all_touched=True)\n",
    "        out_meta = src.meta.copy()\n",
    "        out_meta.update({\n",
    "            \"height\": out_image.shape[1],\n",
    "            \"width\": out_image.shape[2],\n",
    "            \"transform\": out_transform,\n",
    "            \"nodata\": nodata_value\n",
    "        })\n",
    "        \n",
    "        # Apply nodata value manually to the masked array\n",
    "        out_image = np.where(out_image == 0, nodata_value, out_image)\n",
    "    \n",
    "    # Save the masked raster\n",
    "    with rasterio.open(output_file, \"w\", **out_meta) as dst:\n",
    "        for i in range(out_image.shape[0]):\n",
    "            dst.write(out_image[i], i + 1)\n",
    "            dst.set_band_description(i + 1, band_names[i])  # Preserve band names\n",
    "    \n",
    "    print(f\"Successfully created masked and upright {output_file} with {len(band_data)} bands.\")\n",
    "    \n",
    "    # Print band names for verification\n",
    "    print(\"Band Names:\")\n",
    "    for i, name in enumerate(band_names, start=1):\n",
    "        print(f\"Band {i}: {name}\")\n",
    "    \n",
    "    # Plot the raster for inspection using rasterio's built-in function\n",
    "    with rasterio.open(output_file) as src:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        image = src.read(1)\n",
    "        image_flipped = np.flipud(image)  # Ensure the raster is upright\n",
    "        rasterio.plot.show(image_flipped, transform=src.transform, ax=ax, cmap='viridis')\n",
    "        ax.set_title(\"Masked Raster Inspection\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42df5dc-c332-47fc-952f-065fe5c5e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [my_path + \"ca.tif\", my_path + \"pH.tif\", my_path + \"di_N.tif\", my_path + \"do.tif\", my_path + \"phos.tif\", my_path + my_state + \"_rsd.tif\",\n",
    "              my_path + my_state + \"_inv_richness.tif\", my_path + my_state + \"_Native_Fish_Richness.tif\"]\n",
    "output_file = my_path + my_state + \"_raster.tif\"\n",
    "combine_geotiffs(input_files, output_file, state)\n",
    "# Print the band names of the output raster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67be9a6-a904-4536-8914-668c38655e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0566b02-8aba-4a40-ad82-d8c0871578cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3ac17-3d7c-43af-ab4c-b689bc39d120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e61629-1f62-44bd-90bb-ef8f30142f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb6b12e1-c5a3-4dcc-9810-567be116b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "import json\n",
    "import requests\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.sample import sample_gen\n",
    "from rasterio.mask import mask\n",
    "import matplotlib.colors as mcolors\n",
    "from shapely.geometry import mapping\n",
    "from shapely.geometry import shape\n",
    "import folium\n",
    "from folium.plugins import Fullscreen\n",
    "from rasterio.transform import from_bounds\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99af3444-3e5b-4fe9-a4aa-26e5ecf7205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "class Ensemble():\n",
    "    def __init__(self, models = []):\n",
    "        self.models = models\n",
    "        self.accs_dict = None\n",
    "        self.rocs_dict = None\n",
    "        self.model_names = [m.__class__.__name__ for m in models]\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit_all(self, X_train, y_train):\n",
    "        for m in self.models:\n",
    "            print(\"Fitting\", m.__class__.__name__)\n",
    "            m.fit(X_train, y_train)\n",
    "            print(m.__class__.__name__, 'fit.')\n",
    "        \n",
    "    \n",
    "    def evaluate_all(self, X_test, y_true, metric = 'acc'):\n",
    "        accs = [accuracy_score(y_true, m.predict(X_test)) for m in self.models]\n",
    "        accs_dict = dict(zip(self.model_names, accs))\n",
    "        \n",
    "        \n",
    "        rocs = [roc_auc_score(y_true, m.predict(X_test)) for m in self.models]\n",
    "        rocs_dict = dict(zip(self.model_names, rocs))\n",
    "        \n",
    "        self.rocs_dict = rocs_dict\n",
    "        self.accs_dict = accs_dict\n",
    "        \n",
    "        if metric == 'acc':\n",
    "            return accs_dict\n",
    "        \n",
    "        return rocs_dict\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.rocs_dict\n",
    "    \n",
    "    def get_model_names(self):\n",
    "        return self.model_names\n",
    "        \n",
    "# Function that runs the \"Drop Column\" Feature importance technique \n",
    "# I actually have these in a separate .py file which would be much cleaner. \n",
    "\n",
    "def make_imp_df(column_names, importances):\n",
    "    df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "def drop_col(model, X_train, y_train, random_state = 42):\n",
    "    #Clone the model\n",
    "    model_clone = clone(model)\n",
    "    #Reset random state\n",
    "    model_clone.random_state = random_state\n",
    "    #Train and score the benchmark model\n",
    "    model_clone.fit(X_train, y_train)\n",
    "    benchmark_score = model_clone.score(X_train, y_train)\n",
    "    #Store importances\n",
    "    importances = []\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        model_clone = clone(model)\n",
    "        model_clone.random_state = random_state\n",
    "        model_clone.fit(X_train.drop(col,axis=1),y_train)\n",
    "        drop_col_score = model_clone.score(X_train.drop(col, axis = 1), y_train)\n",
    "        importances.append(benchmark_score - drop_col_score)\n",
    "        \n",
    "    importances_df = make_imp_df(X_train.columns, importances)\n",
    "    return importances_df        \n",
    "        \n",
    "def api_call(nas_id, state):\n",
    "    URL_BASE = 'http://nas.er.usgs.gov/api/v2/'\n",
    "    url_request = f\"{URL_BASE}/occurrence/search?species_ID={nas_id}&state={my_state}\"\n",
    "    response = requests.get(url_request, timeout=None).json()\n",
    "    results = pd.json_normalize(response, 'results')\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def sample_multiband_geotiff_with_names(raster_path, gdf):\n",
    "    \"\"\"\n",
    "    Samples a multi-band GeoTIFF at specified point locations from a GeoDataFrame,\n",
    "    using band names from the raster.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_path (str): Path to the GeoTIFF file.\n",
    "    - gdf (GeoDataFrame): GeoDataFrame containing point geometries.\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame with additional columns for each band, using raster band names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the raster file\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Reproject GeoDataFrame to match raster CRS if needed\n",
    "        if gdf.crs != src.crs:\n",
    "            gdf = gdf.to_crs(src.crs)\n",
    "\n",
    "        # Convert point geometries to raster pixel coordinates\n",
    "        coords = [(geom.x, geom.y) for geom in gdf.geometry]\n",
    "\n",
    "        # Sample raster at point locations (returns a list of tuples with values per band)\n",
    "        sampled_values = list(src.sample(coords))\n",
    "\n",
    "        # Get band names (if available, otherwise use default names)\n",
    "        band_names = src.descriptions if all(src.descriptions) else [f\"band_{i+1}\" for i in range(src.count)]\n",
    "\n",
    "        # Create new columns in the GeoDataFrame with the corresponding band names\n",
    "        for band_idx, band_name in enumerate(band_names):\n",
    "            gdf[band_name] = [val[band_idx] for val in sampled_values]\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def calculate_metrics_and_plot(conf_matrices):\n",
    "    sensitivities = []\n",
    "    specificities = []\n",
    "    tss_values = []\n",
    "    \n",
    "    # Iterate through each confusion matrix\n",
    "    for cm in conf_matrices:\n",
    "        TN, FP, FN, TP = cm.ravel()  # Flatten confusion matrix\n",
    "\n",
    "        sensitivity = TP / (TP + FN)  # Sensitivity (TPR)\n",
    "        specificity = TN / (TN + FP)  # Specificity (TNR)\n",
    "        tss = sensitivity + specificity - 1  # True Skill Statistic (TSS)\n",
    "\n",
    "        sensitivities.append(sensitivity)\n",
    "        specificities.append(specificity)\n",
    "        tss_values.append(tss)\n",
    "\n",
    "    # Compute mean specificity and sensitivity\n",
    "    mean_sensitivity = np.mean(sensitivities)\n",
    "    mean_specificity = np.mean(specificities)\n",
    "\n",
    "    # Plot Boxplot of TSS\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(y=tss_values, color=\"skyblue\")\n",
    "    plt.axhline(mean_sensitivity, color=\"red\", linestyle=\"--\", label=f'Mean Sensitivity: {mean_sensitivity:.3f}')\n",
    "    plt.axhline(mean_specificity, color=\"green\", linestyle=\"--\", label=f'Mean Specificity: {mean_specificity:.3f}')\n",
    "    plt.ylabel(\"True Skill Statistic (TSS)\")\n",
    "    plt.title(\"Boxplot of TSS with Mean Sensitivity & Specificity\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return sensitivities, specificities, tss_values\n",
    "\n",
    "def clip_points_with_polygon(points_gdf, polygon_gdf):\n",
    "    clipped_gdf = points_gdf[points_gdf.geometry.within(polygon_gdf.unary_union)]\n",
    "    return clipped_gdf\n",
    "\n",
    "def filter_dataframe_columns(df, feature_choices):\n",
    "    return df[[col for col in df.columns if col in feature_choices]]\n",
    "\n",
    "\n",
    "def mark_true(series):\n",
    "    return [True if feature in series else False for feature in X_train_scaled.columns]\n",
    "\n",
    "def rename_dict(dictionary, tek_name):\n",
    "    return_names = []\n",
    "    return_lists = []\n",
    "    \n",
    "    for item in dictionary.items():\n",
    "        return_names.append(tek_name + str(item[0]))\n",
    "        return_lists.append(mark_true(list(item[1])))\n",
    "        \n",
    "    return dict(zip(return_names, return_lists))\n",
    "# Function to create empty dictionaries for feature importance\n",
    "def make_dict():\n",
    "    return {tup[0]: None for tup in vc_names}\n",
    "\n",
    "def make_dict_imp():\n",
    "    return dict(zip([tup[0] for tup in vc_names], [None]))\n",
    "\n",
    "# Function to append model weights to indv_roc\n",
    "def append_model_weights(classifiers, X_test_scaled, y_test):\n",
    "    weights = [accuracy_score(y_test, clf.predict(X_test_scaled)) for clf in classifiers]\n",
    "    indv_roc.append(weights)\n",
    "    return weights\n",
    "\n",
    "def predict_raster_and_display_folium(model, scaler, input_raster, output_raster, feature_list, state_polygon):\n",
    "    \"\"\"\n",
    "    Predicts species presence probability from a raster using a trained model and displays an interactive Folium map.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained machine learning model (supports `predict_proba`)\n",
    "    - scaler: StandardScaler fitted to training data\n",
    "    - input_raster: Path to input raster file\n",
    "    - output_raster: Path to save predicted probability raster\n",
    "    - feature_list: List of selected feature names (matching band names in the raster)\n",
    "    - state_polygon: GeoDataFrame containing the state boundary\n",
    "    \"\"\"\n",
    "    # Open the raster file\n",
    "    with rasterio.open(input_raster) as src:\n",
    "        profile = src.profile  # Get metadata\n",
    "        band_names = src.descriptions  # Band names in the raster\n",
    "\n",
    "        if band_names is None:\n",
    "            raise ValueError(\"Raster bands are missing names. Ensure input raster has band descriptions.\")\n",
    "\n",
    "        # Select only the bands that were used in training\n",
    "        selected_bands_indices = [i for i, band in enumerate(band_names) if band in feature_list]\n",
    "\n",
    "        if not selected_bands_indices:\n",
    "            raise ValueError(\"No matching bands found in the raster for the provided feature list.\")\n",
    "\n",
    "        # Mask raster to the state polygon\n",
    "        state_geom = [shape(state_polygon.geometry.iloc[0])]\n",
    "        img_data, transform = mask(src, state_geom, crop=True, nodata=np.nan)\n",
    "\n",
    "        # Read only the selected bands\n",
    "        img_data = img_data.astype(float)\n",
    "        img_data = img_data[selected_bands_indices, :, :]\n",
    "\n",
    "        # Reshape to 2D (pixels as rows, bands as columns)\n",
    "        num_bands = len(selected_bands_indices)\n",
    "        num_pixels = img_data.shape[1] * img_data.shape[2]\n",
    "        img_reshaped = img_data.reshape(num_bands, num_pixels).T\n",
    "\n",
    "        # Remove NaN values\n",
    "        valid_mask = ~np.isnan(img_reshaped).any(axis=1)\n",
    "        valid_pixels = img_reshaped[valid_mask]\n",
    "\n",
    "        # Convert to DataFrame with correct feature names\n",
    "        valid_pixels_df = pd.DataFrame(valid_pixels, columns=[feature_list[i] for i in range(len(selected_bands_indices))])\n",
    "\n",
    "        # Ensure valid_pixels_scaled remains a DataFrame with correct feature names\n",
    "        valid_pixels_scaled = pd.DataFrame(scaler.transform(valid_pixels_df), columns=feature_list)\n",
    "        \n",
    "        # Now, pass this DataFrame to the model for prediction\n",
    "        predicted_probs = model.predict_proba(valid_pixels_scaled)[:, 1]\n",
    "\n",
    "\n",
    "        # Create an empty array for the full image, setting invalid pixels to NaN\n",
    "        predicted_raster = np.full(img_reshaped.shape[0], np.nan)\n",
    "        predicted_raster[valid_mask] = predicted_probs\n",
    "\n",
    "        # Reshape back to original raster shape\n",
    "        predicted_raster = predicted_raster.reshape(img_data.shape[1], img_data.shape[2])\n",
    "\n",
    "        # Update metadata for a single-band float output raster\n",
    "        profile.update(dtype=rasterio.float32, count=1, compress='lzw')\n",
    "\n",
    "        # Write the predicted probability raster to a new file\n",
    "        with rasterio.open(output_raster, 'w', **profile) as dst:\n",
    "            dst.write(predicted_raster.astype(rasterio.float32), 1)\n",
    "\n",
    "    minx, maxy = transform * (0, 0)  # Upper-left corner\n",
    "    maxx, miny = transform * (predicted_raster.shape[1], predicted_raster.shape[0])  # Lower-right corner\n",
    "\n",
    "        \n",
    "    # Normalize values for visualization (0 to 1)\n",
    "    normed_raster = (predicted_raster - np.nanmin(predicted_raster)) / (np.nanmax(predicted_raster) - np.nanmin(predicted_raster))\n",
    "\n",
    "    # Convert to RGB heatmap image\n",
    "    cmap = plt.get_cmap(\"jet\")  # Use 'jet' colormap\n",
    "    heatmap_image = (cmap(normed_raster)[:, :, :3] * 255).astype(np.uint8)  # Convert to 8-bit RGB\n",
    "\n",
    "    # Flip image vertically because Folium expects north-up orientation\n",
    "    #heatmap_image = np.flipud(heatmap_image)\n",
    "\n",
    "    # Create a folium map centered on the raster bounds\n",
    "    m = folium.Map(location=[(miny + maxy) / 2, (minx + maxx) / 2], zoom_start=6)\n",
    "\n",
    "    # Overlay heatmap as an image\n",
    "    folium.raster_layers.ImageOverlay(\n",
    "        image=heatmap_image,\n",
    "        bounds=[[miny, minx], [maxy, maxx]],\n",
    "        opacity=0.6,\n",
    "        colormap=lambda x: (x, 0, 1 - x, 1),  # Adjust color mapping\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add fullscreen button\n",
    "    Fullscreen().add_to(m)\n",
    "\n",
    "    # Display map in Jupyter Notebook\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb87a3d-0234-4f8c-a0fd-5cbd9a83de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User defined variables\n",
    "\n",
    "my_state = 'MN' # should be the postal code abbreviation for the state you created the environmental raster for....\n",
    "my_nas_id = 5 # go to USGS NAS database for species_ids\n",
    "\n",
    "feature_choices = [\n",
    "  # 'Freeze_Up',\n",
    "  # 'Ice_Melt',\n",
    "  # 'Spawn_Start',\n",
    "  # 'Spawn_End',\n",
    "  # 'Spawn_Ideal',\n",
    "  'Precip_Winter',\n",
    "  'Precip_Spring',\n",
    "  'Precip_Summer',\n",
    "  'Precip_Fall',\n",
    "  'Flashiness',\n",
    "  'Runoff',\n",
    "  'Drawdown',\n",
    "  'LST_Annual',\n",
    "  'LST_Summer',\n",
    "  'LST_Winter',\n",
    "  'LST_Spring',\n",
    "  'LST_Fall',\n",
    "  'NDVI',\n",
    "  'GPP_Annual',\n",
    "  'GPP_Summer',\n",
    "  'Heat_Insolation',\n",
    "  'Topo_Diversity',\n",
    "  'gHM',\n",
    "  'NDTI',\n",
    "  'NDBI',\n",
    "  'NDCI',\n",
    "  'NDSI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea21606a-582d-4b54-81a8-7ebc43a70e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leif.howard\\AppData\\Local\\Temp\\ipykernel_24108\\1771523472.py:148: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  clipped_gdf = points_gdf[points_gdf.geometry.within(polygon_gdf.unary_union)]\n",
      "C:\\Users\\leif.howard\\AppData\\Local\\Temp\\ipykernel_24108\\1771523472.py:148: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  clipped_gdf = points_gdf[points_gdf.geometry.within(polygon_gdf.unary_union)]\n"
     ]
    }
   ],
   "source": [
    "# retrieve occurence data from USGS NAS API\n",
    "pos_data = api_call(my_nas_id, my_state)\n",
    "my_data = pos_data[[\"decimalLatitude\", \"decimalLongitude\"]]\n",
    "pos_data_gdf = gpd.GeoDataFrame(\n",
    "    my_data, geometry=gpd.points_from_xy(my_data.decimalLongitude, my_data.decimalLatitude))\n",
    "#add Coordinate Reference System (CRS)\n",
    "pos_data_gdf.crs = \"EPSG:4326\"\n",
    "bg_data_gdf = gpd.read_file(\"background.shp\")\n",
    "# Load the GeoTIFF and GeoDataFrame\n",
    "raster_path = \"rsd_all_features.tif\"\n",
    "states_gdf = gpd.read_file(\"conus_polygon.shp\")\n",
    "state_polygon = states_gdf[states_gdf['STUSPS'] == my_state]\n",
    "# Sample raster at points\n",
    "pos_data = sample_multiband_geotiff_with_names(raster_path, pos_data_gdf)\n",
    "neg_data = sample_multiband_geotiff_with_names(raster_path, bg_data_gdf)\n",
    "# clip the data to your state\n",
    "clipped_negs = clip_points_with_polygon(neg_data, state_polygon)\n",
    "clipped_pos = clip_points_with_polygon(pos_data, state_polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80fd6291-495e-47e7-9fec-64a073060366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the environmental data at occurence and background locations\n",
    "my_pos = filter_dataframe_columns(clipped_pos, feature_choices).dropna().astype(float)\n",
    "my_negs = filter_dataframe_columns(clipped_negs, feature_choices).dropna().astype(float)\n",
    "my_pos[\"Present\"], my_negs[\"Present\"] = 1.0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4569024-4f24-4650-9fb9-bd45cce7e432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Iteration 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m classifiers \u001b[38;5;241m=\u001b[39m [MaxEnt, rf, dt, brt, mlp]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m classifiers:\n\u001b[1;32m---> 37\u001b[0m     \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Append individual model weights\u001b[39;00m\n\u001b[0;32m     40\u001b[0m weights \u001b[38;5;241m=\u001b[39m append_model_weights(classifiers, X_test_scaled, y_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nasdm\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nasdm\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nasdm\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    876\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[0;32m    877\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[0;32m    878\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    879\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    882\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 883\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nasdm\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:489\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    486\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    488\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 489\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    494\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nasdm\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nasdm\\lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1404\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nasdm\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test model and get weights for feature importance analysis.  This takes a while.  \n",
    "# Initialize lists for metrics and feature importance\n",
    "indv_roc = []\n",
    "vc_roc_arr, vc_cm_arr = [], []\n",
    "stack_roc_arr, stack_cm_arr = [], []\n",
    "\n",
    "for x in range(1, 11): \n",
    "    print(f\"Iteration {x}\")\n",
    "    \n",
    "    # Splitting positive and negative samples\n",
    "    X_train_pos, X_test_pos, y_train_pos, y_test_pos = train_test_split(\n",
    "        my_pos.drop(columns=['Present']), my_pos[\"Present\"], test_size=0.30, train_size=0.70, random_state=123)\n",
    "    X_train_neg, X_test_neg, y_train_neg, y_test_neg = train_test_split(\n",
    "        my_negs.drop(columns=['Present']), my_negs[\"Present\"], test_size=0.15, train_size=0.25, random_state=123)\n",
    "    \n",
    "    # Combine training sets\n",
    "    X_train = pd.concat([X_train_pos, X_train_neg])\n",
    "    y_train = pd.concat([y_train_pos, y_train_neg])\n",
    "    \n",
    "    # Standardizing the data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test = pd.concat([X_test_pos, X_test_neg])\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_test = pd.concat([y_test_pos, y_test_neg])\n",
    "    \n",
    "    # Define classifiers\n",
    "    MaxEnt = LogisticRegression(max_iter=10000)\n",
    "    rf = RandomForestClassifier(n_estimators=1000)\n",
    "    brt = GradientBoostingClassifier(n_estimators=1000)\n",
    "    dt = DecisionTreeClassifier()\n",
    "    mlp = MLPClassifier(max_iter=10000)\n",
    "    \n",
    "    # Train classifiers\n",
    "    classifiers = [MaxEnt, rf, dt, brt, mlp]\n",
    "    for clf in classifiers:\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Append individual model weights\n",
    "    weights = append_model_weights(classifiers, X_test_scaled, y_test)\n",
    "    \n",
    "    # Ensemble Voting Classifier\n",
    "    vc_names = [('DT', dt), ('MaxEnt', MaxEnt), ('MLP', mlp), ('BRT', brt), ('RF', rf)]\n",
    "    vc = VotingClassifier(estimators=vc_names, voting='soft', weights=weights)\n",
    "    vc.fit(X_train_scaled, y_train)\n",
    "    \n",
    "     \n",
    "    # Performance Metrics\n",
    "    vc_roc = accuracy_score(y_test, vc.predict(X_test_scaled))\n",
    "    vc_cm = confusion_matrix(y_test, vc.predict(X_test_scaled))\n",
    "        \n",
    "    vc_roc_arr.append(vc_roc)\n",
    "    vc_cm_arr.append(vc_cm)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91551f-f25b-4bbd-8141-69c56cb45c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivities, specificities, tss_values = calculate_metrics_and_plot(vc_cm_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aac006-b089-438d-8ede-f998bbba940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering DecisionTreeClassifier()\n",
      "Considering LogisticRegression(max_iter=10000)\n",
      "Considering GradientBoostingClassifier(n_estimators=1000)\n"
     ]
    }
   ],
   "source": [
    "# Calculate average weights for feature importance evaluation\n",
    "avg_weight = [sum(col) / float(len(col)) for col in zip(*indv_roc)]\n",
    "vc_imp = VotingClassifier(estimators=vc_names, voting='soft', weights=avg_weight)\n",
    "\n",
    "# Splitting positive and negative samples\n",
    "X_train_pos, X_test_pos, y_train_pos, y_test_pos = train_test_split(\n",
    "        my_pos.drop(columns=['Present']), my_pos[\"Present\"], test_size=0.30, train_size=0.70, random_state=123)\n",
    "X_train_neg, X_test_neg, y_train_neg, y_test_neg = train_test_split(\n",
    "        my_negs.drop(columns=['Present']), my_negs[\"Present\"], test_size=0.15, train_size=0.25, random_state=123)\n",
    "\n",
    "                  \n",
    "X_train = pd.concat([X_train_pos, X_train_neg])\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train))\n",
    "\n",
    "y_train = pd.concat([y_train_pos, y_train_neg])\n",
    "\n",
    "vc_imp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Initialize feature importance dictionaries\n",
    "rfe_dict, perm_dict, drop_dict = make_dict_imp(), make_dict_imp(), make_dict_imp()\n",
    "\n",
    "for alg in vc_imp.named_estimators:\n",
    "        dict_name = alg\n",
    "        clf = vc_imp.named_estimators[dict_name]\n",
    "        if dict_name == 'MLP':\n",
    "            continue\n",
    "        print(\"Considering\", clf)\n",
    "    \n",
    "        # Recursive Feature Elimination (RFE)\n",
    "        rfe_selector = RFECV(estimator=clf, min_features_to_select=3, step=1)\n",
    "        rfe_selector.fit(X_train_scaled, y_train)\n",
    "        rfe_support = rfe_selector.get_support()\n",
    "        rfe_features = X_train_scaled.loc[:,rfe_support].columns.tolist()\n",
    "        rfe_dict[dict_name] = rfe_features\n",
    "        # Permutation Importance\n",
    "        perm_imp = permutation_importance(clf, X_train_scaled, y_train)\n",
    "        perm_imp['feature'] = X_train_scaled.columns\n",
    "        perm_features = pd.DataFrame(perm_imp['feature'],perm_imp['importances_mean'],columns = ['Feature']) \\\n",
    "                        .sort_index(ascending=False)['Feature'].values[:3]\n",
    "        perm_dict[dict_name] = perm_features\n",
    "        # Drop Column Importance (Assuming a predefined drop_col function)\n",
    "        drop_dict_list = []\n",
    "        \n",
    "        drop_col_feats = drop_col(clf, X_train_scaled, y_train) #, random_state = 10)\n",
    "        drop_col_three = drop_col_feats.sort_values('feature_importance',ascending = False)['feature'][:3]\n",
    "        drop_dict[dict_name] = drop_col_three\n",
    "    \n",
    "print(\"Feature importance evaluation completed.\")\n",
    "\n",
    "\n",
    "#We end up with a dataframe that says, for any model / feature importance technique, whether or not a feature ended up in the top N (i.e. whether or not that feature is important).\n",
    "features_df = pd.concat([pd.DataFrame(rename_dict(perm_dict,'PERM_')).reset_index(drop=True),\n",
    "                        pd.DataFrame(rename_dict(rfe_dict,'RFE_')),\n",
    "                        pd.DataFrame(rename_dict(drop_dict, 'DROP_'))],axis=1)\n",
    "\n",
    "features_df['Total'] = np.sum(features_df, axis=1)\n",
    "features_df['Feature'] = X_train.columns\n",
    "features_df.sort_values(['Total','Feature'] , ascending=False,inplace=True)\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plt.setp( ax.xaxis.get_majorticklabels(), rotation=-45, ha=\"left\" )\n",
    "plt.setp( ax.xaxis.get_majorticklabels(), rotation=-45, ha=\"left\" )\n",
    "\n",
    "# plt.xticks(rotation=75)\n",
    "plt.bar(features_df['Feature'], features_df['Total'])\n",
    "plt.title(\"Variable importances\")\n",
    "plt.ylabel(\"Number of times a variable appeared in top 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3213e03c-8d70-4ef7-bc7e-d5bfb47a9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap with all data\n",
    "X_train_pos = my_pos.drop(columns=['Present'])\n",
    "y_train_pos = my_pos['Present']\n",
    "X_train_neg = my_negs.drop(columns=['Present'])\n",
    "y_train_neg = my_negs['Present']\n",
    "\n",
    "X_train = pd.concat([X_train_pos, X_train_neg])\n",
    "scaler_heatmap = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(scaler_heatmap.transform(X_train), columns=X_train.columns)\n",
    "\n",
    "y_train = pd.concat([y_train_pos, y_train_neg])\n",
    "\n",
    "MaxEnt = LogisticRegression(max_iter = 10000)\n",
    "rf = RandomForestClassifier(n_estimators=1000)\n",
    "brt = GradientBoostingClassifier(n_estimators=1000)\n",
    "dt = DecisionTreeClassifier()\n",
    "mlp = MLPClassifier(max_iter = 10000)\n",
    "\n",
    "# Individual Confusion Matrices\n",
    "MaxEnt.fit(X_train_scaled, y_train)\n",
    "rf.fit(X_train_scaled,y_train)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "brt.fit(X_train_scaled, y_train) \n",
    "mlp.fit(X_train_scaled, y_train)                    \n",
    "########################################################################################################\n",
    "#Ensemble \n",
    "ensemble_vc = Ensemble([dt, MaxEnt, rf, brt, mlp]) \n",
    "ensemble_vc.fit_all(X_train_scaled, y_train)\n",
    "weights = ensemble_vc.get_weights()\n",
    "vc_names = [('DT', dt), ('MaxEnt', MaxEnt), ('MLP', mlp), ('BRT', brt), ('RF', rf)]\n",
    "vc_heatmaps = VotingClassifier(estimators=vc_names, voting='soft', weights = weights)\n",
    "vc_heatmaps.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1d20f-42b3-4700-9673-d6b8441cf8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction and display the heatmap\n",
    "\n",
    "output_tif = \"predicted_probabilities.tif\"\n",
    "\n",
    "# Define the selected feature list (only include features you want to use)\n",
    "selected_features = X_train.columns.tolist()\n",
    "\n",
    "m = predict_raster_and_display_folium(vc_heatmaps, scaler_heatmap, raster_path, output_tif, selected_features, state_polygon)\n",
    "m  # This will display the interactive map in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2873dc-f0b2-4456-ad47-33b2347d6907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

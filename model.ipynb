{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b12e1-c5a3-4dcc-9810-567be116b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "import json\n",
    "import requests\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.sample import sample_gen\n",
    "from rasterio.mask import mask\n",
    "import matplotlib.colors as mcolors\n",
    "from shapely.geometry import mapping\n",
    "from shapely.geometry import shape\n",
    "import folium\n",
    "from folium.plugins import Fullscreen\n",
    "from rasterio.transform import from_bounds\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af3444-3e5b-4fe9-a4aa-26e5ecf7205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "class Ensemble():\n",
    "    def __init__(self, models = []):\n",
    "        self.models = models\n",
    "        self.accs_dict = None\n",
    "        self.rocs_dict = None\n",
    "        self.model_names = [m.__class__.__name__ for m in models]\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit_all(self, X_train, y_train):\n",
    "        for m in self.models:\n",
    "            print(\"Fitting\", m.__class__.__name__)\n",
    "            m.fit(X_train, y_train)\n",
    "            print(m.__class__.__name__, 'fit.')\n",
    "        \n",
    "    \n",
    "    def evaluate_all(self, X_test, y_true, metric = 'acc'):\n",
    "        accs = [accuracy_score(y_true, m.predict(X_test)) for m in self.models]\n",
    "        accs_dict = dict(zip(self.model_names, accs))\n",
    "        \n",
    "        \n",
    "        rocs = [roc_auc_score(y_true, m.predict(X_test)) for m in self.models]\n",
    "        rocs_dict = dict(zip(self.model_names, rocs))\n",
    "        \n",
    "        self.rocs_dict = rocs_dict\n",
    "        self.accs_dict = accs_dict\n",
    "        \n",
    "        if metric == 'acc':\n",
    "            return accs_dict\n",
    "        \n",
    "        return rocs_dict\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.rocs_dict\n",
    "    \n",
    "    def get_model_names(self):\n",
    "        return self.model_names\n",
    "        \n",
    "# Function that runs the \"Drop Column\" Feature importance technique \n",
    "# I actually have these in a separate .py file which would be much cleaner. \n",
    "\n",
    "def make_imp_df(column_names, importances):\n",
    "    df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "def drop_col(model, X_train, y_train, random_state = 42):\n",
    "    #Clone the model\n",
    "    model_clone = clone(model)\n",
    "    #Reset random state\n",
    "    model_clone.random_state = random_state\n",
    "    #Train and score the benchmark model\n",
    "    model_clone.fit(X_train, y_train)\n",
    "    benchmark_score = model_clone.score(X_train, y_train)\n",
    "    #Store importances\n",
    "    importances = []\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        model_clone = clone(model)\n",
    "        model_clone.random_state = random_state\n",
    "        model_clone.fit(X_train.drop(col,axis=1),y_train)\n",
    "        drop_col_score = model_clone.score(X_train.drop(col, axis = 1), y_train)\n",
    "        importances.append(benchmark_score - drop_col_score)\n",
    "        \n",
    "    importances_df = make_imp_df(X_train.columns, importances)\n",
    "    return importances_df        \n",
    "        \n",
    "def api_call(nas_id, state):\n",
    "    URL_BASE = 'http://nas.er.usgs.gov/api/v2/'\n",
    "    url_request = f\"{URL_BASE}/occurrence/search?species_ID={nas_id}&state={my_state}\"\n",
    "    response = requests.get(url_request, timeout=None).json()\n",
    "    results = pd.json_normalize(response, 'results')\n",
    "    return results\n",
    "\n",
    "def sample_multiband_geotiff_with_names(raster_path, gdf):\n",
    "    \"\"\"\n",
    "    Samples a multi-band GeoTIFF at specified point locations from a GeoDataFrame,\n",
    "    using band names from the raster.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_path (str): Path to the GeoTIFF file.\n",
    "    - gdf (GeoDataFrame): GeoDataFrame containing point geometries.\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame with additional columns for each band, using raster band names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the raster file\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Reproject GeoDataFrame to match raster CRS if needed\n",
    "        if gdf.crs != src.crs:\n",
    "            gdf = gdf.to_crs(src.crs)\n",
    "\n",
    "        # Convert point geometries to raster pixel coordinates\n",
    "        coords = [(geom.x, geom.y) for geom in gdf.geometry]\n",
    "\n",
    "        # Sample raster at point locations (returns a list of tuples with values per band)\n",
    "        sampled_values = list(src.sample(coords))\n",
    "\n",
    "        # Get band names (if available, otherwise use default names)\n",
    "        band_names = src.descriptions if all(src.descriptions) else [f\"band_{i+1}\" for i in range(src.count)]\n",
    "\n",
    "        # Create new columns in the GeoDataFrame with the corresponding band names\n",
    "        for band_idx, band_name in enumerate(band_names):\n",
    "            gdf[band_name] = [val[band_idx] for val in sampled_values]\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# Function to clip point geometries by polygon geometries\n",
    "def clip_points_by_polygon(points_gdf, polygon_gdf):\n",
    "    # Ensure that both GeoDataFrames are in the same CRS\n",
    "    if points_gdf.crs != polygon_gdf.crs:\n",
    "        points_gdf = points_gdf.to_crs(polygon_gdf.crs)\n",
    "\n",
    "    # Clip the points with the polygon(s)\n",
    "    clipped_points = gpd.sjoin(points_gdf, polygon_gdf, how='inner')\n",
    "\n",
    "    # Drop the geometry from polygon_gdf that was added during the join (if needed)\n",
    "    clipped_points = clipped_points.drop(columns=polygon_gdf.columns.difference(['geometry']))\n",
    "\n",
    "    return clipped_points\n",
    "\n",
    "\n",
    "def calculate_metrics_and_plot(conf_matrices):\n",
    "    sensitivities = []\n",
    "    specificities = []\n",
    "    tss_values = []\n",
    "    \n",
    "    # Iterate through each confusion matrix\n",
    "    for cm in conf_matrices:\n",
    "        TN, FP, FN, TP = cm.ravel()  # Flatten confusion matrix\n",
    "\n",
    "        sensitivity = TP / (TP + FN)  # Sensitivity (TPR)\n",
    "        specificity = TN / (TN + FP)  # Specificity (TNR)\n",
    "        tss = sensitivity + specificity - 1  # True Skill Statistic (TSS)\n",
    "\n",
    "        sensitivities.append(sensitivity)\n",
    "        specificities.append(specificity)\n",
    "        tss_values.append(tss)\n",
    "\n",
    "    # Compute mean specificity and sensitivity\n",
    "    mean_sensitivity = np.mean(sensitivities)\n",
    "    mean_specificity = np.mean(specificities)\n",
    "\n",
    "    # Plot Boxplot of TSS\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(y=tss_values, color=\"skyblue\")\n",
    "    plt.axhline(mean_sensitivity, color=\"red\", linestyle=\"--\", label=f'Mean Sensitivity: {mean_sensitivity:.3f}')\n",
    "    plt.axhline(mean_specificity, color=\"green\", linestyle=\"--\", label=f'Mean Specificity: {mean_specificity:.3f}')\n",
    "    plt.ylabel(\"True Skill Statistic (TSS)\")\n",
    "    plt.title(\"Boxplot of TSS with Mean Sensitivity & Specificity\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return sensitivities, specificities, tss_values\n",
    "\n",
    "\n",
    "\n",
    "def filter_dataframe_columns(df, feature_choices):\n",
    "    return df[[col for col in df.columns if col in feature_choices]]\n",
    "\n",
    "def mark_true(series):\n",
    "    return [True if feature in series else False for feature in X_train_scaled.columns]\n",
    "\n",
    "def rename_dict(dictionary, tek_name):\n",
    "    return_names = []\n",
    "    return_lists = []\n",
    "    \n",
    "    for item in dictionary.items():\n",
    "        return_names.append(tek_name + str(item[0]))\n",
    "        return_lists.append(mark_true(list(item[1])))\n",
    "        \n",
    "    return dict(zip(return_names, return_lists))\n",
    "# Function to create empty dictionaries for feature importance\n",
    "def make_dict():\n",
    "    return {tup[0]: None for tup in vc_names}\n",
    "\n",
    "def make_dict_imp():\n",
    "    return dict(zip([tup[0] for tup in vc_names], [None]))\n",
    "\n",
    "# Function to append model weights to indv_roc\n",
    "def append_model_weights(classifiers, X_test_scaled, y_test):\n",
    "    weights = [accuracy_score(y_test, clf.predict(X_test_scaled)) for clf in classifiers]\n",
    "    indv_roc.append(weights)\n",
    "    return weights\n",
    "\n",
    "def predict_raster_and_display_folium(model, scaler, input_raster, output_raster, feature_list, state_polygon):\n",
    "    \"\"\"\n",
    "    Predicts species presence probability from a raster using a trained model and displays an interactive Folium map.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained machine learning model (supports `predict_proba`)\n",
    "    - scaler: StandardScaler fitted to training data\n",
    "    - input_raster: Path to input raster file\n",
    "    - output_raster: Path to save predicted probability raster\n",
    "    - feature_list: List of selected feature names (matching band names in the raster)\n",
    "    - state_polygon: GeoDataFrame containing the state boundary\n",
    "    \"\"\"\n",
    "    # Open the raster file\n",
    "    with rasterio.open(input_raster) as src:\n",
    "        profile = src.profile  # Get metadata\n",
    "        band_names = src.descriptions  # Band names in the raster\n",
    "\n",
    "        if band_names is None:\n",
    "            raise ValueError(\"Raster bands are missing names. Ensure input raster has band descriptions.\")\n",
    "\n",
    "        # Select only the bands that were used in training\n",
    "        selected_bands_indices = [i for i, band in enumerate(band_names) if band in feature_list]\n",
    "\n",
    "        if not selected_bands_indices:\n",
    "            raise ValueError(\"No matching bands found in the raster for the provided feature list.\")\n",
    "\n",
    "        # Mask raster to the state polygon\n",
    "        state_geom = [shape(state_polygon.geometry.iloc[0])]\n",
    "        img_data, transform = mask(src, state_geom, crop=True, nodata=np.nan)\n",
    "\n",
    "        # Read only the selected bands\n",
    "        img_data = img_data.astype(float)\n",
    "        img_data = img_data[selected_bands_indices, :, :]\n",
    "\n",
    "        # Reshape to 2D (pixels as rows, bands as columns)\n",
    "        num_bands = len(selected_bands_indices)\n",
    "        num_pixels = img_data.shape[1] * img_data.shape[2]\n",
    "        img_reshaped = img_data.reshape(num_bands, num_pixels).T\n",
    "\n",
    "        # Remove NaN values\n",
    "        valid_mask = ~np.isnan(img_reshaped).any(axis=1)\n",
    "        valid_pixels = img_reshaped[valid_mask]\n",
    "\n",
    "        # Convert to DataFrame with correct feature names\n",
    "        valid_pixels_df = pd.DataFrame(valid_pixels, columns=[feature_list[i] for i in range(len(selected_bands_indices))])\n",
    "\n",
    "        # Ensure valid_pixels_scaled remains a DataFrame with correct feature names\n",
    "        valid_pixels_scaled = pd.DataFrame(scaler.transform(valid_pixels_df), columns=feature_list)\n",
    "        \n",
    "        # Now, pass this DataFrame to the model for prediction\n",
    "        predicted_probs = model.predict_proba(valid_pixels_scaled)[:, 1]\n",
    "\n",
    "\n",
    "        # Create an empty array for the full image, setting invalid pixels to NaN\n",
    "        predicted_raster = np.full(img_reshaped.shape[0], np.nan)\n",
    "        predicted_raster[valid_mask] = predicted_probs\n",
    "\n",
    "        # Reshape back to original raster shape\n",
    "        predicted_raster = predicted_raster.reshape(img_data.shape[1], img_data.shape[2])\n",
    "\n",
    "        # Update metadata for a single-band float output raster\n",
    "        profile.update(dtype=rasterio.float32, count=1, compress='lzw')\n",
    "\n",
    "        # Write the predicted probability raster to a new file\n",
    "        with rasterio.open(output_raster, 'w', **profile) as dst:\n",
    "            dst.write(predicted_raster.astype(rasterio.float32), 1)\n",
    "\n",
    "    minx, maxy = transform * (0, 0)  # Upper-left corner\n",
    "    maxx, miny = transform * (predicted_raster.shape[1], predicted_raster.shape[0])  # Lower-right corner\n",
    "\n",
    "        \n",
    "    # Normalize values for visualization (0 to 1)\n",
    "    normed_raster = (predicted_raster - np.nanmin(predicted_raster)) / (np.nanmax(predicted_raster) - np.nanmin(predicted_raster))\n",
    "\n",
    "    # Convert to RGB heatmap image\n",
    "    cmap = plt.get_cmap(\"jet\")  # Use 'jet' colormap\n",
    "    heatmap_image = (cmap(normed_raster)[:, :, :3] * 255).astype(np.uint8)  # Convert to 8-bit RGB\n",
    "\n",
    "    # Flip image vertically because Folium expects north-up orientation\n",
    "    #heatmap_image = np.flipud(heatmap_image)\n",
    "\n",
    "    # Create a folium map centered on the raster bounds\n",
    "    m = folium.Map(location=[(miny + maxy) / 2, (minx + maxx) / 2], zoom_start=6)\n",
    "\n",
    "    # Overlay heatmap as an image\n",
    "    folium.raster_layers.ImageOverlay(\n",
    "        image=heatmap_image,\n",
    "        bounds=[[miny, minx], [maxy, maxx]],\n",
    "        opacity=0.6,\n",
    "        colormap=lambda x: (x, 0, 1 - x, 1),  # Adjust color mapping\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add fullscreen button\n",
    "    Fullscreen().add_to(m)\n",
    "\n",
    "    # Display map in Jupyter Notebook\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb87a3d-0234-4f8c-a0fd-5cbd9a83de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User defined variables\n",
    "my_state = 'MN' # should be the postal code abbreviation for the state you created the environmental raster for....\n",
    "my_nas_id = 5 # go to USGS NAS database for species_ids\n",
    "my_path = 'data/' + my_state + '/' # leave this alone\n",
    "my_crs = 5070\n",
    "feature_choices = [\n",
    "    # 'Freeze_Up',\n",
    "    # 'Ice_Melt',\n",
    "    # 'Spawn_Start',\n",
    "    # 'Spawn_End',\n",
    "    # 'Spawn_Ideal',\n",
    "    'Ca',\n",
    "    'pH',\n",
    "    'DO',\n",
    "    'Phos',\n",
    "    'Inv_Algae',\n",
    "    'Inv_Crustaceans',\n",
    "    'Inv_Fish',\n",
    "    'Inv_Mollusks',\n",
    "    'Inv_Plants',\n",
    "    'Invasive_Richness',\n",
    "    'Native_Fish',\n",
    "    # 'Precip_Winter',\n",
    "    'Precip_Spring',\n",
    "    'Precip_Summer',\n",
    "    'Precip_Fall',\n",
    "    'Flashiness',\n",
    "    # 'Runoff',\n",
    "    # 'Drawdown',\n",
    "    'LST_Annual',\n",
    "    # 'LST_Summer',\n",
    "    # 'LST_Winter',\n",
    "    # 'LST_Spring',\n",
    "    # 'LST_Fall',\n",
    "    'NDVI',\n",
    "    # 'GPP_Annual',\n",
    "    # 'GPP_Summer',\n",
    "    'Heat_Insolation',\n",
    "    'Topo_Diversity',\n",
    "    'gHM',\n",
    "    'NDTI',\n",
    "    'NDBI',\n",
    "    'NDCI',\n",
    "    'NDSI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21606a-582d-4b54-81a8-7ebc43a70e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve occurence data from USGS NAS API\n",
    "pos_data = api_call(my_nas_id, my_state)\n",
    "my_data = pos_data[[\"decimalLatitude\", \"decimalLongitude\"]]\n",
    "pos_data_gdf = gpd.GeoDataFrame(\n",
    "    my_data, geometry=gpd.points_from_xy(my_data.decimalLongitude, my_data.decimalLatitude)).set_crs(4269, allow_override=True)\n",
    "bg_data_gdf = gpd.read_file(my_path + my_state + \"_bg.shp\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf67331-ae99-4a7a-96f2-150855ae2ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoTIFF and GeoDataFrame\n",
    "raster_path = my_path + my_state + \"_raster.tif\" #If you are training in one state and predicting to another change this to your training state raster filename\n",
    "states_gdf = gpd.read_file(my_path + 'tl_2012_us_state.shp')\n",
    "state_polygon = states_gdf[states_gdf['STUSPS'] == my_state]\n",
    "# Sample raster at points\n",
    "pos_data = sample_multiband_geotiff_with_names(raster_path, pos_data_gdf)\n",
    "neg_data = sample_multiband_geotiff_with_names(raster_path, bg_data_gdf)\n",
    "# clip the data to your state\n",
    "clipped_negs = clip_points_by_polygon(neg_data, state_polygon)\n",
    "clipped_pos = clip_points_by_polygon(pos_data, state_polygon)\n",
    "# Filter to just predictors you selected and add Present column with values.\n",
    "my_pos = filter_dataframe_columns(clipped_pos, feature_choices).dropna().astype(float)\n",
    "my_negs = filter_dataframe_columns(clipped_negs, feature_choices).dropna().astype(float)\n",
    "my_pos[\"Present\"], my_negs[\"Present\"] = 1.0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4569024-4f24-4650-9fb9-bd45cce7e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model and get weights for feature importance analysis.  This takes a while.  \n",
    "# Initialize lists for metrics and feature importance\n",
    "indv_roc = []\n",
    "vc_roc_arr, vc_cm_arr = [], []\n",
    "stack_roc_arr, stack_cm_arr = [], []\n",
    "\n",
    "for x in range(1, 11): \n",
    "    print(f\"Iteration {x}\")\n",
    "    \n",
    "    # Splitting positive and negative samples\n",
    "    X_train_pos, X_test_pos, y_train_pos, y_test_pos = train_test_split(\n",
    "        my_pos.drop(columns=['Present']), my_pos[\"Present\"], test_size=0.25, train_size=0.75, random_state=46)\n",
    "    X_train_neg, X_test_neg, y_train_neg, y_test_neg = train_test_split(\n",
    "        my_negs.drop(columns=['Present']), my_negs[\"Present\"], test_size=0.15, train_size=0.15, random_state=46)\n",
    "    \n",
    "    # Combine training sets\n",
    "    X_train = pd.concat([X_train_pos, X_train_neg])\n",
    "    y_train = pd.concat([y_train_pos, y_train_neg])\n",
    "    \n",
    "    # Standardizing the data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test = pd.concat([X_test_pos, X_test_neg])\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_test = pd.concat([y_test_pos, y_test_neg])\n",
    "    \n",
    "    # Define classifiers\n",
    "    MaxEnt = LogisticRegression(max_iter=10000)\n",
    "    rf = RandomForestClassifier(n_estimators=1000)\n",
    "    brt = GradientBoostingClassifier(n_estimators=1000)\n",
    "    dt = DecisionTreeClassifier()\n",
    "    mlp = MLPClassifier(max_iter=10000)\n",
    "    \n",
    "    # Train classifiers\n",
    "    classifiers = [MaxEnt, rf, dt, brt, mlp]\n",
    "    for clf in classifiers:\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Append individual model weights\n",
    "    weights = append_model_weights(classifiers, X_test_scaled, y_test)\n",
    "    \n",
    "    # Ensemble Voting Classifier\n",
    "    vc_names = [('DT', dt), ('MaxEnt', MaxEnt), ('MLP', mlp), ('BRT', brt), ('RF', rf)]\n",
    "    vc = VotingClassifier(estimators=vc_names, voting='soft', weights=weights)\n",
    "    vc.fit(X_train_scaled, y_train)\n",
    "    \n",
    "     \n",
    "    # Performance Metrics\n",
    "    vc_roc = accuracy_score(y_test, vc.predict(X_test_scaled))\n",
    "    vc_cm = confusion_matrix(y_test, vc.predict(X_test_scaled))\n",
    "        \n",
    "    vc_roc_arr.append(vc_roc)\n",
    "    vc_cm_arr.append(vc_cm)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91551f-f25b-4bbd-8141-69c56cb45c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivities, specificities, tss_values = calculate_metrics_and_plot(vc_cm_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aac006-b089-438d-8ede-f998bbba940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average weights for feature importance evaluation\n",
    "avg_weight = [sum(col) / float(len(col)) for col in zip(*indv_roc)]\n",
    "vc_imp = VotingClassifier(estimators=vc_names, voting='soft', weights=avg_weight)\n",
    "\n",
    "# Splitting positive and negative samples\n",
    "X_train_pos, X_test_pos, y_train_pos, y_test_pos = train_test_split(\n",
    "        my_pos.drop(columns=['Present']), my_pos[\"Present\"], test_size=0.25, train_size=0.75, random_state=46)\n",
    "X_train_neg, X_test_neg, y_train_neg, y_test_neg = train_test_split(\n",
    "        my_negs.drop(columns=['Present']), my_negs[\"Present\"], test_size=0.15, train_size=0.15, random_state=46)\n",
    "\n",
    "                  \n",
    "X_train = pd.concat([X_train_pos, X_train_neg])\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train))\n",
    "\n",
    "y_train = pd.concat([y_train_pos, y_train_neg])\n",
    "\n",
    "vc_imp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Initialize feature importance dictionaries\n",
    "rfe_dict, perm_dict, drop_dict = make_dict_imp(), make_dict_imp(), make_dict_imp()\n",
    "\n",
    "for alg in vc_imp.named_estimators:\n",
    "        dict_name = alg\n",
    "        clf = vc_imp.named_estimators[dict_name]\n",
    "        if dict_name == 'MLP':\n",
    "            continue\n",
    "        print(\"Considering\", clf)\n",
    "    \n",
    "        # Recursive Feature Elimination (RFE)\n",
    "        rfe_selector = RFECV(estimator=clf, min_features_to_select=3, step=1)\n",
    "        rfe_selector.fit(X_train_scaled, y_train)\n",
    "        rfe_support = rfe_selector.get_support()\n",
    "        rfe_features = X_train_scaled.loc[:,rfe_support].columns.tolist()\n",
    "        rfe_dict[dict_name] = rfe_features\n",
    "        # Permutation Importance\n",
    "        perm_imp = permutation_importance(clf, X_train_scaled, y_train)\n",
    "        perm_imp['feature'] = X_train_scaled.columns\n",
    "        perm_features = pd.DataFrame(perm_imp['feature'],perm_imp['importances_mean'],columns = ['Feature']) \\\n",
    "                        .sort_index(ascending=False)['Feature'].values[:3]\n",
    "        perm_dict[dict_name] = perm_features\n",
    "        # Drop Column Importance (Assuming a predefined drop_col function)\n",
    "        drop_dict_list = []\n",
    "        \n",
    "        drop_col_feats = drop_col(clf, X_train_scaled, y_train) #, random_state = 10)\n",
    "        drop_col_three = drop_col_feats.sort_values('feature_importance',ascending = False)['feature'][:3]\n",
    "        drop_dict[dict_name] = drop_col_three\n",
    "    \n",
    "print(\"Feature importance evaluation completed.\")\n",
    "\n",
    "\n",
    "#We end up with a dataframe that says, for any model / feature importance technique, whether or not a feature ended up in the top N (i.e. whether or not that feature is important).\n",
    "features_df = pd.concat([pd.DataFrame(rename_dict(perm_dict,'PERM_')).reset_index(drop=True),\n",
    "                        pd.DataFrame(rename_dict(rfe_dict,'RFE_')),\n",
    "                        pd.DataFrame(rename_dict(drop_dict, 'DROP_'))],axis=1)\n",
    "\n",
    "features_df['Total'] = np.sum(features_df, axis=1)\n",
    "features_df['Feature'] = X_train.columns\n",
    "features_df.sort_values(['Total','Feature'] , ascending=False,inplace=True)\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plt.setp( ax.xaxis.get_majorticklabels(), rotation=-45, ha=\"left\" )\n",
    "plt.setp( ax.xaxis.get_majorticklabels(), rotation=-45, ha=\"left\" )\n",
    "\n",
    "# plt.xticks(rotation=75)\n",
    "plt.bar(features_df['Feature'], features_df['Total'])\n",
    "plt.title(\"Variable importances\")\n",
    "plt.ylabel(\"Number of times a variable appeared in top 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80639fcc-e18b-4b4e-b344-daac814897a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add steps to train from one state and predict to state without presences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3213e03c-8d70-4ef7-bc7e-d5bfb47a9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap with all data\n",
    "X_train_pos = my_pos.drop(columns=['Present'])\n",
    "y_train_pos = my_pos['Present']\n",
    "X_train_neg = my_negs.drop(columns=['Present'])\n",
    "y_train_neg = my_negs['Present']\n",
    "\n",
    "X_train = pd.concat([X_train_pos, X_train_neg])\n",
    "scaler_heatmap = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(scaler_heatmap.transform(X_train), columns=X_train.columns)\n",
    "\n",
    "y_train = pd.concat([y_train_pos, y_train_neg])\n",
    "\n",
    "MaxEnt = LogisticRegression(max_iter = 10000)\n",
    "rf = RandomForestClassifier(n_estimators=10000)\n",
    "brt = GradientBoostingClassifier(n_estimators=10000)\n",
    "dt = DecisionTreeClassifier()\n",
    "mlp = MLPClassifier(max_iter = 10000)\n",
    "\n",
    "# Individual Confusion Matrices\n",
    "MaxEnt.fit(X_train_scaled, y_train)\n",
    "rf.fit(X_train_scaled,y_train)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "brt.fit(X_train_scaled, y_train) \n",
    "mlp.fit(X_train_scaled, y_train)                    \n",
    "########################################################################################################\n",
    "#Ensemble \n",
    "ensemble_vc = Ensemble([dt, MaxEnt, rf, brt, mlp]) \n",
    "ensemble_vc.fit_all(X_train_scaled, y_train)\n",
    "weights = ensemble_vc.get_weights()\n",
    "vc_names = [('DT', dt), ('MaxEnt', MaxEnt), ('MLP', mlp), ('BRT', brt), ('RF', rf)]\n",
    "vc_heatmaps = VotingClassifier(estimators=vc_names, voting='soft', weights = weights)\n",
    "vc_heatmaps.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1d20f-42b3-4700-9673-d6b8441cf8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction and display the heatmap\n",
    "output_tif = \"predicted_probabilities.tif\"\n",
    "predict_to_raster = raster_path # If you made a second raster for your state to predict to.  Define it here.\n",
    "\n",
    "# Define the selected feature list (only include features you want to use)\n",
    "selected_features = X_train.columns.tolist()\n",
    "\n",
    "m = predict_raster_and_display_folium(vc_heatmaps, scaler_heatmap, predict_to_raster, output_tif, selected_features, state_polygon)\n",
    "m  # This will display the interactive map in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2873dc-f0b2-4456-ad47-33b2347d6907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
